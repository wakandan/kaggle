{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T07:56:37.126876Z",
     "start_time": "2018-09-24T07:56:34.525485Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akai/virtualenvs/tensor_flow/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/akai/virtualenvs/tensor_flow/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/home/akai/virtualenvs/tensor_flow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import pandas as pd\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, load_model\n",
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T03:57:29.048469Z",
     "start_time": "2018-09-24T03:57:29.026727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n",
      "----------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      "PassengerId    418 non-null int64\n",
      "Pclass         418 non-null int64\n",
      "Name           418 non-null object\n",
      "Sex            418 non-null object\n",
      "Age            332 non-null float64\n",
      "SibSp          418 non-null int64\n",
      "Parch          418 non-null int64\n",
      "Ticket         418 non-null object\n",
      "Fare           417 non-null float64\n",
      "Cabin          91 non-null object\n",
      "Embarked       418 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "train_df.info()\n",
    "print('-'*40)\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T09:05:35.788806Z",
     "start_time": "2018-09-24T09:05:35.708103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PassengerId  Pclass                                               Name  \\\n",
      "0           892       3                                   Kelly, Mr. James   \n",
      "1           893       3                   Wilkes, Mrs. James (Ellen Needs)   \n",
      "2           894       2                          Myles, Mr. Thomas Francis   \n",
      "3           895       3                                   Wirz, Mr. Albert   \n",
      "4           896       3       Hirvonen, Mrs. Alexander (Helga E Lindqvist)   \n",
      "5           897       3                         Svensson, Mr. Johan Cervin   \n",
      "6           898       3                               Connolly, Miss. Kate   \n",
      "7           899       2                       Caldwell, Mr. Albert Francis   \n",
      "8           900       3          Abrahim, Mrs. Joseph (Sophie Halaut Easu)   \n",
      "9           901       3                            Davies, Mr. John Samuel   \n",
      "10          902       3                                   Ilieff, Mr. Ylio   \n",
      "11          903       1                         Jones, Mr. Charles Cresson   \n",
      "12          904       1      Snyder, Mrs. John Pillsbury (Nelle Stevenson)   \n",
      "13          905       2                               Howard, Mr. Benjamin   \n",
      "14          906       1  Chaffee, Mrs. Herbert Fuller (Carrie Constance...   \n",
      "15          907       2      del Carlo, Mrs. Sebastiano (Argenia Genovesi)   \n",
      "16          908       2                                  Keane, Mr. Daniel   \n",
      "17          909       3                                  Assaf, Mr. Gerios   \n",
      "18          910       3                       Ilmakangas, Miss. Ida Livija   \n",
      "19          911       3              Assaf Khalil, Mrs. Mariana (Miriam\")\"   \n",
      "20          912       1                             Rothschild, Mr. Martin   \n",
      "21          913       3                          Olsen, Master. Artur Karl   \n",
      "22          914       1               Flegenheim, Mrs. Alfred (Antoinette)   \n",
      "23          915       1                    Williams, Mr. Richard Norris II   \n",
      "24          916       1    Ryerson, Mrs. Arthur Larned (Emily Maria Borie)   \n",
      "25          917       3                            Robins, Mr. Alexander A   \n",
      "26          918       1                       Ostby, Miss. Helene Ragnhild   \n",
      "27          919       3                                  Daher, Mr. Shedid   \n",
      "28          920       1                            Brady, Mr. John Bertram   \n",
      "29          921       3                                  Samaan, Mr. Elias   \n",
      "..          ...     ...                                                ...   \n",
      "70          962       3                          Mulvihill, Miss. Bertha E   \n",
      "71          963       3                                 Minkoff, Mr. Lazar   \n",
      "72          964       3                     Nieminen, Miss. Manta Josefina   \n",
      "73          965       1                    Ovies y Rodriguez, Mr. Servando   \n",
      "74          966       1                               Geiger, Miss. Amalie   \n",
      "75          967       1                                 Keeping, Mr. Edwin   \n",
      "76          968       3                                   Miles, Mr. Frank   \n",
      "77          969       1  Cornell, Mrs. Robert Clifford (Malvina Helen L...   \n",
      "78          970       2                     Aldworth, Mr. Charles Augustus   \n",
      "79          971       3                             Doyle, Miss. Elizabeth   \n",
      "80          972       3                               Boulos, Master. Akar   \n",
      "81          973       1                                 Straus, Mr. Isidor   \n",
      "82          974       1                             Case, Mr. Howard Brown   \n",
      "83          975       3                               Demetri, Mr. Marinko   \n",
      "84          976       2                              Lamb, Mr. John Joseph   \n",
      "85          977       3                                 Khalil, Mr. Betros   \n",
      "86          978       3                                 Barry, Miss. Julia   \n",
      "87          979       3                         Badman, Miss. Emily Louisa   \n",
      "88          980       3                            O'Donoghue, Ms. Bridget   \n",
      "89          981       2                        Wells, Master. Ralph Lester   \n",
      "90          982       3  Dyker, Mrs. Adolf Fredrik (Anna Elisabeth Judi...   \n",
      "91          983       3                                 Pedersen, Mr. Olaf   \n",
      "92          984       1               Davidson, Mrs. Thornton (Orian Hays)   \n",
      "93          985       3                                  Guest, Mr. Robert   \n",
      "94          986       1                                Birnbaum, Mr. Jakob   \n",
      "95          987       3                         Tenglin, Mr. Gunnar Isidor   \n",
      "96          988       1  Cavendish, Mrs. Tyrell William (Julia Florence...   \n",
      "97          989       3                          Makinen, Mr. Kalle Edvard   \n",
      "98          990       3                       Braf, Miss. Elin Ester Maria   \n",
      "99          991       3                       Nancarrow, Mr. William Henry   \n",
      "\n",
      "       Sex   Age  SibSp  Parch             Ticket      Fare            Cabin  \\\n",
      "0     male  34.5      0      0             330911    7.8292              NaN   \n",
      "1   female  47.0      1      0             363272    7.0000              NaN   \n",
      "2     male  62.0      0      0             240276    9.6875              NaN   \n",
      "3     male  27.0      0      0             315154    8.6625              NaN   \n",
      "4   female  22.0      1      1            3101298   12.2875              NaN   \n",
      "5     male  14.0      0      0               7538    9.2250              NaN   \n",
      "6   female  30.0      0      0             330972    7.6292              NaN   \n",
      "7     male  26.0      1      1             248738   29.0000              NaN   \n",
      "8   female  18.0      0      0               2657    7.2292              NaN   \n",
      "9     male  21.0      2      0          A/4 48871   24.1500              NaN   \n",
      "10    male   NaN      0      0             349220    7.8958              NaN   \n",
      "11    male  46.0      0      0                694   26.0000              NaN   \n",
      "12  female  23.0      1      0              21228   82.2667              B45   \n",
      "13    male  63.0      1      0              24065   26.0000              NaN   \n",
      "14  female  47.0      1      0        W.E.P. 5734   61.1750              E31   \n",
      "15  female  24.0      1      0      SC/PARIS 2167   27.7208              NaN   \n",
      "16    male  35.0      0      0             233734   12.3500              NaN   \n",
      "17    male  21.0      0      0               2692    7.2250              NaN   \n",
      "18  female  27.0      1      0   STON/O2. 3101270    7.9250              NaN   \n",
      "19  female  45.0      0      0               2696    7.2250              NaN   \n",
      "20    male  55.0      1      0           PC 17603   59.4000              NaN   \n",
      "21    male   9.0      0      1            C 17368    3.1708              NaN   \n",
      "22  female   NaN      0      0           PC 17598   31.6833              NaN   \n",
      "23    male  21.0      0      1           PC 17597   61.3792              NaN   \n",
      "24  female  48.0      1      3           PC 17608  262.3750  B57 B59 B63 B66   \n",
      "25    male  50.0      1      0          A/5. 3337   14.5000              NaN   \n",
      "26  female  22.0      0      1             113509   61.9792              B36   \n",
      "27    male  22.5      0      0               2698    7.2250              NaN   \n",
      "28    male  41.0      0      0             113054   30.5000              A21   \n",
      "29    male   NaN      2      0               2662   21.6792              NaN   \n",
      "..     ...   ...    ...    ...                ...       ...              ...   \n",
      "70  female  24.0      0      0             382653    7.7500              NaN   \n",
      "71    male  21.0      0      0             349211    7.8958              NaN   \n",
      "72  female  29.0      0      0            3101297    7.9250              NaN   \n",
      "73    male  28.5      0      0           PC 17562   27.7208              D43   \n",
      "74  female  35.0      0      0             113503  211.5000             C130   \n",
      "75    male  32.5      0      0             113503  211.5000             C132   \n",
      "76    male   NaN      0      0             359306    8.0500              NaN   \n",
      "77  female  55.0      2      0              11770   25.7000             C101   \n",
      "78    male  30.0      0      0             248744   13.0000              NaN   \n",
      "79  female  24.0      0      0             368702    7.7500              NaN   \n",
      "80    male   6.0      1      1               2678   15.2458              NaN   \n",
      "81    male  67.0      1      0           PC 17483  221.7792          C55 C57   \n",
      "82    male  49.0      0      0              19924   26.0000              NaN   \n",
      "83    male   NaN      0      0             349238    7.8958              NaN   \n",
      "84    male   NaN      0      0             240261   10.7083              NaN   \n",
      "85    male   NaN      1      0               2660   14.4542              NaN   \n",
      "86  female  27.0      0      0             330844    7.8792              NaN   \n",
      "87  female  18.0      0      0          A/4 31416    8.0500              NaN   \n",
      "88  female   NaN      0      0             364856    7.7500              NaN   \n",
      "89    male   2.0      1      1              29103   23.0000              NaN   \n",
      "90  female  22.0      1      0             347072   13.9000              NaN   \n",
      "91    male   NaN      0      0             345498    7.7750              NaN   \n",
      "92  female  27.0      1      2         F.C. 12750   52.0000              B71   \n",
      "93    male   NaN      0      0             376563    8.0500              NaN   \n",
      "94    male  25.0      0      0              13905   26.0000              NaN   \n",
      "95    male  25.0      0      0             350033    7.7958              NaN   \n",
      "96  female  76.0      1      0              19877   78.8500              C46   \n",
      "97    male  29.0      0      0  STON/O 2. 3101268    7.9250              NaN   \n",
      "98  female  20.0      0      0             347471    7.8542              NaN   \n",
      "99    male  33.0      0      0         A./5. 3338    8.0500              NaN   \n",
      "\n",
      "   Embarked Title    Who  Adult_Male  \n",
      "0         Q    Mr    man        True  \n",
      "1         S   Mrs  woman       False  \n",
      "2         Q    Mr    man        True  \n",
      "3         S    Mr    man        True  \n",
      "4         S   Mrs  woman       False  \n",
      "5         S    Mr  child       False  \n",
      "6         Q  Miss  woman       False  \n",
      "7         S    Mr    man        True  \n",
      "8         C   Mrs  woman       False  \n",
      "9         S    Mr    man        True  \n",
      "10        S    Mr    man        True  \n",
      "11        S    Mr    man        True  \n",
      "12        S   Mrs  woman       False  \n",
      "13        S    Mr    man        True  \n",
      "14        S   Mrs  woman       False  \n",
      "15        C   Mrs  woman       False  \n",
      "16        Q    Mr    man        True  \n",
      "17        C    Mr    man        True  \n",
      "18        S  Miss  woman       False  \n",
      "19        C   Mrs  woman       False  \n",
      "20        C    Mr    man        True  \n",
      "21        S   Boy  child       False  \n",
      "22        S   Mrs  woman       False  \n",
      "23        C    Mr    man        True  \n",
      "24        C   Mrs  woman       False  \n",
      "25        S    Mr    man        True  \n",
      "26        C  Miss  woman       False  \n",
      "27        C    Mr    man        True  \n",
      "28        S    Mr    man        True  \n",
      "29        C    Mr    man        True  \n",
      "..      ...   ...    ...         ...  \n",
      "70        Q  Miss  woman       False  \n",
      "71        S    Mr    man        True  \n",
      "72        S  Miss  woman       False  \n",
      "73        C    Mr    man        True  \n",
      "74        C  Miss  woman       False  \n",
      "75        C    Mr    man        True  \n",
      "76        S    Mr    man        True  \n",
      "77        S   Mrs  woman       False  \n",
      "78        S    Mr    man        True  \n",
      "79        Q  Miss  woman       False  \n",
      "80        C   Boy  child       False  \n",
      "81        S    Mr    man        True  \n",
      "82        S    Mr    man        True  \n",
      "83        S    Mr    man        True  \n",
      "84        Q    Mr    man        True  \n",
      "85        C    Mr    man        True  \n",
      "86        Q  Miss  woman       False  \n",
      "87        S  Miss  woman       False  \n",
      "88        Q  Miss  woman       False  \n",
      "89        S   Boy  child       False  \n",
      "90        S   Mrs  woman       False  \n",
      "91        S    Mr    man        True  \n",
      "92        S   Mrs  woman       False  \n",
      "93        S    Mr    man        True  \n",
      "94        C    Mr    man        True  \n",
      "95        S    Mr    man        True  \n",
      "96        S   Mrs  woman       False  \n",
      "97        S    Mr    man        True  \n",
      "98        S  Miss  woman       False  \n",
      "99        S    Mr    man        True  \n",
      "\n",
      "[100 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "combine = [train_df, test_df]\n",
    "y = train_df['Survived']\n",
    "train_df.drop(['Survived'], axis=1, inplace=True)\n",
    "test_ids = test_df['PassengerId']\n",
    "def woman_child_or_man(passenger):\n",
    "    age, sex = passenger\n",
    "    if age < 16:\n",
    "        return \"child\"\n",
    "    else:\n",
    "        return dict(male=\"man\", female=\"woman\")[sex]\n",
    "    \n",
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    dataset.loc[dataset.Title=='Ms', 'Title'] = 'Miss' # unify the naming\n",
    "    dataset['Title'] = dataset['Title'].replace(['Mme', 'Mlle'], 'Mrs') # seem to be the title for old lady\n",
    "    dataset.loc[((dataset.Title=='Master') | (dataset.Title.isnull())) & (dataset.Age<15) & (dataset.Sex=='male'), 'Title'] = 'Boy'\n",
    "    dataset.loc[((dataset.Title=='Mrs') | dataset.Title.isnull() | (dataset.Title=='Miss') ) & (dataset.Sex=='female') & (dataset.Age<15), 'Title'] = 'Girl'\n",
    "    dataset['Title'] = dataset['Title'].replace(['Jonkheer', 'Don', 'Sir', 'Countess', 'Lady', 'Dona'], 'Royalty')\n",
    "    dataset['Title'] = dataset['Title'].replace(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 'Officer')\n",
    "    dataset['Who'] = dataset[['Age', 'Sex']].apply(woman_child_or_man, axis=1) \n",
    "    dataset['Adult_Male'] = dataset['Who'] == 'man'\n",
    "#     print(dataset.loc[(dataset.Title=='Mrs') & dataset.Age<10][['Title', 'Name', 'Age']].sort_values('Age'))\n",
    "print(dataset.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = dict(man=\"#4682B4\", woman=\"#CD5C5C\", child=\"#2E8B57\", male=\"#6495ED\", female=\"#F08080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7faad27c6b38>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot('Sex', data=train_df, kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7faad046cb70>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot('Who', data=train_df, kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7faad05e4b70>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 437.975x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot('Pclass', data=train_df, hue='Sex', kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7faad043f128>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 439.85x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot('Pclass', data=train_df, hue='Who', kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T09:05:38.503189Z",
     "start_time": "2018-09-24T09:05:37.737215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Pclass                                               Name  \\\n",
      "0            1       3                            Braund, Mr. Owen Harris   \n",
      "1            2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
      "2            3       3                             Heikkinen, Miss. Laina   \n",
      "3            4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
      "4            5       3                           Allen, Mr. William Henry   \n",
      "\n",
      "      Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked Title  \n",
      "0    male  22.0      1      0         A/5 21171   7.2500   NaN        S    Mr  \n",
      "1  female  38.0      1      0          PC 17599  71.2833   C85        C   Mrs  \n",
      "2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  Miss  \n",
      "3  female  35.0      1      0            113803  53.1000  C123        S   Mrs  \n",
      "4    male  35.0      0      0            373450   8.0500   NaN        S    Mr  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['IsAlone', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male',\n",
      "       'Ticket_A4', 'Ticket_A5', 'Ticket_AS', 'Ticket_C', 'Ticket_CA',\n",
      "       'Ticket_CASOTON', 'Ticket_FC', 'Ticket_FCC', 'Ticket_Fa', 'Ticket_LINE',\n",
      "       'Ticket_PC', 'Ticket_PP', 'Ticket_PPP', 'Ticket_SC', 'Ticket_SCA4',\n",
      "       'Ticket_SCAH', 'Ticket_SCOW', 'Ticket_SCPARIS', 'Ticket_SCParis',\n",
      "       'Ticket_SOC', 'Ticket_SOP', 'Ticket_SOPP', 'Ticket_SOTONO2',\n",
      "       'Ticket_SOTONOQ', 'Ticket_SP', 'Ticket_STONO', 'Ticket_STONO2',\n",
      "       'Ticket_SWPP', 'Ticket_WC', 'Ticket_WEP', 'Ticket_XXX', 'Cabin_A',\n",
      "       'Cabin_B', 'Cabin_C', 'Cabin_D', 'Cabin_E', 'Cabin_F', 'Cabin_G',\n",
      "       'Cabin_T', 'Cabin_U', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n",
      "       'Title_Boy', 'Title_Girl', 'Title_Master', 'Title_Miss', 'Title_Mr',\n",
      "       'Title_Mrs', 'Title_Officer', 'Title_Royalty', 'AgeBand_0.0',\n",
      "       'AgeBand_1.0', 'AgeBand_2.0', 'AgeBand_nan', 'FareBand_0.0',\n",
      "       'FareBand_1.0', 'FareBand_2.0', 'FareBand_3.0'],\n",
      "      dtype='object')\n",
      "   IsAlone  Pclass_1  Pclass_2  Pclass_3  Sex_female  Sex_male  Ticket_A4  \\\n",
      "0        0         0         0         1           0         1          0   \n",
      "1        0         1         0         0           1         0          0   \n",
      "2        1         0         0         1           1         0          0   \n",
      "3        0         1         0         0           1         0          0   \n",
      "4        1         0         0         1           0         1          0   \n",
      "\n",
      "   Ticket_A5  Ticket_AS  Ticket_C      ...       Title_Officer  Title_Royalty  \\\n",
      "0          1          0         0      ...                   0              0   \n",
      "1          0          0         0      ...                   0              0   \n",
      "2          0          0         0      ...                   0              0   \n",
      "3          0          0         0      ...                   0              0   \n",
      "4          0          0         0      ...                   0              0   \n",
      "\n",
      "   AgeBand_0.0  AgeBand_1.0  AgeBand_2.0  AgeBand_nan  FareBand_0.0  \\\n",
      "0            0            1            0            0             1   \n",
      "1            0            1            0            0             0   \n",
      "2            0            1            0            0             1   \n",
      "3            0            1            0            0             0   \n",
      "4            0            1            0            0             0   \n",
      "\n",
      "   FareBand_1.0  FareBand_2.0  FareBand_3.0  \n",
      "0             0             0             0  \n",
      "1             0             0             1  \n",
      "2             0             0             0  \n",
      "3             0             0             1  \n",
      "4             1             0             0  \n",
      "\n",
      "[5 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_df.head())\n",
    "combined = pd.concat([train_df, test_df])\n",
    "grouped_median_age = combined[train_df.Age.notnull()].groupby(['Sex', 'Pclass', 'Title']).median()\n",
    "grouped_median_age = grouped_median_age.reset_index()[['Sex', 'Pclass', 'Title', 'Age']]\n",
    "grouped_median_age\n",
    "\n",
    "def assign_age(row):\n",
    "    condition = (\n",
    "        (grouped_median_age['Sex'] == row['Sex']) & \n",
    "        (grouped_median_age['Title'] == row['Title']) & \n",
    "        (grouped_median_age['Pclass'] == row['Pclass'])\n",
    "    )\n",
    "    return grouped_median_age[condition]['Age'].values[0]\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset['Age'] = dataset.apply(lambda row: assign_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "    dataset.loc[dataset.Embarked.isnull(), 'Embarked'] = 'S'\n",
    "    dataset.loc[dataset.Cabin.notnull(), 'Cabin'] = dataset.loc[dataset.Cabin.notnull(), 'Cabin'].map(lambda x: x[0])\n",
    "    dataset.loc[dataset.Cabin.isnull(), 'Cabin'] = 'U' #unknown\n",
    "    \n",
    "def cleanTicket(ticket):\n",
    "    ticket = ticket.replace('.', '')\n",
    "    ticket = ticket.replace('/', '')\n",
    "    ticket = ticket.split()\n",
    "    ticket = map(lambda t : t.strip(), ticket)\n",
    "    ticket = list(filter(lambda t : not t.isdigit(), ticket))\n",
    "    if len(ticket) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "for dataset in combine:    \n",
    "    dataset.loc[ dataset['Age'] <= 15, 'AgeBand'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 15) & (dataset['Age'] <= 40), 'AgeBand'] = 1\n",
    "    dataset.loc[ dataset['Age'] > 40, 'AgeBand'] = 2\n",
    "    dataset['AgeBand'] = dataset['AgeBand'].astype(str)\n",
    "    dataset.loc[dataset['Fare'].isnull(), 'Fare'] = 8\n",
    "    dataset.loc[dataset['Fare'] <= 8, 'FareBand'] = 0\n",
    "    dataset.loc[(dataset['Fare'] > 8) & (dataset['Fare'] <= 14.454), 'FareBand'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'FareBand']   = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'FareBand'] = 3\n",
    "    dataset['FareBand'] = dataset['FareBand'].astype(str)\n",
    "    dataset['Pclass'] = dataset['Pclass'].astype(str)\n",
    "    dataset['Ticket'] = dataset['Ticket'].map(cleanTicket)\n",
    "#     for column in ['Ticket']:\n",
    "#         dataset[column] = dataset[column].astype('category')\n",
    "#         dataset[column] = dataset[column].cat.codes\n",
    "for dataset in combine:\n",
    "#     dataset.drop(['PassengerId', 'Name', 'Age', 'SibSp', 'Parch', 'FamilySize'], axis=1, inplace=True)\n",
    "    dataset.drop(['PassengerId', 'Name', 'Age', 'SibSp', 'Parch', 'Fare', 'FamilySize'], axis=1, inplace=True)\n",
    "train_df = pd.get_dummies(train_df)\n",
    "print(train_df.columns)\n",
    "test_df = pd.get_dummies(test_df)\n",
    "# Get missing columns in the training test\n",
    "missing_cols = set( train_df.columns ) - set( test_df.columns )\n",
    "# Add a missing column in test set with default value equal to 0\n",
    "for c in missing_cols:\n",
    "    test_df[c] = 0\n",
    "# Ensure the order of column in the test set is in the same order than in train set\n",
    "test_df = test_df[train_df.columns]\n",
    "train_df.to_csv('train_temp.csv')\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T09:06:00.540343Z",
     "start_time": "2018-09-24T09:05:52.433236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** before reduced = 0.7923456275243485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/sklearn/utils/__init__.py:93: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced shape (891, 14) (418, 14)\n",
      "*** retrain after reducing***\n",
      "0.8024519937144188\n",
      "Cross-validation of : <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "CV score = 0.7923897088680811\n",
      "**********\n",
      "Cross-validation of : <class 'sklearn.linear_model.logistic.LogisticRegressionCV'>\n",
      "CV score = 0.796928385016315\n",
      "**********\n",
      "Cross-validation of : <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "CV score = 0.8171471107569472\n",
      "**********\n",
      "Cross-validation of : <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>\n",
      "CV score = 0.8036698871226798\n",
      "**********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cb21e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## array([0.81564246, 0.80446927, 0.80898876, 0.79213483, 0.8079096 ])\n",
    "# clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, min_samples_leaf=100, max_depth=2, random_state=random_state)\n",
    "\n",
    "## array([0.79888268, 0.81564246, 0.81460674, 0.80337079, 0.85875706])\n",
    "clf = RandomForestClassifier(max_depth=4, max_features=3, n_estimators=400)\n",
    "\n",
    "## array([0.75977654, 0.75977654, 0.7752809 , 0.76966292, 0.82485876])\n",
    "# clf = VotingClassifier(\n",
    "#     estimators=[\n",
    "#         ('lr', LogisticRegression()),\n",
    "#         ('svc', SVC()),\n",
    "#         ('tree', DecisionTreeClassifier(max_depth=2, max_features=3)),\n",
    "#         ('sgd', SGDClassifier()),\n",
    "#         ('linear_svc', LinearSVC()),\n",
    "#         ('gaussian', GaussianNB()),\n",
    "#         ('knn', KNeighborsClassifier(n_neighbors = 2))\n",
    "#     ],\n",
    "#     voting='hard',\n",
    "# )\n",
    "\n",
    "## array([0.82122905, 0.80446927, 0.79775281, 0.75842697, 0.8079096 ])\n",
    "# clf = AdaBoostClassifier(\n",
    "#     DecisionTreeClassifier(max_depth=1), n_estimators=300,\n",
    "#     algorithm='SAMME', learning_rate=0.2\n",
    "# )\n",
    "\n",
    "# clf = DecisionTreeClassifier()\n",
    "# cv = cross_val_score(clf, train_df, y, cv=5)\n",
    "# print(cv)\n",
    "clf.fit(train_df, y)\n",
    "# print(\"*\"*40, \"running grid search\")\n",
    "# param_test = {'n_estimators': np.linspace(100,500,5, dtype=np.int),\n",
    "#               'max_depth': [1,2,3,4,6,8],\n",
    "#               'max_features': ['sqrt', 'auto', 'log2'],\n",
    "#               'min_samples_split': [2, 3, 10],\n",
    "#               'min_samples_leaf': [1, 3, 10],\n",
    "#               'bootstrap': [True, False],\n",
    "#              }\n",
    "# gs = GridSearchCV(estimator=clf, param_grid=param_test, cv=5, n_jobs=5, verbose=1)\n",
    "# gs.fit(train_df, y)\n",
    "# print(\"*\"*40, \"best params\", gs.best_params_, gs.best_score_)\n",
    "# clf.fit(train_df, y)\n",
    "cv = np.mean(cross_val_score(clf, train_df, y, cv=5))\n",
    "print(f\"** before reduced = {cv}\")\n",
    "features = pd.DataFrame()\n",
    "features['feature'] = train_df.columns\n",
    "features['importance'] = clf.feature_importances_\n",
    "features.sort_values(by=['importance'], ascending=True, inplace=True)\n",
    "features.set_index('feature', inplace=True)\n",
    "features.plot(kind='barh', figsize=(25, 25))\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "train_reduced = model.transform(train_df)\n",
    "test_reduced = model.transform(test_df)\n",
    "print(\"reduced shape\", train_reduced.shape, test_reduced.shape)\n",
    "print(\"*** retrain after reducing***\")\n",
    "print(np.mean(cross_val_score(clf, train_reduced, y, cv=5)))\n",
    "clf.fit(train_reduced, y)\n",
    "logreg = LogisticRegression()\n",
    "logreg_cv = LogisticRegressionCV()\n",
    "rf = RandomForestClassifier()\n",
    "gboost = GradientBoostingClassifier()\n",
    "\n",
    "models = [logreg, logreg_cv, rf, gboost]\n",
    "for m in models:\n",
    "    print('Cross-validation of : {0}'.format(m.__class__))\n",
    "    score = np.mean(cross_val_score(m, X=train_reduced, y=y, scoring='accuracy', cv=5))\n",
    "    print('CV score = {0}'.format(score))\n",
    "    print('*'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:29:14.890139Z",
     "start_time": "2018-09-24T08:02:31.531399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************** running grid search\n",
      "Fitting 5 folds for each of 3240 candidates, totalling 16200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:   42.4s\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=5)]: Done 1790 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=5)]: Done 2440 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=5)]: Done 3190 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=5)]: Done 4040 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=5)]: Done 4990 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=5)]: Done 6040 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=5)]: Done 7190 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=5)]: Done 8440 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=5)]: Done 9790 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=5)]: Done 11240 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=5)]: Done 12790 tasks      | elapsed: 21.4min\n",
      "[Parallel(n_jobs=5)]: Done 14440 tasks      | elapsed: 23.9min\n",
      "[Parallel(n_jobs=5)]: Done 16190 tasks      | elapsed: 26.7min\n",
      "[Parallel(n_jobs=5)]: Done 16200 out of 16200 | elapsed: 26.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************** best params {'bootstrap': True, 'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200} 0.8148148148148148\n"
     ]
    }
   ],
   "source": [
    "print(\"*\"*40, \"running grid search\")\n",
    "param_test = {'n_estimators': np.linspace(100,500,5, dtype=np.int),\n",
    "              'max_depth': [1,2,3,4,6,8],\n",
    "              'max_features': ['sqrt', 'auto', 'log2', 2, 3, 4],\n",
    "              'min_samples_split': [2, 3, 10],\n",
    "              'min_samples_leaf': [1, 3, 10],\n",
    "              'bootstrap': [True, False],\n",
    "             }\n",
    "gs = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_test, cv=5, n_jobs=5, verbose=1)\n",
    "gs.fit(train_reduced, y)\n",
    "print(\"*\"*40, \"best params\", gs.best_params_, gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T07:14:45.501102Z",
     "start_time": "2018-09-24T06:39:03.922274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3600 candidates, totalling 18000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done 175 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=5)]: Done 625 tasks      | elapsed:   25.0s\n",
      "[Parallel(n_jobs=5)]: Done 1017 tasks      | elapsed:   46.6s\n",
      "[Parallel(n_jobs=5)]: Done 1367 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=5)]: Done 1817 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=5)]: Done 2367 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=5)]: Done 3017 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=5)]: Done 3767 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=5)]: Done 4617 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=5)]: Done 5567 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=5)]: Done 6617 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=5)]: Done 7767 tasks      | elapsed: 15.1min\n",
      "[Parallel(n_jobs=5)]: Done 9017 tasks      | elapsed: 16.7min\n",
      "[Parallel(n_jobs=5)]: Done 10367 tasks      | elapsed: 19.9min\n",
      "[Parallel(n_jobs=5)]: Done 11817 tasks      | elapsed: 22.3min\n",
      "[Parallel(n_jobs=5)]: Done 13367 tasks      | elapsed: 24.8min\n",
      "[Parallel(n_jobs=5)]: Done 15017 tasks      | elapsed: 28.6min\n",
      "[Parallel(n_jobs=5)]: Done 16767 tasks      | elapsed: 31.4min\n",
      "[Parallel(n_jobs=5)]: Done 18000 out of 18000 | elapsed: 35.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=5,\n",
       "       param_grid={'learning_rate': array([0.1, 0.2, 0.3, 0.4, 0.5]), 'n_estimators': array([ 100,  212,  325,  437,  550,  662,  775,  887, 1000]), 'max_depth': array([1, 2, 3, 4, 5]), 'min_samples_split': [2, 5, 10, 20], 'max_features': [2, 4, 6, 8]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test = {\n",
    "    'learning_rate': np.linspace(0.1, 0.5, 5),\n",
    "    'n_estimators': np.linspace(100, 1000, 9, dtype=np.int),\n",
    "    'max_depth': np.linspace(1, 5, 5, dtype=np.int),\n",
    "    'min_samples_split': [2,5,10,20],\n",
    "    'max_features': [2,4,6,8]\n",
    "}\n",
    "gs = GridSearchCV(estimator=GradientBoostingClassifier(), param_grid=param_test, cv=5, n_jobs=5, verbose=1)\n",
    "gs.fit(train_reduced, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T03:58:02.373900Z",
     "start_time": "2018-09-24T03:57:27.405Z"
    }
   },
   "outputs": [],
   "source": [
    "gs.best_params_, gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T09:13:49.527300Z",
     "start_time": "2018-09-24T09:13:48.167802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** best score for random forest = 0.8025338793320204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=6, max_features=3, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=3, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_forest = RandomForestClassifier(**{'bootstrap': False, 'max_depth': 6, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 100})\n",
    "score = np.mean(cross_val_score(best_forest, train_reduced, y, cv=5))\n",
    "print(f\"** best score for random forest = {score}\")\n",
    "best_forest.fit(train_reduced, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T09:35:36.158814Z",
     "start_time": "2018-09-24T09:35:33.144719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** score = 0.7844803171303365\n"
     ]
    }
   ],
   "source": [
    "ada_boosting = AdaBoostClassifier(n_estimators=500, learning_rate=0.1, algorithm='SAMME', random_state=random_state)\n",
    "score = np.mean(cross_val_score(ada_boosting, train_reduced, y, cv=5))\n",
    "print(f\"** score = {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T06:38:41.024658Z",
     "start_time": "2018-09-24T06:38:39.833955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** score = 0.81939423084084\n"
     ]
    }
   ],
   "source": [
    "gradient_boosting = GradientBoostingClassifier(\n",
    "    learning_rate=0.5,\n",
    "    n_estimators=437, \n",
    "    max_depth= 2,\n",
    "    min_samples_leaf= 2,\n",
    "    min_samples_split=2)\n",
    "score = np.mean(cross_val_score(gradient_boosting, train_reduced, y, cv=5))\n",
    "gradient_boosting.fit(train_reduced, y)\n",
    "print(f\"** score = {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T06:28:29.698641Z",
     "start_time": "2018-09-24T06:28:29.664494Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = best_forest.predict(test_reduced)\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": test_ids,\n",
    "    \"Survived\": y_pred\n",
    "})\n",
    "submission.to_csv('titanic_result_feature_engineer_reduced_random_forest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T06:28:49.681804Z",
     "start_time": "2018-09-24T06:28:44.861953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by otherusers on this system! To fix this, you can run'chmod 600 /Users/kdang/.kaggle/kaggle.json'\n",
      "Successfully submitted to Titanic: Machine Learning from Disaster"
     ]
    }
   ],
   "source": [
    "! kaggle competitions submit titanic -f titanic_result_feature_engineer_reduced_random_forest.csv -m \"feature engineer + grid search for random forest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T04:29:42.388511Z",
     "start_time": "2018-09-24T04:29:42.344004Z"
    }
   },
   "outputs": [],
   "source": [
    "# try using stacking\n",
    "from mlxtend.classifier import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T07:48:03.184053Z",
     "start_time": "2018-09-24T07:47:57.010174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8171470398296041"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = KNeighborsClassifier(n_neighbors=2)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[best_forest, ada_boosting, gradient_boosting, clf1, clf3], \n",
    "                          meta_classifier=lr)\n",
    "np.mean(cross_val_score(sclf, train_reduced, y, cv=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T07:57:28.036289Z",
     "start_time": "2018-09-24T07:57:28.032290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reduced.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:02:18.986796Z",
     "start_time": "2018-09-24T08:00:53.707652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 712 samples, validate on 179 samples\n",
      "Epoch 1/1000\n",
      "712/712 [==============================] - 1s 971us/step - loss: 0.6998 - acc: 0.5084 - val_loss: 0.6752 - val_acc: 0.7095\n",
      "Epoch 2/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6893 - acc: 0.5478 - val_loss: 0.6751 - val_acc: 0.7095\n",
      "Epoch 3/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6964 - acc: 0.5028 - val_loss: 0.6751 - val_acc: 0.7095\n",
      "Epoch 4/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6903 - acc: 0.5295 - val_loss: 0.6751 - val_acc: 0.7095\n",
      "Epoch 5/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6889 - acc: 0.5520 - val_loss: 0.6751 - val_acc: 0.7095\n",
      "Epoch 6/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6967 - acc: 0.5098 - val_loss: 0.6751 - val_acc: 0.7095\n",
      "Epoch 7/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6919 - acc: 0.5281 - val_loss: 0.6751 - val_acc: 0.7095\n",
      "Epoch 8/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6953 - acc: 0.5154 - val_loss: 0.6750 - val_acc: 0.7095\n",
      "Epoch 9/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6930 - acc: 0.5239 - val_loss: 0.6750 - val_acc: 0.7095\n",
      "Epoch 10/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6935 - acc: 0.5295 - val_loss: 0.6750 - val_acc: 0.7095\n",
      "Epoch 11/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6908 - acc: 0.5028 - val_loss: 0.6750 - val_acc: 0.7095\n",
      "Epoch 12/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6877 - acc: 0.5393 - val_loss: 0.6750 - val_acc: 0.7095\n",
      "Epoch 13/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6969 - acc: 0.5098 - val_loss: 0.6749 - val_acc: 0.7095\n",
      "Epoch 14/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6940 - acc: 0.5309 - val_loss: 0.6749 - val_acc: 0.7095\n",
      "Epoch 15/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6845 - acc: 0.5646 - val_loss: 0.6749 - val_acc: 0.7095\n",
      "Epoch 16/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6863 - acc: 0.5646 - val_loss: 0.6749 - val_acc: 0.7095\n",
      "Epoch 17/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6926 - acc: 0.5365 - val_loss: 0.6749 - val_acc: 0.7095\n",
      "Epoch 18/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6955 - acc: 0.5379 - val_loss: 0.6749 - val_acc: 0.7095\n",
      "Epoch 19/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6871 - acc: 0.5492 - val_loss: 0.6748 - val_acc: 0.7095\n",
      "Epoch 20/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6883 - acc: 0.5506 - val_loss: 0.6748 - val_acc: 0.6983\n",
      "Epoch 21/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6934 - acc: 0.5070 - val_loss: 0.6748 - val_acc: 0.6983\n",
      "Epoch 22/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6989 - acc: 0.5056 - val_loss: 0.6748 - val_acc: 0.6983\n",
      "Epoch 23/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6856 - acc: 0.5379 - val_loss: 0.6748 - val_acc: 0.6983\n",
      "Epoch 24/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6954 - acc: 0.5028 - val_loss: 0.6747 - val_acc: 0.6983\n",
      "Epoch 25/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6930 - acc: 0.5084 - val_loss: 0.6747 - val_acc: 0.6983\n",
      "Epoch 26/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6911 - acc: 0.5323 - val_loss: 0.6747 - val_acc: 0.6983\n",
      "Epoch 27/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6905 - acc: 0.5309 - val_loss: 0.6747 - val_acc: 0.6983\n",
      "Epoch 28/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6875 - acc: 0.5407 - val_loss: 0.6747 - val_acc: 0.6983\n",
      "Epoch 29/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6977 - acc: 0.5084 - val_loss: 0.6747 - val_acc: 0.6983\n",
      "Epoch 30/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6911 - acc: 0.5337 - val_loss: 0.6746 - val_acc: 0.6983\n",
      "Epoch 31/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6954 - acc: 0.5267 - val_loss: 0.6746 - val_acc: 0.6983\n",
      "Epoch 32/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6900 - acc: 0.5267 - val_loss: 0.6746 - val_acc: 0.6983\n",
      "Epoch 33/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6888 - acc: 0.5548 - val_loss: 0.6746 - val_acc: 0.6983\n",
      "Epoch 34/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6942 - acc: 0.5154 - val_loss: 0.6746 - val_acc: 0.6983\n",
      "Epoch 35/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6962 - acc: 0.5183 - val_loss: 0.6746 - val_acc: 0.6983\n",
      "Epoch 36/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6873 - acc: 0.5730 - val_loss: 0.6745 - val_acc: 0.6927\n",
      "Epoch 37/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6954 - acc: 0.5098 - val_loss: 0.6745 - val_acc: 0.6927\n",
      "Epoch 38/1000\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.6927 - acc: 0.5309 - val_loss: 0.6745 - val_acc: 0.6927\n",
      "Epoch 39/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6903 - acc: 0.5140 - val_loss: 0.6745 - val_acc: 0.6927\n",
      "Epoch 40/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6928 - acc: 0.5140 - val_loss: 0.6745 - val_acc: 0.6927\n",
      "Epoch 41/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6949 - acc: 0.5239 - val_loss: 0.6744 - val_acc: 0.6927\n",
      "Epoch 42/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6844 - acc: 0.5463 - val_loss: 0.6744 - val_acc: 0.6927\n",
      "Epoch 43/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6926 - acc: 0.5169 - val_loss: 0.6744 - val_acc: 0.6927\n",
      "Epoch 44/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6939 - acc: 0.5211 - val_loss: 0.6744 - val_acc: 0.6927\n",
      "Epoch 45/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6953 - acc: 0.5112 - val_loss: 0.6744 - val_acc: 0.6927\n",
      "Epoch 46/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6987 - acc: 0.5056 - val_loss: 0.6744 - val_acc: 0.6927\n",
      "Epoch 47/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6913 - acc: 0.5197 - val_loss: 0.6743 - val_acc: 0.6927\n",
      "Epoch 48/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6834 - acc: 0.5590 - val_loss: 0.6743 - val_acc: 0.6927\n",
      "Epoch 49/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6857 - acc: 0.5548 - val_loss: 0.6743 - val_acc: 0.6927\n",
      "Epoch 50/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6908 - acc: 0.5253 - val_loss: 0.6743 - val_acc: 0.6927\n",
      "Epoch 51/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6932 - acc: 0.5337 - val_loss: 0.6743 - val_acc: 0.6927\n",
      "Epoch 52/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6862 - acc: 0.5421 - val_loss: 0.6743 - val_acc: 0.6927\n",
      "Epoch 53/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6937 - acc: 0.5295 - val_loss: 0.6742 - val_acc: 0.6927\n",
      "Epoch 54/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6873 - acc: 0.5393 - val_loss: 0.6742 - val_acc: 0.6927\n",
      "Epoch 55/1000\n",
      "712/712 [==============================] - 0s 108us/step - loss: 0.6879 - acc: 0.5295 - val_loss: 0.6742 - val_acc: 0.6927\n",
      "Epoch 56/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6854 - acc: 0.5365 - val_loss: 0.6742 - val_acc: 0.6927\n",
      "Epoch 57/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6922 - acc: 0.5253 - val_loss: 0.6742 - val_acc: 0.6927\n",
      "Epoch 58/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6921 - acc: 0.5225 - val_loss: 0.6741 - val_acc: 0.6927\n",
      "Epoch 59/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6891 - acc: 0.5295 - val_loss: 0.6741 - val_acc: 0.6927\n",
      "Epoch 60/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6932 - acc: 0.5239 - val_loss: 0.6741 - val_acc: 0.6927\n",
      "Epoch 61/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6879 - acc: 0.5295 - val_loss: 0.6741 - val_acc: 0.6927\n",
      "Epoch 62/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6863 - acc: 0.5421 - val_loss: 0.6741 - val_acc: 0.6927\n",
      "Epoch 63/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6902 - acc: 0.5379 - val_loss: 0.6741 - val_acc: 0.6927\n",
      "Epoch 64/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6948 - acc: 0.4874 - val_loss: 0.6740 - val_acc: 0.6927\n",
      "Epoch 65/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6926 - acc: 0.5112 - val_loss: 0.6740 - val_acc: 0.6927\n",
      "Epoch 66/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6902 - acc: 0.5492 - val_loss: 0.6740 - val_acc: 0.6927\n",
      "Epoch 67/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6972 - acc: 0.5154 - val_loss: 0.6740 - val_acc: 0.6927\n",
      "Epoch 68/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6827 - acc: 0.5632 - val_loss: 0.6740 - val_acc: 0.6927\n",
      "Epoch 69/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6929 - acc: 0.5253 - val_loss: 0.6739 - val_acc: 0.6927\n",
      "Epoch 70/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6933 - acc: 0.5211 - val_loss: 0.6739 - val_acc: 0.6927\n",
      "Epoch 71/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6845 - acc: 0.5365 - val_loss: 0.6739 - val_acc: 0.6927\n",
      "Epoch 72/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6914 - acc: 0.5337 - val_loss: 0.6739 - val_acc: 0.6927\n",
      "Epoch 73/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6940 - acc: 0.5211 - val_loss: 0.6739 - val_acc: 0.6927\n",
      "Epoch 74/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6916 - acc: 0.5197 - val_loss: 0.6738 - val_acc: 0.6927\n",
      "Epoch 75/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6922 - acc: 0.5323 - val_loss: 0.6738 - val_acc: 0.6927\n",
      "Epoch 76/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6945 - acc: 0.5393 - val_loss: 0.6738 - val_acc: 0.6927\n",
      "Epoch 77/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6912 - acc: 0.5267 - val_loss: 0.6738 - val_acc: 0.6927\n",
      "Epoch 78/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6854 - acc: 0.5295 - val_loss: 0.6738 - val_acc: 0.6927\n",
      "Epoch 79/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6896 - acc: 0.5421 - val_loss: 0.6738 - val_acc: 0.6927\n",
      "Epoch 80/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6945 - acc: 0.5295 - val_loss: 0.6737 - val_acc: 0.6927\n",
      "Epoch 81/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6926 - acc: 0.5365 - val_loss: 0.6737 - val_acc: 0.6927\n",
      "Epoch 82/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6873 - acc: 0.5365 - val_loss: 0.6737 - val_acc: 0.6927\n",
      "Epoch 83/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6932 - acc: 0.4972 - val_loss: 0.6737 - val_acc: 0.6927\n",
      "Epoch 84/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6925 - acc: 0.5393 - val_loss: 0.6737 - val_acc: 0.6927\n",
      "Epoch 85/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6975 - acc: 0.5014 - val_loss: 0.6737 - val_acc: 0.6927\n",
      "Epoch 86/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6912 - acc: 0.5393 - val_loss: 0.6736 - val_acc: 0.6927\n",
      "Epoch 87/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6904 - acc: 0.5183 - val_loss: 0.6736 - val_acc: 0.6927\n",
      "Epoch 88/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6924 - acc: 0.5393 - val_loss: 0.6736 - val_acc: 0.6927\n",
      "Epoch 89/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6917 - acc: 0.5449 - val_loss: 0.6736 - val_acc: 0.6927\n",
      "Epoch 90/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6866 - acc: 0.5421 - val_loss: 0.6736 - val_acc: 0.6927\n",
      "Epoch 91/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6920 - acc: 0.5197 - val_loss: 0.6736 - val_acc: 0.6927\n",
      "Epoch 92/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6855 - acc: 0.5646 - val_loss: 0.6735 - val_acc: 0.6927\n",
      "Epoch 93/1000\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.6876 - acc: 0.5323 - val_loss: 0.6735 - val_acc: 0.6927\n",
      "Epoch 94/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6930 - acc: 0.5295 - val_loss: 0.6735 - val_acc: 0.6927\n",
      "Epoch 95/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6850 - acc: 0.5716 - val_loss: 0.6735 - val_acc: 0.6927\n",
      "Epoch 96/1000\n",
      "712/712 [==============================] - 0s 177us/step - loss: 0.6876 - acc: 0.5337 - val_loss: 0.6735 - val_acc: 0.6927\n",
      "Epoch 97/1000\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.6836 - acc: 0.5534 - val_loss: 0.6734 - val_acc: 0.6927\n",
      "Epoch 98/1000\n",
      "712/712 [==============================] - 0s 162us/step - loss: 0.6916 - acc: 0.5281 - val_loss: 0.6734 - val_acc: 0.6927\n",
      "Epoch 99/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6875 - acc: 0.5688 - val_loss: 0.6734 - val_acc: 0.6927\n",
      "Epoch 100/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6870 - acc: 0.5337 - val_loss: 0.6734 - val_acc: 0.6927\n",
      "Epoch 101/1000\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.6850 - acc: 0.5520 - val_loss: 0.6734 - val_acc: 0.6927\n",
      "Epoch 102/1000\n",
      "712/712 [==============================] - 0s 162us/step - loss: 0.6905 - acc: 0.5239 - val_loss: 0.6734 - val_acc: 0.6927\n",
      "Epoch 103/1000\n",
      "712/712 [==============================] - 0s 183us/step - loss: 0.6873 - acc: 0.5492 - val_loss: 0.6733 - val_acc: 0.6927\n",
      "Epoch 104/1000\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.6895 - acc: 0.5337 - val_loss: 0.6733 - val_acc: 0.6927\n",
      "Epoch 105/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6931 - acc: 0.5478 - val_loss: 0.6733 - val_acc: 0.6927\n",
      "Epoch 106/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6933 - acc: 0.5197 - val_loss: 0.6733 - val_acc: 0.6927\n",
      "Epoch 107/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6932 - acc: 0.5225 - val_loss: 0.6733 - val_acc: 0.6927\n",
      "Epoch 108/1000\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.6934 - acc: 0.5407 - val_loss: 0.6732 - val_acc: 0.6927\n",
      "Epoch 109/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6927 - acc: 0.5365 - val_loss: 0.6732 - val_acc: 0.6927\n",
      "Epoch 110/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6929 - acc: 0.5042 - val_loss: 0.6732 - val_acc: 0.6927\n",
      "Epoch 111/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6884 - acc: 0.5337 - val_loss: 0.6732 - val_acc: 0.6927\n",
      "Epoch 112/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6908 - acc: 0.5520 - val_loss: 0.6732 - val_acc: 0.6927\n",
      "Epoch 113/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6932 - acc: 0.5309 - val_loss: 0.6732 - val_acc: 0.6927\n",
      "Epoch 114/1000\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.6857 - acc: 0.5449 - val_loss: 0.6731 - val_acc: 0.6927\n",
      "Epoch 115/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6894 - acc: 0.5323 - val_loss: 0.6731 - val_acc: 0.6927\n",
      "Epoch 116/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6892 - acc: 0.5281 - val_loss: 0.6731 - val_acc: 0.6927\n",
      "Epoch 117/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6890 - acc: 0.5351 - val_loss: 0.6731 - val_acc: 0.6927\n",
      "Epoch 118/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6887 - acc: 0.5169 - val_loss: 0.6731 - val_acc: 0.6927\n",
      "Epoch 119/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6982 - acc: 0.4860 - val_loss: 0.6731 - val_acc: 0.6927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6967 - acc: 0.5154 - val_loss: 0.6730 - val_acc: 0.6927\n",
      "Epoch 121/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6897 - acc: 0.5351 - val_loss: 0.6730 - val_acc: 0.6927\n",
      "Epoch 122/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6908 - acc: 0.5407 - val_loss: 0.6730 - val_acc: 0.6927\n",
      "Epoch 123/1000\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.6868 - acc: 0.5548 - val_loss: 0.6730 - val_acc: 0.6927\n",
      "Epoch 124/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6871 - acc: 0.5323 - val_loss: 0.6730 - val_acc: 0.6927\n",
      "Epoch 125/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6915 - acc: 0.5421 - val_loss: 0.6729 - val_acc: 0.6927\n",
      "Epoch 126/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6932 - acc: 0.5379 - val_loss: 0.6729 - val_acc: 0.6927\n",
      "Epoch 127/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6850 - acc: 0.5449 - val_loss: 0.6729 - val_acc: 0.6927\n",
      "Epoch 128/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6891 - acc: 0.5463 - val_loss: 0.6729 - val_acc: 0.6927\n",
      "Epoch 129/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6837 - acc: 0.5590 - val_loss: 0.6729 - val_acc: 0.6927\n",
      "Epoch 130/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6929 - acc: 0.5267 - val_loss: 0.6729 - val_acc: 0.6927\n",
      "Epoch 131/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6946 - acc: 0.5393 - val_loss: 0.6728 - val_acc: 0.6927\n",
      "Epoch 132/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6911 - acc: 0.5084 - val_loss: 0.6728 - val_acc: 0.6927\n",
      "Epoch 133/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6917 - acc: 0.5281 - val_loss: 0.6728 - val_acc: 0.6927\n",
      "Epoch 134/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6849 - acc: 0.5506 - val_loss: 0.6728 - val_acc: 0.6927\n",
      "Epoch 135/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6950 - acc: 0.5225 - val_loss: 0.6728 - val_acc: 0.6927\n",
      "Epoch 136/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6905 - acc: 0.5548 - val_loss: 0.6728 - val_acc: 0.6927\n",
      "Epoch 137/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6947 - acc: 0.5183 - val_loss: 0.6727 - val_acc: 0.6927\n",
      "Epoch 138/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6867 - acc: 0.5421 - val_loss: 0.6727 - val_acc: 0.6927\n",
      "Epoch 139/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6916 - acc: 0.4972 - val_loss: 0.6727 - val_acc: 0.6927\n",
      "Epoch 140/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6914 - acc: 0.5407 - val_loss: 0.6727 - val_acc: 0.6927\n",
      "Epoch 141/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6929 - acc: 0.5365 - val_loss: 0.6727 - val_acc: 0.6927\n",
      "Epoch 142/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6852 - acc: 0.5463 - val_loss: 0.6727 - val_acc: 0.6927\n",
      "Epoch 143/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6957 - acc: 0.5295 - val_loss: 0.6726 - val_acc: 0.6927\n",
      "Epoch 144/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6954 - acc: 0.5140 - val_loss: 0.6726 - val_acc: 0.6927\n",
      "Epoch 145/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6861 - acc: 0.5449 - val_loss: 0.6726 - val_acc: 0.6927\n",
      "Epoch 146/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6838 - acc: 0.5379 - val_loss: 0.6726 - val_acc: 0.6927\n",
      "Epoch 147/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6938 - acc: 0.5183 - val_loss: 0.6726 - val_acc: 0.6927\n",
      "Epoch 148/1000\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.6848 - acc: 0.5449 - val_loss: 0.6725 - val_acc: 0.6927\n",
      "Epoch 149/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6970 - acc: 0.5028 - val_loss: 0.6725 - val_acc: 0.6927\n",
      "Epoch 150/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6938 - acc: 0.5267 - val_loss: 0.6725 - val_acc: 0.6927\n",
      "Epoch 151/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6861 - acc: 0.5492 - val_loss: 0.6725 - val_acc: 0.6927\n",
      "Epoch 152/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6879 - acc: 0.5492 - val_loss: 0.6725 - val_acc: 0.6927\n",
      "Epoch 153/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6850 - acc: 0.5604 - val_loss: 0.6725 - val_acc: 0.6927\n",
      "Epoch 154/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6856 - acc: 0.5520 - val_loss: 0.6724 - val_acc: 0.6927\n",
      "Epoch 155/1000\n",
      "712/712 [==============================] - 0s 170us/step - loss: 0.6926 - acc: 0.5183 - val_loss: 0.6724 - val_acc: 0.6927\n",
      "Epoch 156/1000\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.6796 - acc: 0.5871 - val_loss: 0.6724 - val_acc: 0.6927\n",
      "Epoch 157/1000\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.6904 - acc: 0.5225 - val_loss: 0.6724 - val_acc: 0.6927\n",
      "Epoch 158/1000\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.6824 - acc: 0.5688 - val_loss: 0.6724 - val_acc: 0.6927\n",
      "Epoch 159/1000\n",
      "712/712 [==============================] - 0s 164us/step - loss: 0.6854 - acc: 0.5506 - val_loss: 0.6724 - val_acc: 0.6927\n",
      "Epoch 160/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6903 - acc: 0.5379 - val_loss: 0.6723 - val_acc: 0.6927\n",
      "Epoch 161/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6887 - acc: 0.5534 - val_loss: 0.6723 - val_acc: 0.6927\n",
      "Epoch 162/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6875 - acc: 0.5337 - val_loss: 0.6723 - val_acc: 0.6927\n",
      "Epoch 163/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6921 - acc: 0.5169 - val_loss: 0.6723 - val_acc: 0.6927\n",
      "Epoch 164/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6891 - acc: 0.5449 - val_loss: 0.6723 - val_acc: 0.6927\n",
      "Epoch 165/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6974 - acc: 0.5309 - val_loss: 0.6723 - val_acc: 0.6927\n",
      "Epoch 166/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6965 - acc: 0.4972 - val_loss: 0.6722 - val_acc: 0.6927\n",
      "Epoch 167/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6873 - acc: 0.5435 - val_loss: 0.6722 - val_acc: 0.6927\n",
      "Epoch 168/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6842 - acc: 0.5632 - val_loss: 0.6722 - val_acc: 0.6927\n",
      "Epoch 169/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6885 - acc: 0.5520 - val_loss: 0.6722 - val_acc: 0.6927\n",
      "Epoch 170/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6864 - acc: 0.5183 - val_loss: 0.6722 - val_acc: 0.6927\n",
      "Epoch 171/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6878 - acc: 0.5295 - val_loss: 0.6721 - val_acc: 0.6927\n",
      "Epoch 172/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6891 - acc: 0.5562 - val_loss: 0.6721 - val_acc: 0.6927\n",
      "Epoch 173/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6967 - acc: 0.5084 - val_loss: 0.6721 - val_acc: 0.6927\n",
      "Epoch 174/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6867 - acc: 0.5548 - val_loss: 0.6721 - val_acc: 0.6927\n",
      "Epoch 175/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6897 - acc: 0.5183 - val_loss: 0.6721 - val_acc: 0.6927\n",
      "Epoch 176/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6919 - acc: 0.5379 - val_loss: 0.6721 - val_acc: 0.6927\n",
      "Epoch 177/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6895 - acc: 0.5492 - val_loss: 0.6720 - val_acc: 0.6927\n",
      "Epoch 178/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6897 - acc: 0.5281 - val_loss: 0.6720 - val_acc: 0.6927\n",
      "Epoch 179/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6928 - acc: 0.5449 - val_loss: 0.6720 - val_acc: 0.6927\n",
      "Epoch 180/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6908 - acc: 0.5154 - val_loss: 0.6720 - val_acc: 0.6927\n",
      "Epoch 181/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6865 - acc: 0.5365 - val_loss: 0.6720 - val_acc: 0.6927\n",
      "Epoch 182/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6858 - acc: 0.5646 - val_loss: 0.6720 - val_acc: 0.6927\n",
      "Epoch 183/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6829 - acc: 0.5393 - val_loss: 0.6719 - val_acc: 0.6927\n",
      "Epoch 184/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6905 - acc: 0.5267 - val_loss: 0.6719 - val_acc: 0.6927\n",
      "Epoch 185/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6888 - acc: 0.5393 - val_loss: 0.6719 - val_acc: 0.6927\n",
      "Epoch 186/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6879 - acc: 0.5267 - val_loss: 0.6719 - val_acc: 0.6927\n",
      "Epoch 187/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6828 - acc: 0.5365 - val_loss: 0.6719 - val_acc: 0.6927\n",
      "Epoch 188/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6887 - acc: 0.5183 - val_loss: 0.6718 - val_acc: 0.6927\n",
      "Epoch 189/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6912 - acc: 0.5463 - val_loss: 0.6718 - val_acc: 0.6927\n",
      "Epoch 190/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6898 - acc: 0.5281 - val_loss: 0.6718 - val_acc: 0.6927\n",
      "Epoch 191/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6886 - acc: 0.5478 - val_loss: 0.6718 - val_acc: 0.6927\n",
      "Epoch 192/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6943 - acc: 0.5154 - val_loss: 0.6718 - val_acc: 0.6927\n",
      "Epoch 193/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6871 - acc: 0.5253 - val_loss: 0.6718 - val_acc: 0.6927\n",
      "Epoch 194/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6849 - acc: 0.5407 - val_loss: 0.6717 - val_acc: 0.6927\n",
      "Epoch 195/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6873 - acc: 0.5351 - val_loss: 0.6717 - val_acc: 0.6927\n",
      "Epoch 196/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6909 - acc: 0.5169 - val_loss: 0.6717 - val_acc: 0.6927\n",
      "Epoch 197/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6897 - acc: 0.5140 - val_loss: 0.6717 - val_acc: 0.6927\n",
      "Epoch 198/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6886 - acc: 0.5281 - val_loss: 0.6717 - val_acc: 0.6927\n",
      "Epoch 199/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6918 - acc: 0.5309 - val_loss: 0.6717 - val_acc: 0.6927\n",
      "Epoch 200/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6836 - acc: 0.5562 - val_loss: 0.6716 - val_acc: 0.6927\n",
      "Epoch 201/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6956 - acc: 0.5183 - val_loss: 0.6716 - val_acc: 0.6927\n",
      "Epoch 202/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6898 - acc: 0.5211 - val_loss: 0.6716 - val_acc: 0.6927\n",
      "Epoch 203/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6828 - acc: 0.5744 - val_loss: 0.6716 - val_acc: 0.6927\n",
      "Epoch 204/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6870 - acc: 0.5042 - val_loss: 0.6716 - val_acc: 0.6927\n",
      "Epoch 205/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6884 - acc: 0.5562 - val_loss: 0.6716 - val_acc: 0.6927\n",
      "Epoch 206/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6847 - acc: 0.5632 - val_loss: 0.6715 - val_acc: 0.6927\n",
      "Epoch 207/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6891 - acc: 0.5407 - val_loss: 0.6715 - val_acc: 0.6927\n",
      "Epoch 208/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6890 - acc: 0.5267 - val_loss: 0.6715 - val_acc: 0.6927\n",
      "Epoch 209/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6798 - acc: 0.5674 - val_loss: 0.6715 - val_acc: 0.6927\n",
      "Epoch 210/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6896 - acc: 0.5421 - val_loss: 0.6715 - val_acc: 0.6927\n",
      "Epoch 211/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6862 - acc: 0.5463 - val_loss: 0.6715 - val_acc: 0.6927\n",
      "Epoch 212/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6793 - acc: 0.5632 - val_loss: 0.6714 - val_acc: 0.6927\n",
      "Epoch 213/1000\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.6861 - acc: 0.5548 - val_loss: 0.6714 - val_acc: 0.6983\n",
      "Epoch 214/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6902 - acc: 0.5365 - val_loss: 0.6714 - val_acc: 0.6983\n",
      "Epoch 215/1000\n",
      "712/712 [==============================] - 0s 163us/step - loss: 0.6816 - acc: 0.5520 - val_loss: 0.6714 - val_acc: 0.6983\n",
      "Epoch 216/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6965 - acc: 0.5098 - val_loss: 0.6714 - val_acc: 0.6983\n",
      "Epoch 217/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6911 - acc: 0.5211 - val_loss: 0.6714 - val_acc: 0.6983\n",
      "Epoch 218/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6845 - acc: 0.5435 - val_loss: 0.6713 - val_acc: 0.6983\n",
      "Epoch 219/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6946 - acc: 0.5098 - val_loss: 0.6713 - val_acc: 0.6983\n",
      "Epoch 220/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6848 - acc: 0.5253 - val_loss: 0.6713 - val_acc: 0.6983\n",
      "Epoch 221/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6899 - acc: 0.5197 - val_loss: 0.6713 - val_acc: 0.6983\n",
      "Epoch 222/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6855 - acc: 0.5365 - val_loss: 0.6713 - val_acc: 0.6983\n",
      "Epoch 223/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6884 - acc: 0.5562 - val_loss: 0.6713 - val_acc: 0.6983\n",
      "Epoch 224/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6907 - acc: 0.5548 - val_loss: 0.6712 - val_acc: 0.6983\n",
      "Epoch 225/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6893 - acc: 0.5239 - val_loss: 0.6712 - val_acc: 0.6983\n",
      "Epoch 226/1000\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.6934 - acc: 0.5154 - val_loss: 0.6712 - val_acc: 0.6983\n",
      "Epoch 227/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6867 - acc: 0.5337 - val_loss: 0.6712 - val_acc: 0.6983\n",
      "Epoch 228/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6904 - acc: 0.5239 - val_loss: 0.6712 - val_acc: 0.6983\n",
      "Epoch 229/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6951 - acc: 0.5028 - val_loss: 0.6712 - val_acc: 0.6983\n",
      "Epoch 230/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6859 - acc: 0.5351 - val_loss: 0.6711 - val_acc: 0.6983\n",
      "Epoch 231/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6905 - acc: 0.5126 - val_loss: 0.6711 - val_acc: 0.6983\n",
      "Epoch 232/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6887 - acc: 0.5112 - val_loss: 0.6711 - val_acc: 0.6983\n",
      "Epoch 233/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6795 - acc: 0.5843 - val_loss: 0.6711 - val_acc: 0.6983\n",
      "Epoch 234/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6899 - acc: 0.5323 - val_loss: 0.6711 - val_acc: 0.6983\n",
      "Epoch 235/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6852 - acc: 0.5421 - val_loss: 0.6711 - val_acc: 0.6983\n",
      "Epoch 236/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6821 - acc: 0.5618 - val_loss: 0.6710 - val_acc: 0.6983\n",
      "Epoch 237/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6856 - acc: 0.5407 - val_loss: 0.6710 - val_acc: 0.6983\n",
      "Epoch 238/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 138us/step - loss: 0.6839 - acc: 0.5435 - val_loss: 0.6710 - val_acc: 0.6983\n",
      "Epoch 239/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6899 - acc: 0.5421 - val_loss: 0.6710 - val_acc: 0.6983\n",
      "Epoch 240/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6904 - acc: 0.5604 - val_loss: 0.6710 - val_acc: 0.6983\n",
      "Epoch 241/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6839 - acc: 0.5744 - val_loss: 0.6710 - val_acc: 0.6983\n",
      "Epoch 242/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6950 - acc: 0.5183 - val_loss: 0.6709 - val_acc: 0.6983\n",
      "Epoch 243/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6897 - acc: 0.5309 - val_loss: 0.6709 - val_acc: 0.6983\n",
      "Epoch 244/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6763 - acc: 0.5702 - val_loss: 0.6709 - val_acc: 0.6983\n",
      "Epoch 245/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6813 - acc: 0.5674 - val_loss: 0.6709 - val_acc: 0.6983\n",
      "Epoch 246/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6751 - acc: 0.5913 - val_loss: 0.6709 - val_acc: 0.6983\n",
      "Epoch 247/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6895 - acc: 0.5225 - val_loss: 0.6708 - val_acc: 0.6983\n",
      "Epoch 248/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6849 - acc: 0.5449 - val_loss: 0.6708 - val_acc: 0.6983\n",
      "Epoch 249/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6923 - acc: 0.5421 - val_loss: 0.6708 - val_acc: 0.6983\n",
      "Epoch 250/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6949 - acc: 0.5014 - val_loss: 0.6708 - val_acc: 0.6983\n",
      "Epoch 251/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6861 - acc: 0.5323 - val_loss: 0.6708 - val_acc: 0.6983\n",
      "Epoch 252/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6866 - acc: 0.5407 - val_loss: 0.6708 - val_acc: 0.6983\n",
      "Epoch 253/1000\n",
      "712/712 [==============================] - 0s 90us/step - loss: 0.6808 - acc: 0.5646 - val_loss: 0.6707 - val_acc: 0.6983\n",
      "Epoch 254/1000\n",
      "712/712 [==============================] - 0s 76us/step - loss: 0.6847 - acc: 0.5506 - val_loss: 0.6707 - val_acc: 0.6983\n",
      "Epoch 255/1000\n",
      "712/712 [==============================] - 0s 91us/step - loss: 0.6947 - acc: 0.5183 - val_loss: 0.6707 - val_acc: 0.6983\n",
      "Epoch 256/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6868 - acc: 0.5449 - val_loss: 0.6707 - val_acc: 0.6983\n",
      "Epoch 257/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6839 - acc: 0.5632 - val_loss: 0.6707 - val_acc: 0.6983\n",
      "Epoch 258/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6804 - acc: 0.5478 - val_loss: 0.6707 - val_acc: 0.6983\n",
      "Epoch 259/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6867 - acc: 0.5604 - val_loss: 0.6706 - val_acc: 0.6983\n",
      "Epoch 260/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6914 - acc: 0.5337 - val_loss: 0.6706 - val_acc: 0.6983\n",
      "Epoch 261/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6883 - acc: 0.5253 - val_loss: 0.6706 - val_acc: 0.6983\n",
      "Epoch 262/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6802 - acc: 0.5787 - val_loss: 0.6706 - val_acc: 0.6983\n",
      "Epoch 263/1000\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.6936 - acc: 0.5183 - val_loss: 0.6706 - val_acc: 0.6983\n",
      "Epoch 264/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6779 - acc: 0.5646 - val_loss: 0.6706 - val_acc: 0.6704\n",
      "Epoch 265/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6845 - acc: 0.5393 - val_loss: 0.6705 - val_acc: 0.6704\n",
      "Epoch 266/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6898 - acc: 0.5154 - val_loss: 0.6705 - val_acc: 0.6704\n",
      "Epoch 267/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6899 - acc: 0.5323 - val_loss: 0.6705 - val_acc: 0.6704\n",
      "Epoch 268/1000\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.6868 - acc: 0.5407 - val_loss: 0.6705 - val_acc: 0.6704\n",
      "Epoch 269/1000\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.6911 - acc: 0.5197 - val_loss: 0.6705 - val_acc: 0.6704\n",
      "Epoch 270/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6960 - acc: 0.4986 - val_loss: 0.6705 - val_acc: 0.6704\n",
      "Epoch 271/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6875 - acc: 0.5379 - val_loss: 0.6704 - val_acc: 0.6704\n",
      "Epoch 272/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6906 - acc: 0.5253 - val_loss: 0.6704 - val_acc: 0.6704\n",
      "Epoch 273/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6819 - acc: 0.5730 - val_loss: 0.6704 - val_acc: 0.6704\n",
      "Epoch 274/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6916 - acc: 0.5323 - val_loss: 0.6704 - val_acc: 0.6704\n",
      "Epoch 275/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6843 - acc: 0.5520 - val_loss: 0.6704 - val_acc: 0.6704\n",
      "Epoch 276/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6878 - acc: 0.5365 - val_loss: 0.6704 - val_acc: 0.6704\n",
      "Epoch 277/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6904 - acc: 0.5253 - val_loss: 0.6703 - val_acc: 0.6704\n",
      "Epoch 278/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6937 - acc: 0.5169 - val_loss: 0.6703 - val_acc: 0.6704\n",
      "Epoch 279/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6812 - acc: 0.5843 - val_loss: 0.6703 - val_acc: 0.6704\n",
      "Epoch 280/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6955 - acc: 0.5239 - val_loss: 0.6703 - val_acc: 0.6704\n",
      "Epoch 281/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6906 - acc: 0.5154 - val_loss: 0.6703 - val_acc: 0.6704\n",
      "Epoch 282/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6821 - acc: 0.5590 - val_loss: 0.6703 - val_acc: 0.6704\n",
      "Epoch 283/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6879 - acc: 0.5323 - val_loss: 0.6702 - val_acc: 0.6704\n",
      "Epoch 284/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6943 - acc: 0.5281 - val_loss: 0.6702 - val_acc: 0.6704\n",
      "Epoch 285/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6836 - acc: 0.5534 - val_loss: 0.6702 - val_acc: 0.6704\n",
      "Epoch 286/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6862 - acc: 0.5660 - val_loss: 0.6702 - val_acc: 0.6704\n",
      "Epoch 287/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6856 - acc: 0.5379 - val_loss: 0.6702 - val_acc: 0.6704\n",
      "Epoch 288/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6863 - acc: 0.5576 - val_loss: 0.6702 - val_acc: 0.6704\n",
      "Epoch 289/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6879 - acc: 0.5393 - val_loss: 0.6701 - val_acc: 0.6704\n",
      "Epoch 290/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6869 - acc: 0.5674 - val_loss: 0.6701 - val_acc: 0.6704\n",
      "Epoch 291/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6997 - acc: 0.4944 - val_loss: 0.6701 - val_acc: 0.6704\n",
      "Epoch 292/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6906 - acc: 0.5197 - val_loss: 0.6701 - val_acc: 0.6704\n",
      "Epoch 293/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6851 - acc: 0.5393 - val_loss: 0.6701 - val_acc: 0.6704\n",
      "Epoch 294/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6835 - acc: 0.5449 - val_loss: 0.6700 - val_acc: 0.6704\n",
      "Epoch 295/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6865 - acc: 0.5478 - val_loss: 0.6700 - val_acc: 0.6704\n",
      "Epoch 296/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6831 - acc: 0.5379 - val_loss: 0.6700 - val_acc: 0.6704\n",
      "Epoch 297/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6913 - acc: 0.5140 - val_loss: 0.6700 - val_acc: 0.6704\n",
      "Epoch 298/1000\n",
      "712/712 [==============================] - 0s 120us/step - loss: 0.6839 - acc: 0.5772 - val_loss: 0.6700 - val_acc: 0.6704\n",
      "Epoch 299/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6818 - acc: 0.5590 - val_loss: 0.6700 - val_acc: 0.6704\n",
      "Epoch 300/1000\n",
      "712/712 [==============================] - 0s 109us/step - loss: 0.6878 - acc: 0.5295 - val_loss: 0.6699 - val_acc: 0.6704\n",
      "Epoch 301/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6926 - acc: 0.5140 - val_loss: 0.6699 - val_acc: 0.6704\n",
      "Epoch 302/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6891 - acc: 0.5183 - val_loss: 0.6699 - val_acc: 0.6704\n",
      "Epoch 303/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6868 - acc: 0.5351 - val_loss: 0.6699 - val_acc: 0.6704\n",
      "Epoch 304/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6887 - acc: 0.5323 - val_loss: 0.6699 - val_acc: 0.6704\n",
      "Epoch 305/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6894 - acc: 0.5197 - val_loss: 0.6699 - val_acc: 0.6704\n",
      "Epoch 306/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6929 - acc: 0.5154 - val_loss: 0.6698 - val_acc: 0.6704\n",
      "Epoch 307/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6879 - acc: 0.5478 - val_loss: 0.6698 - val_acc: 0.6704\n",
      "Epoch 308/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6948 - acc: 0.5028 - val_loss: 0.6698 - val_acc: 0.6704\n",
      "Epoch 309/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6936 - acc: 0.5225 - val_loss: 0.6698 - val_acc: 0.6704\n",
      "Epoch 310/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6872 - acc: 0.5478 - val_loss: 0.6698 - val_acc: 0.6704\n",
      "Epoch 311/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6879 - acc: 0.5183 - val_loss: 0.6698 - val_acc: 0.6704\n",
      "Epoch 312/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6909 - acc: 0.5197 - val_loss: 0.6697 - val_acc: 0.6704\n",
      "Epoch 313/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6822 - acc: 0.5716 - val_loss: 0.6697 - val_acc: 0.6704\n",
      "Epoch 314/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6881 - acc: 0.5435 - val_loss: 0.6697 - val_acc: 0.6704\n",
      "Epoch 315/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6890 - acc: 0.5281 - val_loss: 0.6697 - val_acc: 0.6704\n",
      "Epoch 316/1000\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.6919 - acc: 0.5309 - val_loss: 0.6697 - val_acc: 0.6704\n",
      "Epoch 317/1000\n",
      "712/712 [==============================] - 0s 162us/step - loss: 0.6833 - acc: 0.5590 - val_loss: 0.6697 - val_acc: 0.6704\n",
      "Epoch 318/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6925 - acc: 0.5337 - val_loss: 0.6696 - val_acc: 0.6704\n",
      "Epoch 319/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6783 - acc: 0.5604 - val_loss: 0.6696 - val_acc: 0.6704\n",
      "Epoch 320/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6758 - acc: 0.6025 - val_loss: 0.6696 - val_acc: 0.6704\n",
      "Epoch 321/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6925 - acc: 0.5379 - val_loss: 0.6696 - val_acc: 0.6704\n",
      "Epoch 322/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6956 - acc: 0.5112 - val_loss: 0.6696 - val_acc: 0.6704\n",
      "Epoch 323/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.7000 - acc: 0.5070 - val_loss: 0.6696 - val_acc: 0.6704\n",
      "Epoch 324/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6747 - acc: 0.5969 - val_loss: 0.6695 - val_acc: 0.6704\n",
      "Epoch 325/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6890 - acc: 0.5351 - val_loss: 0.6695 - val_acc: 0.6704\n",
      "Epoch 326/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6957 - acc: 0.5281 - val_loss: 0.6695 - val_acc: 0.6704\n",
      "Epoch 327/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6854 - acc: 0.5463 - val_loss: 0.6695 - val_acc: 0.6704\n",
      "Epoch 328/1000\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.6936 - acc: 0.5098 - val_loss: 0.6695 - val_acc: 0.6704\n",
      "Epoch 329/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6904 - acc: 0.5281 - val_loss: 0.6695 - val_acc: 0.6704\n",
      "Epoch 330/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6906 - acc: 0.5309 - val_loss: 0.6694 - val_acc: 0.6704\n",
      "Epoch 331/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6886 - acc: 0.5183 - val_loss: 0.6694 - val_acc: 0.6704\n",
      "Epoch 332/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6893 - acc: 0.5281 - val_loss: 0.6694 - val_acc: 0.6704\n",
      "Epoch 333/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6868 - acc: 0.5506 - val_loss: 0.6694 - val_acc: 0.6704\n",
      "Epoch 334/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6812 - acc: 0.5702 - val_loss: 0.6694 - val_acc: 0.6704\n",
      "Epoch 335/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6867 - acc: 0.5520 - val_loss: 0.6694 - val_acc: 0.6704\n",
      "Epoch 336/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6872 - acc: 0.5463 - val_loss: 0.6693 - val_acc: 0.6704\n",
      "Epoch 337/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6930 - acc: 0.5239 - val_loss: 0.6693 - val_acc: 0.6704\n",
      "Epoch 338/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6832 - acc: 0.5632 - val_loss: 0.6693 - val_acc: 0.6704\n",
      "Epoch 339/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6850 - acc: 0.5449 - val_loss: 0.6693 - val_acc: 0.6704\n",
      "Epoch 340/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6897 - acc: 0.5323 - val_loss: 0.6693 - val_acc: 0.6704\n",
      "Epoch 341/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6882 - acc: 0.5449 - val_loss: 0.6693 - val_acc: 0.6704\n",
      "Epoch 342/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6891 - acc: 0.5337 - val_loss: 0.6692 - val_acc: 0.6704\n",
      "Epoch 343/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6860 - acc: 0.5267 - val_loss: 0.6692 - val_acc: 0.6704\n",
      "Epoch 344/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6870 - acc: 0.5449 - val_loss: 0.6692 - val_acc: 0.6704\n",
      "Epoch 345/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6910 - acc: 0.5295 - val_loss: 0.6692 - val_acc: 0.6704\n",
      "Epoch 346/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6947 - acc: 0.5309 - val_loss: 0.6692 - val_acc: 0.6704\n",
      "Epoch 347/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6770 - acc: 0.5857 - val_loss: 0.6692 - val_acc: 0.6704\n",
      "Epoch 348/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6854 - acc: 0.5590 - val_loss: 0.6691 - val_acc: 0.6704\n",
      "Epoch 349/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6887 - acc: 0.5183 - val_loss: 0.6691 - val_acc: 0.6704\n",
      "Epoch 350/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6874 - acc: 0.5126 - val_loss: 0.6691 - val_acc: 0.6704\n",
      "Epoch 351/1000\n",
      "712/712 [==============================] - 0s 94us/step - loss: 0.6790 - acc: 0.5885 - val_loss: 0.6691 - val_acc: 0.6704\n",
      "Epoch 352/1000\n",
      "712/712 [==============================] - 0s 106us/step - loss: 0.6816 - acc: 0.5758 - val_loss: 0.6691 - val_acc: 0.6704\n",
      "Epoch 353/1000\n",
      "712/712 [==============================] - 0s 97us/step - loss: 0.6896 - acc: 0.5351 - val_loss: 0.6691 - val_acc: 0.6704\n",
      "Epoch 354/1000\n",
      "712/712 [==============================] - 0s 100us/step - loss: 0.6800 - acc: 0.5562 - val_loss: 0.6690 - val_acc: 0.6704\n",
      "Epoch 355/1000\n",
      "712/712 [==============================] - 0s 101us/step - loss: 0.6837 - acc: 0.5281 - val_loss: 0.6690 - val_acc: 0.6704\n",
      "Epoch 356/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 109us/step - loss: 0.6889 - acc: 0.5478 - val_loss: 0.6690 - val_acc: 0.6704\n",
      "Epoch 357/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6941 - acc: 0.5112 - val_loss: 0.6690 - val_acc: 0.6704\n",
      "Epoch 358/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6838 - acc: 0.5646 - val_loss: 0.6690 - val_acc: 0.6704\n",
      "Epoch 359/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6912 - acc: 0.5112 - val_loss: 0.6690 - val_acc: 0.6704\n",
      "Epoch 360/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6926 - acc: 0.5351 - val_loss: 0.6689 - val_acc: 0.6704\n",
      "Epoch 361/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6870 - acc: 0.5463 - val_loss: 0.6689 - val_acc: 0.6704\n",
      "Epoch 362/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6884 - acc: 0.5295 - val_loss: 0.6689 - val_acc: 0.6704\n",
      "Epoch 363/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6920 - acc: 0.5140 - val_loss: 0.6689 - val_acc: 0.6704\n",
      "Epoch 364/1000\n",
      "712/712 [==============================] - 0s 108us/step - loss: 0.6872 - acc: 0.5407 - val_loss: 0.6689 - val_acc: 0.6704\n",
      "Epoch 365/1000\n",
      "712/712 [==============================] - 0s 101us/step - loss: 0.6856 - acc: 0.5520 - val_loss: 0.6688 - val_acc: 0.6704\n",
      "Epoch 366/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6881 - acc: 0.5239 - val_loss: 0.6688 - val_acc: 0.6704\n",
      "Epoch 367/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6895 - acc: 0.5393 - val_loss: 0.6688 - val_acc: 0.6704\n",
      "Epoch 368/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6813 - acc: 0.5604 - val_loss: 0.6688 - val_acc: 0.6704\n",
      "Epoch 369/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6897 - acc: 0.5225 - val_loss: 0.6688 - val_acc: 0.6704\n",
      "Epoch 370/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6838 - acc: 0.5688 - val_loss: 0.6688 - val_acc: 0.6704\n",
      "Epoch 371/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6918 - acc: 0.5421 - val_loss: 0.6688 - val_acc: 0.6704\n",
      "Epoch 372/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6878 - acc: 0.5365 - val_loss: 0.6687 - val_acc: 0.6704\n",
      "Epoch 373/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6896 - acc: 0.5295 - val_loss: 0.6687 - val_acc: 0.6704\n",
      "Epoch 374/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6872 - acc: 0.5267 - val_loss: 0.6687 - val_acc: 0.6704\n",
      "Epoch 375/1000\n",
      "712/712 [==============================] - 0s 106us/step - loss: 0.6905 - acc: 0.5225 - val_loss: 0.6687 - val_acc: 0.6704\n",
      "Epoch 376/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6903 - acc: 0.5351 - val_loss: 0.6687 - val_acc: 0.6592\n",
      "Epoch 377/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6876 - acc: 0.5281 - val_loss: 0.6687 - val_acc: 0.6592\n",
      "Epoch 378/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6866 - acc: 0.5379 - val_loss: 0.6686 - val_acc: 0.6592\n",
      "Epoch 379/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6832 - acc: 0.5407 - val_loss: 0.6686 - val_acc: 0.6592\n",
      "Epoch 380/1000\n",
      "712/712 [==============================] - 0s 102us/step - loss: 0.6886 - acc: 0.5478 - val_loss: 0.6686 - val_acc: 0.6592\n",
      "Epoch 381/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6864 - acc: 0.5365 - val_loss: 0.6686 - val_acc: 0.6592\n",
      "Epoch 382/1000\n",
      "712/712 [==============================] - 0s 103us/step - loss: 0.6797 - acc: 0.5492 - val_loss: 0.6686 - val_acc: 0.6592\n",
      "Epoch 383/1000\n",
      "712/712 [==============================] - 0s 108us/step - loss: 0.6902 - acc: 0.5309 - val_loss: 0.6686 - val_acc: 0.6592\n",
      "Epoch 384/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6860 - acc: 0.5506 - val_loss: 0.6685 - val_acc: 0.6592\n",
      "Epoch 385/1000\n",
      "712/712 [==============================] - 0s 103us/step - loss: 0.6900 - acc: 0.5351 - val_loss: 0.6685 - val_acc: 0.6592\n",
      "Epoch 386/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6879 - acc: 0.5351 - val_loss: 0.6685 - val_acc: 0.6592\n",
      "Epoch 387/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6854 - acc: 0.5744 - val_loss: 0.6685 - val_acc: 0.6592\n",
      "Epoch 388/1000\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.6848 - acc: 0.5674 - val_loss: 0.6685 - val_acc: 0.6592\n",
      "Epoch 389/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6878 - acc: 0.5506 - val_loss: 0.6685 - val_acc: 0.6592\n",
      "Epoch 390/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6850 - acc: 0.5435 - val_loss: 0.6684 - val_acc: 0.6592\n",
      "Epoch 391/1000\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.6793 - acc: 0.5899 - val_loss: 0.6684 - val_acc: 0.6592\n",
      "Epoch 392/1000\n",
      "712/712 [==============================] - 0s 163us/step - loss: 0.6870 - acc: 0.5309 - val_loss: 0.6684 - val_acc: 0.6592\n",
      "Epoch 393/1000\n",
      "712/712 [==============================] - 0s 171us/step - loss: 0.6882 - acc: 0.5337 - val_loss: 0.6684 - val_acc: 0.6592\n",
      "Epoch 394/1000\n",
      "712/712 [==============================] - 0s 169us/step - loss: 0.6784 - acc: 0.5857 - val_loss: 0.6684 - val_acc: 0.6592\n",
      "Epoch 395/1000\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.6825 - acc: 0.5688 - val_loss: 0.6684 - val_acc: 0.6592\n",
      "Epoch 396/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6868 - acc: 0.5323 - val_loss: 0.6683 - val_acc: 0.6592\n",
      "Epoch 397/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6909 - acc: 0.5534 - val_loss: 0.6683 - val_acc: 0.6592\n",
      "Epoch 398/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6944 - acc: 0.5211 - val_loss: 0.6683 - val_acc: 0.6592\n",
      "Epoch 399/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6864 - acc: 0.5492 - val_loss: 0.6683 - val_acc: 0.6592\n",
      "Epoch 400/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6909 - acc: 0.5225 - val_loss: 0.6683 - val_acc: 0.6592\n",
      "Epoch 401/1000\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.6903 - acc: 0.5126 - val_loss: 0.6683 - val_acc: 0.6592\n",
      "Epoch 402/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6873 - acc: 0.5351 - val_loss: 0.6682 - val_acc: 0.6592\n",
      "Epoch 403/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6849 - acc: 0.5435 - val_loss: 0.6682 - val_acc: 0.6592\n",
      "Epoch 404/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6829 - acc: 0.5562 - val_loss: 0.6682 - val_acc: 0.6592\n",
      "Epoch 405/1000\n",
      "712/712 [==============================] - 0s 108us/step - loss: 0.6858 - acc: 0.5323 - val_loss: 0.6682 - val_acc: 0.6592\n",
      "Epoch 406/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6839 - acc: 0.5365 - val_loss: 0.6682 - val_acc: 0.6592\n",
      "Epoch 407/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6916 - acc: 0.5323 - val_loss: 0.6682 - val_acc: 0.6592\n",
      "Epoch 408/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6886 - acc: 0.5295 - val_loss: 0.6681 - val_acc: 0.6592\n",
      "Epoch 409/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6934 - acc: 0.5140 - val_loss: 0.6681 - val_acc: 0.6592\n",
      "Epoch 410/1000\n",
      "712/712 [==============================] - 0s 109us/step - loss: 0.6865 - acc: 0.5435 - val_loss: 0.6681 - val_acc: 0.6592\n",
      "Epoch 411/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6830 - acc: 0.5758 - val_loss: 0.6681 - val_acc: 0.6592\n",
      "Epoch 412/1000\n",
      "712/712 [==============================] - 0s 176us/step - loss: 0.6882 - acc: 0.5506 - val_loss: 0.6681 - val_acc: 0.6592\n",
      "Epoch 413/1000\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.6810 - acc: 0.5618 - val_loss: 0.6681 - val_acc: 0.6592\n",
      "Epoch 414/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6860 - acc: 0.5548 - val_loss: 0.6680 - val_acc: 0.6592\n",
      "Epoch 415/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6856 - acc: 0.5604 - val_loss: 0.6680 - val_acc: 0.6592\n",
      "Epoch 416/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6810 - acc: 0.5576 - val_loss: 0.6680 - val_acc: 0.6592\n",
      "Epoch 417/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6900 - acc: 0.5463 - val_loss: 0.6680 - val_acc: 0.6592\n",
      "Epoch 418/1000\n",
      "712/712 [==============================] - 0s 98us/step - loss: 0.6885 - acc: 0.5267 - val_loss: 0.6680 - val_acc: 0.6592\n",
      "Epoch 419/1000\n",
      "712/712 [==============================] - 0s 120us/step - loss: 0.6847 - acc: 0.5744 - val_loss: 0.6680 - val_acc: 0.6592\n",
      "Epoch 420/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6876 - acc: 0.5562 - val_loss: 0.6679 - val_acc: 0.6592\n",
      "Epoch 421/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6821 - acc: 0.5660 - val_loss: 0.6679 - val_acc: 0.6592\n",
      "Epoch 422/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6890 - acc: 0.5183 - val_loss: 0.6679 - val_acc: 0.6592\n",
      "Epoch 423/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6856 - acc: 0.5548 - val_loss: 0.6679 - val_acc: 0.6592\n",
      "Epoch 424/1000\n",
      "712/712 [==============================] - 0s 106us/step - loss: 0.6934 - acc: 0.5211 - val_loss: 0.6679 - val_acc: 0.6592\n",
      "Epoch 425/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6841 - acc: 0.5604 - val_loss: 0.6679 - val_acc: 0.6592\n",
      "Epoch 426/1000\n",
      "712/712 [==============================] - ETA: 0s - loss: 0.6996 - acc: 0.502 - 0s 135us/step - loss: 0.6954 - acc: 0.5239 - val_loss: 0.6678 - val_acc: 0.6592\n",
      "Epoch 427/1000\n",
      "712/712 [==============================] - 0s 96us/step - loss: 0.6847 - acc: 0.5337 - val_loss: 0.6678 - val_acc: 0.6592\n",
      "Epoch 428/1000\n",
      "712/712 [==============================] - 0s 98us/step - loss: 0.6895 - acc: 0.5520 - val_loss: 0.6678 - val_acc: 0.6592\n",
      "Epoch 429/1000\n",
      "712/712 [==============================] - 0s 98us/step - loss: 0.6920 - acc: 0.5478 - val_loss: 0.6678 - val_acc: 0.6592\n",
      "Epoch 430/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6807 - acc: 0.5520 - val_loss: 0.6678 - val_acc: 0.6592\n",
      "Epoch 431/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6900 - acc: 0.5239 - val_loss: 0.6678 - val_acc: 0.6592\n",
      "Epoch 432/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6883 - acc: 0.5309 - val_loss: 0.6677 - val_acc: 0.6592\n",
      "Epoch 433/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6857 - acc: 0.5506 - val_loss: 0.6677 - val_acc: 0.6592\n",
      "Epoch 434/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6930 - acc: 0.5098 - val_loss: 0.6677 - val_acc: 0.6592\n",
      "Epoch 435/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6870 - acc: 0.5351 - val_loss: 0.6677 - val_acc: 0.6592\n",
      "Epoch 436/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6833 - acc: 0.5688 - val_loss: 0.6677 - val_acc: 0.6592\n",
      "Epoch 437/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6932 - acc: 0.5225 - val_loss: 0.6677 - val_acc: 0.6592\n",
      "Epoch 438/1000\n",
      "712/712 [==============================] - 0s 100us/step - loss: 0.6891 - acc: 0.5365 - val_loss: 0.6676 - val_acc: 0.6592\n",
      "Epoch 439/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6864 - acc: 0.5351 - val_loss: 0.6676 - val_acc: 0.6592\n",
      "Epoch 440/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6890 - acc: 0.5140 - val_loss: 0.6676 - val_acc: 0.6592\n",
      "Epoch 441/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6868 - acc: 0.5393 - val_loss: 0.6676 - val_acc: 0.6592\n",
      "Epoch 442/1000\n",
      "712/712 [==============================] - 0s 97us/step - loss: 0.6820 - acc: 0.5520 - val_loss: 0.6676 - val_acc: 0.6592\n",
      "Epoch 443/1000\n",
      "712/712 [==============================] - 0s 101us/step - loss: 0.6810 - acc: 0.5590 - val_loss: 0.6676 - val_acc: 0.6592\n",
      "Epoch 444/1000\n",
      "712/712 [==============================] - 0s 99us/step - loss: 0.6856 - acc: 0.5393 - val_loss: 0.6675 - val_acc: 0.6592\n",
      "Epoch 445/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6837 - acc: 0.5618 - val_loss: 0.6675 - val_acc: 0.6592\n",
      "Epoch 446/1000\n",
      "712/712 [==============================] - 0s 102us/step - loss: 0.6901 - acc: 0.5449 - val_loss: 0.6675 - val_acc: 0.6592\n",
      "Epoch 447/1000\n",
      "712/712 [==============================] - 0s 101us/step - loss: 0.6927 - acc: 0.5211 - val_loss: 0.6675 - val_acc: 0.6592\n",
      "Epoch 448/1000\n",
      "712/712 [==============================] - 0s 100us/step - loss: 0.6824 - acc: 0.5435 - val_loss: 0.6675 - val_acc: 0.6592\n",
      "Epoch 449/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6750 - acc: 0.5829 - val_loss: 0.6675 - val_acc: 0.6592\n",
      "Epoch 450/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6865 - acc: 0.5534 - val_loss: 0.6675 - val_acc: 0.6592\n",
      "Epoch 451/1000\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.6866 - acc: 0.5421 - val_loss: 0.6674 - val_acc: 0.6592\n",
      "Epoch 452/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6794 - acc: 0.5815 - val_loss: 0.6674 - val_acc: 0.6592\n",
      "Epoch 453/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6832 - acc: 0.5463 - val_loss: 0.6674 - val_acc: 0.6592\n",
      "Epoch 454/1000\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.6781 - acc: 0.5787 - val_loss: 0.6674 - val_acc: 0.6592\n",
      "Epoch 455/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6806 - acc: 0.5365 - val_loss: 0.6674 - val_acc: 0.6592\n",
      "Epoch 456/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6813 - acc: 0.5801 - val_loss: 0.6674 - val_acc: 0.6592\n",
      "Epoch 457/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6766 - acc: 0.5801 - val_loss: 0.6673 - val_acc: 0.6592\n",
      "Epoch 458/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6839 - acc: 0.5646 - val_loss: 0.6673 - val_acc: 0.6592\n",
      "Epoch 459/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6890 - acc: 0.5323 - val_loss: 0.6673 - val_acc: 0.6592\n",
      "Epoch 460/1000\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.6910 - acc: 0.5169 - val_loss: 0.6673 - val_acc: 0.6592\n",
      "Epoch 461/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6906 - acc: 0.5323 - val_loss: 0.6673 - val_acc: 0.6592\n",
      "Epoch 462/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6862 - acc: 0.5492 - val_loss: 0.6673 - val_acc: 0.6592\n",
      "Epoch 463/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6825 - acc: 0.5548 - val_loss: 0.6672 - val_acc: 0.6592\n",
      "Epoch 464/1000\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.6908 - acc: 0.5449 - val_loss: 0.6672 - val_acc: 0.6592\n",
      "Epoch 465/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6910 - acc: 0.5295 - val_loss: 0.6672 - val_acc: 0.6592\n",
      "Epoch 466/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6915 - acc: 0.5239 - val_loss: 0.6672 - val_acc: 0.6592\n",
      "Epoch 467/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6808 - acc: 0.5548 - val_loss: 0.6672 - val_acc: 0.6592\n",
      "Epoch 468/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6871 - acc: 0.5492 - val_loss: 0.6672 - val_acc: 0.6592\n",
      "Epoch 469/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6828 - acc: 0.5576 - val_loss: 0.6671 - val_acc: 0.6592\n",
      "Epoch 470/1000\n",
      "712/712 [==============================] - 0s 164us/step - loss: 0.6817 - acc: 0.5393 - val_loss: 0.6671 - val_acc: 0.6592\n",
      "Epoch 471/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6845 - acc: 0.5646 - val_loss: 0.6671 - val_acc: 0.6592\n",
      "Epoch 472/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6827 - acc: 0.5829 - val_loss: 0.6671 - val_acc: 0.6592\n",
      "Epoch 473/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6809 - acc: 0.5492 - val_loss: 0.6671 - val_acc: 0.6592\n",
      "Epoch 474/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 128us/step - loss: 0.6874 - acc: 0.5463 - val_loss: 0.6671 - val_acc: 0.6592\n",
      "Epoch 475/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6838 - acc: 0.5534 - val_loss: 0.6670 - val_acc: 0.6592\n",
      "Epoch 476/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6828 - acc: 0.5478 - val_loss: 0.6670 - val_acc: 0.6592\n",
      "Epoch 477/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6805 - acc: 0.5646 - val_loss: 0.6670 - val_acc: 0.6592\n",
      "Epoch 478/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6914 - acc: 0.5225 - val_loss: 0.6670 - val_acc: 0.6592\n",
      "Epoch 479/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6775 - acc: 0.5955 - val_loss: 0.6670 - val_acc: 0.6592\n",
      "Epoch 480/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6898 - acc: 0.5435 - val_loss: 0.6670 - val_acc: 0.6592\n",
      "Epoch 481/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6866 - acc: 0.5506 - val_loss: 0.6670 - val_acc: 0.6592\n",
      "Epoch 482/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6807 - acc: 0.5604 - val_loss: 0.6669 - val_acc: 0.6592\n",
      "Epoch 483/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6815 - acc: 0.5590 - val_loss: 0.6669 - val_acc: 0.6592\n",
      "Epoch 484/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6869 - acc: 0.5351 - val_loss: 0.6669 - val_acc: 0.6592\n",
      "Epoch 485/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6842 - acc: 0.5674 - val_loss: 0.6669 - val_acc: 0.6592\n",
      "Epoch 486/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6799 - acc: 0.5632 - val_loss: 0.6669 - val_acc: 0.6592\n",
      "Epoch 487/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6809 - acc: 0.5435 - val_loss: 0.6669 - val_acc: 0.6592\n",
      "Epoch 488/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6923 - acc: 0.5337 - val_loss: 0.6668 - val_acc: 0.6592\n",
      "Epoch 489/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6888 - acc: 0.5337 - val_loss: 0.6668 - val_acc: 0.6592\n",
      "Epoch 490/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6810 - acc: 0.5688 - val_loss: 0.6668 - val_acc: 0.6592\n",
      "Epoch 491/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6785 - acc: 0.5815 - val_loss: 0.6668 - val_acc: 0.6592\n",
      "Epoch 492/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6879 - acc: 0.5351 - val_loss: 0.6668 - val_acc: 0.6592\n",
      "Epoch 493/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6817 - acc: 0.5660 - val_loss: 0.6668 - val_acc: 0.6592\n",
      "Epoch 494/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6828 - acc: 0.5449 - val_loss: 0.6667 - val_acc: 0.6592\n",
      "Epoch 495/1000\n",
      "712/712 [==============================] - 0s 105us/step - loss: 0.6853 - acc: 0.5646 - val_loss: 0.6667 - val_acc: 0.6592\n",
      "Epoch 496/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6837 - acc: 0.5562 - val_loss: 0.6667 - val_acc: 0.6592\n",
      "Epoch 497/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6871 - acc: 0.5323 - val_loss: 0.6667 - val_acc: 0.6592\n",
      "Epoch 498/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6821 - acc: 0.5787 - val_loss: 0.6667 - val_acc: 0.6592\n",
      "Epoch 499/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6910 - acc: 0.4944 - val_loss: 0.6667 - val_acc: 0.6592\n",
      "Epoch 500/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6920 - acc: 0.5140 - val_loss: 0.6666 - val_acc: 0.6592\n",
      "Epoch 501/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6879 - acc: 0.5407 - val_loss: 0.6666 - val_acc: 0.6592\n",
      "Epoch 502/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6890 - acc: 0.5253 - val_loss: 0.6666 - val_acc: 0.6592\n",
      "Epoch 503/1000\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.6860 - acc: 0.5506 - val_loss: 0.6666 - val_acc: 0.6592\n",
      "Epoch 504/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6808 - acc: 0.5365 - val_loss: 0.6666 - val_acc: 0.6592\n",
      "Epoch 505/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6865 - acc: 0.5463 - val_loss: 0.6666 - val_acc: 0.6592\n",
      "Epoch 506/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6882 - acc: 0.5478 - val_loss: 0.6666 - val_acc: 0.6592\n",
      "Epoch 507/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6871 - acc: 0.5351 - val_loss: 0.6665 - val_acc: 0.6592\n",
      "Epoch 508/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6863 - acc: 0.5548 - val_loss: 0.6665 - val_acc: 0.6592\n",
      "Epoch 509/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6823 - acc: 0.5927 - val_loss: 0.6665 - val_acc: 0.6592\n",
      "Epoch 510/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6781 - acc: 0.5772 - val_loss: 0.6665 - val_acc: 0.6592\n",
      "Epoch 511/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6806 - acc: 0.5618 - val_loss: 0.6665 - val_acc: 0.6592\n",
      "Epoch 512/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6876 - acc: 0.5197 - val_loss: 0.6665 - val_acc: 0.6592\n",
      "Epoch 513/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6915 - acc: 0.5112 - val_loss: 0.6664 - val_acc: 0.6592\n",
      "Epoch 514/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6884 - acc: 0.5169 - val_loss: 0.6664 - val_acc: 0.6592\n",
      "Epoch 515/1000\n",
      "712/712 [==============================] - 0s 163us/step - loss: 0.6869 - acc: 0.5281 - val_loss: 0.6664 - val_acc: 0.6592\n",
      "Epoch 516/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6863 - acc: 0.5534 - val_loss: 0.6664 - val_acc: 0.6592\n",
      "Epoch 517/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6815 - acc: 0.5787 - val_loss: 0.6664 - val_acc: 0.6592\n",
      "Epoch 518/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6854 - acc: 0.5463 - val_loss: 0.6664 - val_acc: 0.6592\n",
      "Epoch 519/1000\n",
      "712/712 [==============================] - 0s 120us/step - loss: 0.6845 - acc: 0.5632 - val_loss: 0.6663 - val_acc: 0.6592\n",
      "Epoch 520/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6874 - acc: 0.5393 - val_loss: 0.6663 - val_acc: 0.6592\n",
      "Epoch 521/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6850 - acc: 0.5744 - val_loss: 0.6663 - val_acc: 0.6592\n",
      "Epoch 522/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6863 - acc: 0.5463 - val_loss: 0.6663 - val_acc: 0.6592\n",
      "Epoch 523/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6872 - acc: 0.5534 - val_loss: 0.6663 - val_acc: 0.6592\n",
      "Epoch 524/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6790 - acc: 0.5660 - val_loss: 0.6663 - val_acc: 0.6592\n",
      "Epoch 525/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6847 - acc: 0.5365 - val_loss: 0.6662 - val_acc: 0.6592\n",
      "Epoch 526/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6826 - acc: 0.5449 - val_loss: 0.6662 - val_acc: 0.6592\n",
      "Epoch 527/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6861 - acc: 0.5506 - val_loss: 0.6662 - val_acc: 0.6592\n",
      "Epoch 528/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6933 - acc: 0.5520 - val_loss: 0.6662 - val_acc: 0.6592\n",
      "Epoch 529/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6877 - acc: 0.5351 - val_loss: 0.6662 - val_acc: 0.6592\n",
      "Epoch 530/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6845 - acc: 0.5562 - val_loss: 0.6662 - val_acc: 0.6592\n",
      "Epoch 531/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6867 - acc: 0.5337 - val_loss: 0.6661 - val_acc: 0.6592\n",
      "Epoch 532/1000\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.6923 - acc: 0.5239 - val_loss: 0.6661 - val_acc: 0.6592\n",
      "Epoch 533/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6846 - acc: 0.5548 - val_loss: 0.6661 - val_acc: 0.6592\n",
      "Epoch 534/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6839 - acc: 0.5506 - val_loss: 0.6661 - val_acc: 0.6592\n",
      "Epoch 535/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6906 - acc: 0.5042 - val_loss: 0.6661 - val_acc: 0.6592\n",
      "Epoch 536/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6940 - acc: 0.5154 - val_loss: 0.6661 - val_acc: 0.6592\n",
      "Epoch 537/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6807 - acc: 0.5871 - val_loss: 0.6661 - val_acc: 0.6592\n",
      "Epoch 538/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6838 - acc: 0.5492 - val_loss: 0.6660 - val_acc: 0.6592\n",
      "Epoch 539/1000\n",
      "712/712 [==============================] - 0s 120us/step - loss: 0.6815 - acc: 0.5618 - val_loss: 0.6660 - val_acc: 0.6592\n",
      "Epoch 540/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6784 - acc: 0.5646 - val_loss: 0.6660 - val_acc: 0.6592\n",
      "Epoch 541/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6915 - acc: 0.5421 - val_loss: 0.6660 - val_acc: 0.6592\n",
      "Epoch 542/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6863 - acc: 0.5478 - val_loss: 0.6660 - val_acc: 0.6592\n",
      "Epoch 543/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6878 - acc: 0.5267 - val_loss: 0.6660 - val_acc: 0.6592\n",
      "Epoch 544/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6772 - acc: 0.5772 - val_loss: 0.6659 - val_acc: 0.6592\n",
      "Epoch 545/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6848 - acc: 0.5548 - val_loss: 0.6659 - val_acc: 0.6592\n",
      "Epoch 546/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6857 - acc: 0.5435 - val_loss: 0.6659 - val_acc: 0.6592\n",
      "Epoch 547/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6851 - acc: 0.5435 - val_loss: 0.6659 - val_acc: 0.6592\n",
      "Epoch 548/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6835 - acc: 0.5478 - val_loss: 0.6659 - val_acc: 0.6592\n",
      "Epoch 549/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6816 - acc: 0.5604 - val_loss: 0.6659 - val_acc: 0.6592\n",
      "Epoch 550/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6832 - acc: 0.5815 - val_loss: 0.6658 - val_acc: 0.6592\n",
      "Epoch 551/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6875 - acc: 0.5463 - val_loss: 0.6658 - val_acc: 0.6592\n",
      "Epoch 552/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6783 - acc: 0.5660 - val_loss: 0.6658 - val_acc: 0.6592\n",
      "Epoch 553/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6928 - acc: 0.5211 - val_loss: 0.6658 - val_acc: 0.6592\n",
      "Epoch 554/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6879 - acc: 0.5393 - val_loss: 0.6658 - val_acc: 0.6592\n",
      "Epoch 555/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6845 - acc: 0.5590 - val_loss: 0.6658 - val_acc: 0.6592\n",
      "Epoch 556/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6820 - acc: 0.5365 - val_loss: 0.6657 - val_acc: 0.6592\n",
      "Epoch 557/1000\n",
      "712/712 [==============================] - 0s 100us/step - loss: 0.6801 - acc: 0.5463 - val_loss: 0.6657 - val_acc: 0.6592\n",
      "Epoch 558/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6872 - acc: 0.5365 - val_loss: 0.6657 - val_acc: 0.6592\n",
      "Epoch 559/1000\n",
      "712/712 [==============================] - 0s 104us/step - loss: 0.6831 - acc: 0.5590 - val_loss: 0.6657 - val_acc: 0.6592\n",
      "Epoch 560/1000\n",
      "712/712 [==============================] - 0s 99us/step - loss: 0.6852 - acc: 0.5393 - val_loss: 0.6657 - val_acc: 0.6592\n",
      "Epoch 561/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6844 - acc: 0.5365 - val_loss: 0.6657 - val_acc: 0.6592\n",
      "Epoch 562/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6861 - acc: 0.5632 - val_loss: 0.6657 - val_acc: 0.6592\n",
      "Epoch 563/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6772 - acc: 0.5955 - val_loss: 0.6656 - val_acc: 0.6592\n",
      "Epoch 564/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6843 - acc: 0.5590 - val_loss: 0.6656 - val_acc: 0.6592\n",
      "Epoch 565/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6825 - acc: 0.5534 - val_loss: 0.6656 - val_acc: 0.6592\n",
      "Epoch 566/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6789 - acc: 0.5632 - val_loss: 0.6656 - val_acc: 0.6592\n",
      "Epoch 567/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6870 - acc: 0.5534 - val_loss: 0.6656 - val_acc: 0.6592\n",
      "Epoch 568/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6901 - acc: 0.5548 - val_loss: 0.6656 - val_acc: 0.6592\n",
      "Epoch 569/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6837 - acc: 0.5660 - val_loss: 0.6655 - val_acc: 0.6592\n",
      "Epoch 570/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6840 - acc: 0.5548 - val_loss: 0.6655 - val_acc: 0.6592\n",
      "Epoch 571/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6777 - acc: 0.5801 - val_loss: 0.6655 - val_acc: 0.6592\n",
      "Epoch 572/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6959 - acc: 0.5042 - val_loss: 0.6655 - val_acc: 0.6592\n",
      "Epoch 573/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6791 - acc: 0.5618 - val_loss: 0.6655 - val_acc: 0.6592\n",
      "Epoch 574/1000\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.6830 - acc: 0.5492 - val_loss: 0.6655 - val_acc: 0.6592\n",
      "Epoch 575/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6867 - acc: 0.5309 - val_loss: 0.6654 - val_acc: 0.6592\n",
      "Epoch 576/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6836 - acc: 0.5548 - val_loss: 0.6654 - val_acc: 0.6592\n",
      "Epoch 577/1000\n",
      "712/712 [==============================] - 0s 166us/step - loss: 0.6899 - acc: 0.5365 - val_loss: 0.6654 - val_acc: 0.6592\n",
      "Epoch 578/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6840 - acc: 0.5604 - val_loss: 0.6654 - val_acc: 0.6592\n",
      "Epoch 579/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6859 - acc: 0.5365 - val_loss: 0.6654 - val_acc: 0.6592\n",
      "Epoch 580/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6847 - acc: 0.5548 - val_loss: 0.6654 - val_acc: 0.6592\n",
      "Epoch 581/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6922 - acc: 0.5169 - val_loss: 0.6654 - val_acc: 0.6592\n",
      "Epoch 582/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6868 - acc: 0.5393 - val_loss: 0.6653 - val_acc: 0.6592\n",
      "Epoch 583/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6873 - acc: 0.5253 - val_loss: 0.6653 - val_acc: 0.6592\n",
      "Epoch 584/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6904 - acc: 0.5323 - val_loss: 0.6653 - val_acc: 0.6592\n",
      "Epoch 585/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6893 - acc: 0.5604 - val_loss: 0.6653 - val_acc: 0.6592\n",
      "Epoch 586/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6817 - acc: 0.5843 - val_loss: 0.6653 - val_acc: 0.6592\n",
      "Epoch 587/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6809 - acc: 0.5674 - val_loss: 0.6653 - val_acc: 0.6592\n",
      "Epoch 588/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6784 - acc: 0.5632 - val_loss: 0.6652 - val_acc: 0.6592\n",
      "Epoch 589/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6868 - acc: 0.5126 - val_loss: 0.6652 - val_acc: 0.6592\n",
      "Epoch 590/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6823 - acc: 0.5576 - val_loss: 0.6652 - val_acc: 0.6592\n",
      "Epoch 591/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6830 - acc: 0.5548 - val_loss: 0.6652 - val_acc: 0.6592\n",
      "Epoch 592/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 146us/step - loss: 0.6797 - acc: 0.5758 - val_loss: 0.6652 - val_acc: 0.6592\n",
      "Epoch 593/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6801 - acc: 0.5618 - val_loss: 0.6652 - val_acc: 0.6592\n",
      "Epoch 594/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6843 - acc: 0.5562 - val_loss: 0.6651 - val_acc: 0.6592\n",
      "Epoch 595/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6913 - acc: 0.5253 - val_loss: 0.6651 - val_acc: 0.6592\n",
      "Epoch 596/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6854 - acc: 0.5407 - val_loss: 0.6651 - val_acc: 0.6592\n",
      "Epoch 597/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6757 - acc: 0.5548 - val_loss: 0.6651 - val_acc: 0.6592\n",
      "Epoch 598/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6788 - acc: 0.5520 - val_loss: 0.6651 - val_acc: 0.6592\n",
      "Epoch 599/1000\n",
      "712/712 [==============================] - 0s 120us/step - loss: 0.6858 - acc: 0.5449 - val_loss: 0.6651 - val_acc: 0.6592\n",
      "Epoch 600/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6813 - acc: 0.5520 - val_loss: 0.6651 - val_acc: 0.6592\n",
      "Epoch 601/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6809 - acc: 0.5576 - val_loss: 0.6650 - val_acc: 0.6592\n",
      "Epoch 602/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6903 - acc: 0.5351 - val_loss: 0.6650 - val_acc: 0.6592\n",
      "Epoch 603/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6841 - acc: 0.5688 - val_loss: 0.6650 - val_acc: 0.6592\n",
      "Epoch 604/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6801 - acc: 0.5379 - val_loss: 0.6650 - val_acc: 0.6592\n",
      "Epoch 605/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6882 - acc: 0.5407 - val_loss: 0.6650 - val_acc: 0.6592\n",
      "Epoch 606/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6868 - acc: 0.5281 - val_loss: 0.6650 - val_acc: 0.6592\n",
      "Epoch 607/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6827 - acc: 0.5435 - val_loss: 0.6650 - val_acc: 0.6592\n",
      "Epoch 608/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6853 - acc: 0.5463 - val_loss: 0.6649 - val_acc: 0.6592\n",
      "Epoch 609/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6796 - acc: 0.5815 - val_loss: 0.6649 - val_acc: 0.6592\n",
      "Epoch 610/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6876 - acc: 0.5421 - val_loss: 0.6649 - val_acc: 0.6592\n",
      "Epoch 611/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6832 - acc: 0.5337 - val_loss: 0.6649 - val_acc: 0.6592\n",
      "Epoch 612/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6871 - acc: 0.5632 - val_loss: 0.6649 - val_acc: 0.6592\n",
      "Epoch 613/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6846 - acc: 0.5646 - val_loss: 0.6649 - val_acc: 0.6592\n",
      "Epoch 614/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6867 - acc: 0.5421 - val_loss: 0.6648 - val_acc: 0.6592\n",
      "Epoch 615/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6889 - acc: 0.5618 - val_loss: 0.6648 - val_acc: 0.6592\n",
      "Epoch 616/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6849 - acc: 0.5534 - val_loss: 0.6648 - val_acc: 0.6592\n",
      "Epoch 617/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6906 - acc: 0.5365 - val_loss: 0.6648 - val_acc: 0.6592\n",
      "Epoch 618/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6864 - acc: 0.5365 - val_loss: 0.6648 - val_acc: 0.6592\n",
      "Epoch 619/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6772 - acc: 0.5730 - val_loss: 0.6648 - val_acc: 0.6592\n",
      "Epoch 620/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6813 - acc: 0.5590 - val_loss: 0.6648 - val_acc: 0.6592\n",
      "Epoch 621/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6840 - acc: 0.5660 - val_loss: 0.6647 - val_acc: 0.6536\n",
      "Epoch 622/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6908 - acc: 0.5478 - val_loss: 0.6647 - val_acc: 0.6536\n",
      "Epoch 623/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6819 - acc: 0.5688 - val_loss: 0.6647 - val_acc: 0.6536\n",
      "Epoch 624/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6813 - acc: 0.5618 - val_loss: 0.6647 - val_acc: 0.6536\n",
      "Epoch 625/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6903 - acc: 0.5211 - val_loss: 0.6647 - val_acc: 0.6536\n",
      "Epoch 626/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6855 - acc: 0.5393 - val_loss: 0.6647 - val_acc: 0.6536\n",
      "Epoch 627/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6804 - acc: 0.5590 - val_loss: 0.6646 - val_acc: 0.6536\n",
      "Epoch 628/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6890 - acc: 0.5323 - val_loss: 0.6646 - val_acc: 0.6536\n",
      "Epoch 629/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6818 - acc: 0.5674 - val_loss: 0.6646 - val_acc: 0.6536\n",
      "Epoch 630/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6815 - acc: 0.5646 - val_loss: 0.6646 - val_acc: 0.6536\n",
      "Epoch 631/1000\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.6773 - acc: 0.5688 - val_loss: 0.6646 - val_acc: 0.6536\n",
      "Epoch 632/1000\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.6771 - acc: 0.5787 - val_loss: 0.6646 - val_acc: 0.6536\n",
      "Epoch 633/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6839 - acc: 0.5309 - val_loss: 0.6645 - val_acc: 0.6536\n",
      "Epoch 634/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6898 - acc: 0.5534 - val_loss: 0.6645 - val_acc: 0.6536\n",
      "Epoch 635/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6937 - acc: 0.5351 - val_loss: 0.6645 - val_acc: 0.6536\n",
      "Epoch 636/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6815 - acc: 0.5576 - val_loss: 0.6645 - val_acc: 0.6536\n",
      "Epoch 637/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6854 - acc: 0.5520 - val_loss: 0.6645 - val_acc: 0.6536\n",
      "Epoch 638/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6853 - acc: 0.5421 - val_loss: 0.6645 - val_acc: 0.6536\n",
      "Epoch 639/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6806 - acc: 0.5632 - val_loss: 0.6645 - val_acc: 0.6536\n",
      "Epoch 640/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6864 - acc: 0.5492 - val_loss: 0.6644 - val_acc: 0.6536\n",
      "Epoch 641/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6924 - acc: 0.5154 - val_loss: 0.6644 - val_acc: 0.6536\n",
      "Epoch 642/1000\n",
      "712/712 [==============================] - 0s 109us/step - loss: 0.6887 - acc: 0.5506 - val_loss: 0.6644 - val_acc: 0.6536\n",
      "Epoch 643/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6917 - acc: 0.5421 - val_loss: 0.6644 - val_acc: 0.6536\n",
      "Epoch 644/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6817 - acc: 0.5632 - val_loss: 0.6644 - val_acc: 0.6536\n",
      "Epoch 645/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6790 - acc: 0.5899 - val_loss: 0.6644 - val_acc: 0.6536\n",
      "Epoch 646/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6845 - acc: 0.5449 - val_loss: 0.6643 - val_acc: 0.6536\n",
      "Epoch 647/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6868 - acc: 0.5646 - val_loss: 0.6643 - val_acc: 0.6536\n",
      "Epoch 648/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6961 - acc: 0.5126 - val_loss: 0.6643 - val_acc: 0.6536\n",
      "Epoch 649/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6825 - acc: 0.5548 - val_loss: 0.6643 - val_acc: 0.6536\n",
      "Epoch 650/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6871 - acc: 0.5478 - val_loss: 0.6643 - val_acc: 0.6536\n",
      "Epoch 651/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6824 - acc: 0.5506 - val_loss: 0.6643 - val_acc: 0.6536\n",
      "Epoch 652/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6941 - acc: 0.5351 - val_loss: 0.6642 - val_acc: 0.6536\n",
      "Epoch 653/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6835 - acc: 0.5744 - val_loss: 0.6642 - val_acc: 0.6536\n",
      "Epoch 654/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6875 - acc: 0.5253 - val_loss: 0.6642 - val_acc: 0.6536\n",
      "Epoch 655/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6752 - acc: 0.5969 - val_loss: 0.6642 - val_acc: 0.6536\n",
      "Epoch 656/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6898 - acc: 0.5520 - val_loss: 0.6642 - val_acc: 0.6536\n",
      "Epoch 657/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6904 - acc: 0.5337 - val_loss: 0.6642 - val_acc: 0.6536\n",
      "Epoch 658/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6794 - acc: 0.5520 - val_loss: 0.6642 - val_acc: 0.6536\n",
      "Epoch 659/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6839 - acc: 0.5534 - val_loss: 0.6641 - val_acc: 0.6536\n",
      "Epoch 660/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6823 - acc: 0.5548 - val_loss: 0.6641 - val_acc: 0.6536\n",
      "Epoch 661/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6786 - acc: 0.5688 - val_loss: 0.6641 - val_acc: 0.6536\n",
      "Epoch 662/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6889 - acc: 0.5309 - val_loss: 0.6641 - val_acc: 0.6536\n",
      "Epoch 663/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6817 - acc: 0.5618 - val_loss: 0.6641 - val_acc: 0.6536\n",
      "Epoch 664/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6942 - acc: 0.5225 - val_loss: 0.6641 - val_acc: 0.6536\n",
      "Epoch 665/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6871 - acc: 0.5815 - val_loss: 0.6640 - val_acc: 0.6536\n",
      "Epoch 666/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6782 - acc: 0.5927 - val_loss: 0.6640 - val_acc: 0.6536\n",
      "Epoch 667/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6871 - acc: 0.5520 - val_loss: 0.6640 - val_acc: 0.6536\n",
      "Epoch 668/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6835 - acc: 0.5576 - val_loss: 0.6640 - val_acc: 0.6536\n",
      "Epoch 669/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6827 - acc: 0.5435 - val_loss: 0.6640 - val_acc: 0.6536\n",
      "Epoch 670/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6906 - acc: 0.5267 - val_loss: 0.6640 - val_acc: 0.6536\n",
      "Epoch 671/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6834 - acc: 0.5506 - val_loss: 0.6640 - val_acc: 0.6536\n",
      "Epoch 672/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6870 - acc: 0.5393 - val_loss: 0.6639 - val_acc: 0.6536\n",
      "Epoch 673/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6840 - acc: 0.5506 - val_loss: 0.6639 - val_acc: 0.6536\n",
      "Epoch 674/1000\n",
      "712/712 [==============================] - 0s 98us/step - loss: 0.6805 - acc: 0.5716 - val_loss: 0.6639 - val_acc: 0.6536\n",
      "Epoch 675/1000\n",
      "712/712 [==============================] - 0s 108us/step - loss: 0.6909 - acc: 0.5211 - val_loss: 0.6639 - val_acc: 0.6536\n",
      "Epoch 676/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6830 - acc: 0.5646 - val_loss: 0.6639 - val_acc: 0.6536\n",
      "Epoch 677/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6853 - acc: 0.5548 - val_loss: 0.6639 - val_acc: 0.6536\n",
      "Epoch 678/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6799 - acc: 0.5744 - val_loss: 0.6639 - val_acc: 0.6536\n",
      "Epoch 679/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6808 - acc: 0.5632 - val_loss: 0.6638 - val_acc: 0.6536\n",
      "Epoch 680/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6796 - acc: 0.5506 - val_loss: 0.6638 - val_acc: 0.6536\n",
      "Epoch 681/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6792 - acc: 0.5857 - val_loss: 0.6638 - val_acc: 0.6536\n",
      "Epoch 682/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6809 - acc: 0.5520 - val_loss: 0.6638 - val_acc: 0.6536\n",
      "Epoch 683/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6896 - acc: 0.5309 - val_loss: 0.6638 - val_acc: 0.6536\n",
      "Epoch 684/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6775 - acc: 0.5646 - val_loss: 0.6638 - val_acc: 0.6536\n",
      "Epoch 685/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6772 - acc: 0.5815 - val_loss: 0.6637 - val_acc: 0.6536\n",
      "Epoch 686/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6824 - acc: 0.5548 - val_loss: 0.6637 - val_acc: 0.6536\n",
      "Epoch 687/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6846 - acc: 0.5576 - val_loss: 0.6637 - val_acc: 0.6536\n",
      "Epoch 688/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6778 - acc: 0.5787 - val_loss: 0.6637 - val_acc: 0.6536\n",
      "Epoch 689/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6762 - acc: 0.5688 - val_loss: 0.6637 - val_acc: 0.6536\n",
      "Epoch 690/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6799 - acc: 0.5744 - val_loss: 0.6637 - val_acc: 0.6536\n",
      "Epoch 691/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6885 - acc: 0.5632 - val_loss: 0.6637 - val_acc: 0.6536\n",
      "Epoch 692/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6863 - acc: 0.5365 - val_loss: 0.6636 - val_acc: 0.6536\n",
      "Epoch 693/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6804 - acc: 0.5632 - val_loss: 0.6636 - val_acc: 0.6536\n",
      "Epoch 694/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6864 - acc: 0.5520 - val_loss: 0.6636 - val_acc: 0.6536\n",
      "Epoch 695/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6844 - acc: 0.5660 - val_loss: 0.6636 - val_acc: 0.6536\n",
      "Epoch 696/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6790 - acc: 0.5829 - val_loss: 0.6636 - val_acc: 0.6536\n",
      "Epoch 697/1000\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.6874 - acc: 0.5576 - val_loss: 0.6636 - val_acc: 0.6536\n",
      "Epoch 698/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6809 - acc: 0.5660 - val_loss: 0.6635 - val_acc: 0.6536\n",
      "Epoch 699/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6728 - acc: 0.5955 - val_loss: 0.6635 - val_acc: 0.6536\n",
      "Epoch 700/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6870 - acc: 0.5702 - val_loss: 0.6635 - val_acc: 0.6536\n",
      "Epoch 701/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6831 - acc: 0.5492 - val_loss: 0.6635 - val_acc: 0.6536\n",
      "Epoch 702/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6863 - acc: 0.5618 - val_loss: 0.6635 - val_acc: 0.6536\n",
      "Epoch 703/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6784 - acc: 0.5646 - val_loss: 0.6635 - val_acc: 0.6536\n",
      "Epoch 704/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6824 - acc: 0.5534 - val_loss: 0.6635 - val_acc: 0.6536\n",
      "Epoch 705/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6820 - acc: 0.5562 - val_loss: 0.6634 - val_acc: 0.6536\n",
      "Epoch 706/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6878 - acc: 0.5492 - val_loss: 0.6634 - val_acc: 0.6536\n",
      "Epoch 707/1000\n",
      "712/712 [==============================] - 0s 103us/step - loss: 0.6732 - acc: 0.5955 - val_loss: 0.6634 - val_acc: 0.6536\n",
      "Epoch 708/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6847 - acc: 0.5772 - val_loss: 0.6634 - val_acc: 0.6536\n",
      "Epoch 709/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6880 - acc: 0.5323 - val_loss: 0.6634 - val_acc: 0.6536\n",
      "Epoch 710/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 109us/step - loss: 0.6789 - acc: 0.5618 - val_loss: 0.6634 - val_acc: 0.6536\n",
      "Epoch 711/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6825 - acc: 0.5295 - val_loss: 0.6634 - val_acc: 0.6536\n",
      "Epoch 712/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6792 - acc: 0.5520 - val_loss: 0.6633 - val_acc: 0.6536\n",
      "Epoch 713/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6854 - acc: 0.5744 - val_loss: 0.6633 - val_acc: 0.6536\n",
      "Epoch 714/1000\n",
      "712/712 [==============================] - 0s 104us/step - loss: 0.6877 - acc: 0.5562 - val_loss: 0.6633 - val_acc: 0.6536\n",
      "Epoch 715/1000\n",
      "712/712 [==============================] - 0s 100us/step - loss: 0.6827 - acc: 0.5534 - val_loss: 0.6633 - val_acc: 0.6536\n",
      "Epoch 716/1000\n",
      "712/712 [==============================] - 0s 106us/step - loss: 0.6863 - acc: 0.5323 - val_loss: 0.6633 - val_acc: 0.6536\n",
      "Epoch 717/1000\n",
      "712/712 [==============================] - 0s 104us/step - loss: 0.6850 - acc: 0.5562 - val_loss: 0.6633 - val_acc: 0.6536\n",
      "Epoch 718/1000\n",
      "712/712 [==============================] - 0s 102us/step - loss: 0.6827 - acc: 0.5632 - val_loss: 0.6632 - val_acc: 0.6536\n",
      "Epoch 719/1000\n",
      "712/712 [==============================] - 0s 98us/step - loss: 0.6815 - acc: 0.5618 - val_loss: 0.6632 - val_acc: 0.6536\n",
      "Epoch 720/1000\n",
      "712/712 [==============================] - 0s 97us/step - loss: 0.6846 - acc: 0.5520 - val_loss: 0.6632 - val_acc: 0.6536\n",
      "Epoch 721/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6803 - acc: 0.5646 - val_loss: 0.6632 - val_acc: 0.6536\n",
      "Epoch 722/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6843 - acc: 0.5604 - val_loss: 0.6632 - val_acc: 0.6536\n",
      "Epoch 723/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6853 - acc: 0.5534 - val_loss: 0.6632 - val_acc: 0.6536\n",
      "Epoch 724/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6875 - acc: 0.5323 - val_loss: 0.6632 - val_acc: 0.6536\n",
      "Epoch 725/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6764 - acc: 0.5716 - val_loss: 0.6631 - val_acc: 0.6536\n",
      "Epoch 726/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6831 - acc: 0.5506 - val_loss: 0.6631 - val_acc: 0.6536\n",
      "Epoch 727/1000\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.6915 - acc: 0.5225 - val_loss: 0.6631 - val_acc: 0.6536\n",
      "Epoch 728/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6773 - acc: 0.5772 - val_loss: 0.6631 - val_acc: 0.6536\n",
      "Epoch 729/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6795 - acc: 0.5520 - val_loss: 0.6631 - val_acc: 0.6536\n",
      "Epoch 730/1000\n",
      "712/712 [==============================] - 0s 101us/step - loss: 0.6796 - acc: 0.5590 - val_loss: 0.6631 - val_acc: 0.6536\n",
      "Epoch 731/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6873 - acc: 0.5379 - val_loss: 0.6631 - val_acc: 0.6536\n",
      "Epoch 732/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6817 - acc: 0.5520 - val_loss: 0.6630 - val_acc: 0.6536\n",
      "Epoch 733/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6822 - acc: 0.5463 - val_loss: 0.6630 - val_acc: 0.6536\n",
      "Epoch 734/1000\n",
      "712/712 [==============================] - 0s 168us/step - loss: 0.6776 - acc: 0.5815 - val_loss: 0.6630 - val_acc: 0.6536\n",
      "Epoch 735/1000\n",
      "712/712 [==============================] - 0s 174us/step - loss: 0.6765 - acc: 0.5857 - val_loss: 0.6630 - val_acc: 0.6536\n",
      "Epoch 736/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6799 - acc: 0.5632 - val_loss: 0.6630 - val_acc: 0.6536\n",
      "Epoch 737/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6761 - acc: 0.5857 - val_loss: 0.6630 - val_acc: 0.6536\n",
      "Epoch 738/1000\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.6762 - acc: 0.5660 - val_loss: 0.6629 - val_acc: 0.6536\n",
      "Epoch 739/1000\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.6770 - acc: 0.5492 - val_loss: 0.6629 - val_acc: 0.6536\n",
      "Epoch 740/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6824 - acc: 0.5463 - val_loss: 0.6629 - val_acc: 0.6536\n",
      "Epoch 741/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6779 - acc: 0.5801 - val_loss: 0.6629 - val_acc: 0.6536\n",
      "Epoch 742/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6878 - acc: 0.5520 - val_loss: 0.6629 - val_acc: 0.6536\n",
      "Epoch 743/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6836 - acc: 0.5520 - val_loss: 0.6629 - val_acc: 0.6536\n",
      "Epoch 744/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6879 - acc: 0.5534 - val_loss: 0.6629 - val_acc: 0.6536\n",
      "Epoch 745/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6823 - acc: 0.5590 - val_loss: 0.6628 - val_acc: 0.6536\n",
      "Epoch 746/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6780 - acc: 0.5913 - val_loss: 0.6628 - val_acc: 0.6536\n",
      "Epoch 747/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6774 - acc: 0.5787 - val_loss: 0.6628 - val_acc: 0.6536\n",
      "Epoch 748/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6924 - acc: 0.5253 - val_loss: 0.6628 - val_acc: 0.6536\n",
      "Epoch 749/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6782 - acc: 0.5801 - val_loss: 0.6628 - val_acc: 0.6536\n",
      "Epoch 750/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6860 - acc: 0.5421 - val_loss: 0.6628 - val_acc: 0.6536\n",
      "Epoch 751/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6818 - acc: 0.5393 - val_loss: 0.6627 - val_acc: 0.6536\n",
      "Epoch 752/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6816 - acc: 0.5492 - val_loss: 0.6627 - val_acc: 0.6536\n",
      "Epoch 753/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6822 - acc: 0.5857 - val_loss: 0.6627 - val_acc: 0.6536\n",
      "Epoch 754/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6825 - acc: 0.5590 - val_loss: 0.6627 - val_acc: 0.6536\n",
      "Epoch 755/1000\n",
      "712/712 [==============================] - 0s 162us/step - loss: 0.6801 - acc: 0.5646 - val_loss: 0.6627 - val_acc: 0.6536\n",
      "Epoch 756/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6806 - acc: 0.5660 - val_loss: 0.6627 - val_acc: 0.6536\n",
      "Epoch 757/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6820 - acc: 0.5688 - val_loss: 0.6627 - val_acc: 0.6536\n",
      "Epoch 758/1000\n",
      "712/712 [==============================] - 0s 91us/step - loss: 0.6844 - acc: 0.5449 - val_loss: 0.6626 - val_acc: 0.6536\n",
      "Epoch 759/1000\n",
      "712/712 [==============================] - 0s 76us/step - loss: 0.6905 - acc: 0.5225 - val_loss: 0.6626 - val_acc: 0.6536\n",
      "Epoch 760/1000\n",
      "712/712 [==============================] - 0s 92us/step - loss: 0.6903 - acc: 0.5379 - val_loss: 0.6626 - val_acc: 0.6536\n",
      "Epoch 761/1000\n",
      "712/712 [==============================] - 0s 76us/step - loss: 0.6907 - acc: 0.5309 - val_loss: 0.6626 - val_acc: 0.6536\n",
      "Epoch 762/1000\n",
      "712/712 [==============================] - 0s 75us/step - loss: 0.6831 - acc: 0.5702 - val_loss: 0.6626 - val_acc: 0.6536\n",
      "Epoch 763/1000\n",
      "712/712 [==============================] - 0s 75us/step - loss: 0.6733 - acc: 0.5871 - val_loss: 0.6626 - val_acc: 0.6536\n",
      "Epoch 764/1000\n",
      "712/712 [==============================] - 0s 76us/step - loss: 0.6862 - acc: 0.5646 - val_loss: 0.6625 - val_acc: 0.6536\n",
      "Epoch 765/1000\n",
      "712/712 [==============================] - 0s 77us/step - loss: 0.6776 - acc: 0.5674 - val_loss: 0.6625 - val_acc: 0.6536\n",
      "Epoch 766/1000\n",
      "712/712 [==============================] - 0s 76us/step - loss: 0.6900 - acc: 0.5534 - val_loss: 0.6625 - val_acc: 0.6536\n",
      "Epoch 767/1000\n",
      "712/712 [==============================] - 0s 76us/step - loss: 0.6846 - acc: 0.5435 - val_loss: 0.6625 - val_acc: 0.6536\n",
      "Epoch 768/1000\n",
      "712/712 [==============================] - 0s 96us/step - loss: 0.6811 - acc: 0.5674 - val_loss: 0.6625 - val_acc: 0.6536\n",
      "Epoch 769/1000\n",
      "712/712 [==============================] - 0s 78us/step - loss: 0.6788 - acc: 0.5674 - val_loss: 0.6625 - val_acc: 0.6536\n",
      "Epoch 770/1000\n",
      "712/712 [==============================] - 0s 76us/step - loss: 0.6841 - acc: 0.5534 - val_loss: 0.6625 - val_acc: 0.6536\n",
      "Epoch 771/1000\n",
      "712/712 [==============================] - 0s 75us/step - loss: 0.6767 - acc: 0.5758 - val_loss: 0.6624 - val_acc: 0.6536\n",
      "Epoch 772/1000\n",
      "712/712 [==============================] - 0s 75us/step - loss: 0.6794 - acc: 0.5646 - val_loss: 0.6624 - val_acc: 0.6536\n",
      "Epoch 773/1000\n",
      "712/712 [==============================] - 0s 75us/step - loss: 0.6800 - acc: 0.5829 - val_loss: 0.6624 - val_acc: 0.6536\n",
      "Epoch 774/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6736 - acc: 0.5829 - val_loss: 0.6624 - val_acc: 0.6536\n",
      "Epoch 775/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6820 - acc: 0.5688 - val_loss: 0.6624 - val_acc: 0.6536\n",
      "Epoch 776/1000\n",
      "712/712 [==============================] - 0s 98us/step - loss: 0.6728 - acc: 0.6039 - val_loss: 0.6624 - val_acc: 0.6536\n",
      "Epoch 777/1000\n",
      "712/712 [==============================] - 0s 97us/step - loss: 0.6858 - acc: 0.5407 - val_loss: 0.6623 - val_acc: 0.6536\n",
      "Epoch 778/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6809 - acc: 0.5337 - val_loss: 0.6623 - val_acc: 0.6536\n",
      "Epoch 779/1000\n",
      "712/712 [==============================] - 0s 97us/step - loss: 0.6868 - acc: 0.5478 - val_loss: 0.6623 - val_acc: 0.6536\n",
      "Epoch 780/1000\n",
      "712/712 [==============================] - 0s 103us/step - loss: 0.6904 - acc: 0.5211 - val_loss: 0.6623 - val_acc: 0.6536\n",
      "Epoch 781/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6908 - acc: 0.5435 - val_loss: 0.6623 - val_acc: 0.6536\n",
      "Epoch 782/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6821 - acc: 0.5590 - val_loss: 0.6623 - val_acc: 0.6536\n",
      "Epoch 783/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6810 - acc: 0.5548 - val_loss: 0.6623 - val_acc: 0.6536\n",
      "Epoch 784/1000\n",
      "712/712 [==============================] - 0s 106us/step - loss: 0.6803 - acc: 0.5702 - val_loss: 0.6622 - val_acc: 0.6536\n",
      "Epoch 785/1000\n",
      "712/712 [==============================] - 0s 102us/step - loss: 0.6877 - acc: 0.5506 - val_loss: 0.6622 - val_acc: 0.6536\n",
      "Epoch 786/1000\n",
      "712/712 [==============================] - 0s 103us/step - loss: 0.6777 - acc: 0.5857 - val_loss: 0.6622 - val_acc: 0.6536\n",
      "Epoch 787/1000\n",
      "712/712 [==============================] - 0s 106us/step - loss: 0.6851 - acc: 0.5393 - val_loss: 0.6622 - val_acc: 0.6536\n",
      "Epoch 788/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6815 - acc: 0.5716 - val_loss: 0.6622 - val_acc: 0.6536\n",
      "Epoch 789/1000\n",
      "712/712 [==============================] - 0s 109us/step - loss: 0.6895 - acc: 0.5295 - val_loss: 0.6622 - val_acc: 0.6536\n",
      "Epoch 790/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6840 - acc: 0.5730 - val_loss: 0.6622 - val_acc: 0.6536\n",
      "Epoch 791/1000\n",
      "712/712 [==============================] - 0s 120us/step - loss: 0.6862 - acc: 0.5295 - val_loss: 0.6621 - val_acc: 0.6536\n",
      "Epoch 792/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6817 - acc: 0.5674 - val_loss: 0.6621 - val_acc: 0.6536\n",
      "Epoch 793/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6765 - acc: 0.5787 - val_loss: 0.6621 - val_acc: 0.6536\n",
      "Epoch 794/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6828 - acc: 0.5351 - val_loss: 0.6621 - val_acc: 0.6536\n",
      "Epoch 795/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6840 - acc: 0.5506 - val_loss: 0.6621 - val_acc: 0.6536\n",
      "Epoch 796/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6741 - acc: 0.5829 - val_loss: 0.6621 - val_acc: 0.6536\n",
      "Epoch 797/1000\n",
      "712/712 [==============================] - 0s 120us/step - loss: 0.6728 - acc: 0.5758 - val_loss: 0.6620 - val_acc: 0.6536\n",
      "Epoch 798/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6815 - acc: 0.5772 - val_loss: 0.6620 - val_acc: 0.6536\n",
      "Epoch 799/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6828 - acc: 0.5674 - val_loss: 0.6620 - val_acc: 0.6536\n",
      "Epoch 800/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6771 - acc: 0.5730 - val_loss: 0.6620 - val_acc: 0.6536\n",
      "Epoch 801/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6855 - acc: 0.5576 - val_loss: 0.6620 - val_acc: 0.6536\n",
      "Epoch 802/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6894 - acc: 0.5211 - val_loss: 0.6620 - val_acc: 0.6536\n",
      "Epoch 803/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6853 - acc: 0.5646 - val_loss: 0.6620 - val_acc: 0.6536\n",
      "Epoch 804/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6856 - acc: 0.5478 - val_loss: 0.6619 - val_acc: 0.6536\n",
      "Epoch 805/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6751 - acc: 0.5871 - val_loss: 0.6619 - val_acc: 0.6536\n",
      "Epoch 806/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6814 - acc: 0.5730 - val_loss: 0.6619 - val_acc: 0.6536\n",
      "Epoch 807/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6784 - acc: 0.5548 - val_loss: 0.6619 - val_acc: 0.6536\n",
      "Epoch 808/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6789 - acc: 0.5913 - val_loss: 0.6619 - val_acc: 0.6536\n",
      "Epoch 809/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6805 - acc: 0.5421 - val_loss: 0.6619 - val_acc: 0.6536\n",
      "Epoch 810/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6828 - acc: 0.5618 - val_loss: 0.6619 - val_acc: 0.6536\n",
      "Epoch 811/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6858 - acc: 0.5449 - val_loss: 0.6618 - val_acc: 0.6536\n",
      "Epoch 812/1000\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.6819 - acc: 0.5646 - val_loss: 0.6618 - val_acc: 0.6536\n",
      "Epoch 813/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6821 - acc: 0.5337 - val_loss: 0.6618 - val_acc: 0.6536\n",
      "Epoch 814/1000\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.6764 - acc: 0.5829 - val_loss: 0.6618 - val_acc: 0.6536\n",
      "Epoch 815/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6835 - acc: 0.5801 - val_loss: 0.6618 - val_acc: 0.6536\n",
      "Epoch 816/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6795 - acc: 0.5632 - val_loss: 0.6618 - val_acc: 0.6536\n",
      "Epoch 817/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6896 - acc: 0.5154 - val_loss: 0.6617 - val_acc: 0.6536\n",
      "Epoch 818/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6833 - acc: 0.5379 - val_loss: 0.6617 - val_acc: 0.6536\n",
      "Epoch 819/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6830 - acc: 0.5253 - val_loss: 0.6617 - val_acc: 0.6536\n",
      "Epoch 820/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6827 - acc: 0.5478 - val_loss: 0.6617 - val_acc: 0.6536\n",
      "Epoch 821/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6803 - acc: 0.5730 - val_loss: 0.6617 - val_acc: 0.6536\n",
      "Epoch 822/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6915 - acc: 0.5323 - val_loss: 0.6617 - val_acc: 0.6536\n",
      "Epoch 823/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6832 - acc: 0.5534 - val_loss: 0.6617 - val_acc: 0.6536\n",
      "Epoch 824/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6760 - acc: 0.5702 - val_loss: 0.6616 - val_acc: 0.6536\n",
      "Epoch 825/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6768 - acc: 0.5815 - val_loss: 0.6616 - val_acc: 0.6536\n",
      "Epoch 826/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6841 - acc: 0.5506 - val_loss: 0.6616 - val_acc: 0.6536\n",
      "Epoch 827/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6828 - acc: 0.5435 - val_loss: 0.6616 - val_acc: 0.6536\n",
      "Epoch 828/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 146us/step - loss: 0.6783 - acc: 0.5772 - val_loss: 0.6616 - val_acc: 0.6536\n",
      "Epoch 829/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6827 - acc: 0.5562 - val_loss: 0.6616 - val_acc: 0.6536\n",
      "Epoch 830/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6788 - acc: 0.5632 - val_loss: 0.6616 - val_acc: 0.6536\n",
      "Epoch 831/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6819 - acc: 0.5576 - val_loss: 0.6615 - val_acc: 0.6536\n",
      "Epoch 832/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6760 - acc: 0.5829 - val_loss: 0.6615 - val_acc: 0.6536\n",
      "Epoch 833/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6785 - acc: 0.5801 - val_loss: 0.6615 - val_acc: 0.6536\n",
      "Epoch 834/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6775 - acc: 0.5688 - val_loss: 0.6615 - val_acc: 0.6536\n",
      "Epoch 835/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6903 - acc: 0.5295 - val_loss: 0.6615 - val_acc: 0.6536\n",
      "Epoch 836/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6840 - acc: 0.5590 - val_loss: 0.6615 - val_acc: 0.6536\n",
      "Epoch 837/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6838 - acc: 0.5506 - val_loss: 0.6614 - val_acc: 0.6536\n",
      "Epoch 838/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6886 - acc: 0.5393 - val_loss: 0.6614 - val_acc: 0.6536\n",
      "Epoch 839/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6827 - acc: 0.5801 - val_loss: 0.6614 - val_acc: 0.6536\n",
      "Epoch 840/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6837 - acc: 0.5646 - val_loss: 0.6614 - val_acc: 0.6536\n",
      "Epoch 841/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6773 - acc: 0.5744 - val_loss: 0.6614 - val_acc: 0.6536\n",
      "Epoch 842/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6808 - acc: 0.5576 - val_loss: 0.6614 - val_acc: 0.6536\n",
      "Epoch 843/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6765 - acc: 0.5843 - val_loss: 0.6614 - val_acc: 0.6536\n",
      "Epoch 844/1000\n",
      "712/712 [==============================] - 0s 120us/step - loss: 0.6787 - acc: 0.5744 - val_loss: 0.6613 - val_acc: 0.6536\n",
      "Epoch 845/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6831 - acc: 0.5688 - val_loss: 0.6613 - val_acc: 0.6536\n",
      "Epoch 846/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6873 - acc: 0.5253 - val_loss: 0.6613 - val_acc: 0.6536\n",
      "Epoch 847/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6853 - acc: 0.5478 - val_loss: 0.6613 - val_acc: 0.6536\n",
      "Epoch 848/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6710 - acc: 0.6081 - val_loss: 0.6613 - val_acc: 0.6536\n",
      "Epoch 849/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6773 - acc: 0.5702 - val_loss: 0.6613 - val_acc: 0.6536\n",
      "Epoch 850/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6827 - acc: 0.5604 - val_loss: 0.6613 - val_acc: 0.6536\n",
      "Epoch 851/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6783 - acc: 0.5562 - val_loss: 0.6612 - val_acc: 0.6536\n",
      "Epoch 852/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6780 - acc: 0.5604 - val_loss: 0.6612 - val_acc: 0.6536\n",
      "Epoch 853/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6799 - acc: 0.5674 - val_loss: 0.6612 - val_acc: 0.6536\n",
      "Epoch 854/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6803 - acc: 0.5506 - val_loss: 0.6612 - val_acc: 0.6536\n",
      "Epoch 855/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6760 - acc: 0.5885 - val_loss: 0.6612 - val_acc: 0.6536\n",
      "Epoch 856/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6776 - acc: 0.5758 - val_loss: 0.6612 - val_acc: 0.6536\n",
      "Epoch 857/1000\n",
      "712/712 [==============================] - 0s 120us/step - loss: 0.6913 - acc: 0.5337 - val_loss: 0.6611 - val_acc: 0.6536\n",
      "Epoch 858/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6864 - acc: 0.5211 - val_loss: 0.6611 - val_acc: 0.6536\n",
      "Epoch 859/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6830 - acc: 0.5632 - val_loss: 0.6611 - val_acc: 0.6536\n",
      "Epoch 860/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6850 - acc: 0.5492 - val_loss: 0.6611 - val_acc: 0.6536\n",
      "Epoch 861/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6795 - acc: 0.5730 - val_loss: 0.6611 - val_acc: 0.6536\n",
      "Epoch 862/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6850 - acc: 0.5379 - val_loss: 0.6611 - val_acc: 0.6536\n",
      "Epoch 863/1000\n",
      "712/712 [==============================] - 0s 98us/step - loss: 0.6847 - acc: 0.5449 - val_loss: 0.6611 - val_acc: 0.6536\n",
      "Epoch 864/1000\n",
      "712/712 [==============================] - 0s 101us/step - loss: 0.6850 - acc: 0.5449 - val_loss: 0.6610 - val_acc: 0.6536\n",
      "Epoch 865/1000\n",
      "712/712 [==============================] - 0s 108us/step - loss: 0.6771 - acc: 0.5927 - val_loss: 0.6610 - val_acc: 0.6536\n",
      "Epoch 866/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6828 - acc: 0.5660 - val_loss: 0.6610 - val_acc: 0.6536\n",
      "Epoch 867/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6841 - acc: 0.5618 - val_loss: 0.6610 - val_acc: 0.6536\n",
      "Epoch 868/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6758 - acc: 0.5787 - val_loss: 0.6610 - val_acc: 0.6536\n",
      "Epoch 869/1000\n",
      "712/712 [==============================] - 0s 99us/step - loss: 0.6734 - acc: 0.5857 - val_loss: 0.6610 - val_acc: 0.6536\n",
      "Epoch 870/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6849 - acc: 0.5463 - val_loss: 0.6609 - val_acc: 0.6536\n",
      "Epoch 871/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6835 - acc: 0.5548 - val_loss: 0.6609 - val_acc: 0.6536\n",
      "Epoch 872/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6831 - acc: 0.5337 - val_loss: 0.6609 - val_acc: 0.6536\n",
      "Epoch 873/1000\n",
      "712/712 [==============================] - 0s 104us/step - loss: 0.6769 - acc: 0.5843 - val_loss: 0.6609 - val_acc: 0.6536\n",
      "Epoch 874/1000\n",
      "712/712 [==============================] - 0s 102us/step - loss: 0.6794 - acc: 0.5660 - val_loss: 0.6609 - val_acc: 0.6536\n",
      "Epoch 875/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6758 - acc: 0.5618 - val_loss: 0.6609 - val_acc: 0.6536\n",
      "Epoch 876/1000\n",
      "712/712 [==============================] - 0s 103us/step - loss: 0.6862 - acc: 0.5534 - val_loss: 0.6609 - val_acc: 0.6536\n",
      "Epoch 877/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6801 - acc: 0.5590 - val_loss: 0.6608 - val_acc: 0.6536\n",
      "Epoch 878/1000\n",
      "712/712 [==============================] - 0s 108us/step - loss: 0.6774 - acc: 0.5562 - val_loss: 0.6608 - val_acc: 0.6536\n",
      "Epoch 879/1000\n",
      "712/712 [==============================] - 0s 105us/step - loss: 0.6752 - acc: 0.5871 - val_loss: 0.6608 - val_acc: 0.6536\n",
      "Epoch 880/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6848 - acc: 0.5379 - val_loss: 0.6608 - val_acc: 0.6536\n",
      "Epoch 881/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6844 - acc: 0.5492 - val_loss: 0.6608 - val_acc: 0.6536\n",
      "Epoch 882/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6752 - acc: 0.5744 - val_loss: 0.6608 - val_acc: 0.6536\n",
      "Epoch 883/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6866 - acc: 0.5506 - val_loss: 0.6607 - val_acc: 0.6536\n",
      "Epoch 884/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6806 - acc: 0.5520 - val_loss: 0.6607 - val_acc: 0.6536\n",
      "Epoch 885/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6870 - acc: 0.5435 - val_loss: 0.6607 - val_acc: 0.6536\n",
      "Epoch 886/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6783 - acc: 0.5646 - val_loss: 0.6607 - val_acc: 0.6536\n",
      "Epoch 887/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6792 - acc: 0.5534 - val_loss: 0.6607 - val_acc: 0.6536\n",
      "Epoch 888/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6783 - acc: 0.5604 - val_loss: 0.6607 - val_acc: 0.6536\n",
      "Epoch 889/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6812 - acc: 0.5562 - val_loss: 0.6607 - val_acc: 0.6536\n",
      "Epoch 890/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6823 - acc: 0.5463 - val_loss: 0.6606 - val_acc: 0.6536\n",
      "Epoch 891/1000\n",
      " 32/712 [>.............................] - ETA: 0s - loss: 0.7262 - acc: 0.3438"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-a6024ef984eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_reduced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/tensorflow/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn = Sequential([\n",
    "    Dense(64, input_shape=(train_reduced.shape[1],), activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10, verbose=1, mode='auto')\n",
    "callback_list = [earlystop]\n",
    "sgd = optimizers.SGD(lr=1e-6, decay=1e-8, momentum=0.9, nesterov=True)\n",
    "# For a binary classification problem\n",
    "nn.compile(optimizer=sgd,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "nn.fit(train_reduced, y, epochs=1000, batch_size=32, validation_split=0.2, shuffle=True, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
