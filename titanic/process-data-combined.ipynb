{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T10:07:41.534134Z",
     "start_time": "2018-09-27T10:07:41.498670Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import pandas as pd\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, load_model\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T10:07:43.839839Z",
     "start_time": "2018-09-27T10:07:43.817287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n",
      "----------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      "PassengerId    418 non-null int64\n",
      "Pclass         418 non-null int64\n",
      "Name           418 non-null object\n",
      "Sex            418 non-null object\n",
      "Age            332 non-null float64\n",
      "SibSp          418 non-null int64\n",
      "Parch          418 non-null int64\n",
      "Ticket         418 non-null object\n",
      "Fare           417 non-null float64\n",
      "Cabin          91 non-null object\n",
      "Embarked       418 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "train_df.info()\n",
    "print('-'*40)\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T10:21:41.315679Z",
     "start_time": "2018-09-27T10:21:41.185961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PassengerId  Pclass                                               Name  \\\n",
      "0           892       3                                   Kelly, Mr. James   \n",
      "1           893       3                   Wilkes, Mrs. James (Ellen Needs)   \n",
      "2           894       2                          Myles, Mr. Thomas Francis   \n",
      "3           895       3                                   Wirz, Mr. Albert   \n",
      "4           896       3       Hirvonen, Mrs. Alexander (Helga E Lindqvist)   \n",
      "5           897       3                         Svensson, Mr. Johan Cervin   \n",
      "6           898       3                               Connolly, Miss. Kate   \n",
      "7           899       2                       Caldwell, Mr. Albert Francis   \n",
      "8           900       3          Abrahim, Mrs. Joseph (Sophie Halaut Easu)   \n",
      "9           901       3                            Davies, Mr. John Samuel   \n",
      "10          902       3                                   Ilieff, Mr. Ylio   \n",
      "11          903       1                         Jones, Mr. Charles Cresson   \n",
      "12          904       1      Snyder, Mrs. John Pillsbury (Nelle Stevenson)   \n",
      "13          905       2                               Howard, Mr. Benjamin   \n",
      "14          906       1  Chaffee, Mrs. Herbert Fuller (Carrie Constance...   \n",
      "15          907       2      del Carlo, Mrs. Sebastiano (Argenia Genovesi)   \n",
      "16          908       2                                  Keane, Mr. Daniel   \n",
      "17          909       3                                  Assaf, Mr. Gerios   \n",
      "18          910       3                       Ilmakangas, Miss. Ida Livija   \n",
      "19          911       3              Assaf Khalil, Mrs. Mariana (Miriam\")\"   \n",
      "20          912       1                             Rothschild, Mr. Martin   \n",
      "21          913       3                          Olsen, Master. Artur Karl   \n",
      "22          914       1               Flegenheim, Mrs. Alfred (Antoinette)   \n",
      "23          915       1                    Williams, Mr. Richard Norris II   \n",
      "24          916       1    Ryerson, Mrs. Arthur Larned (Emily Maria Borie)   \n",
      "25          917       3                            Robins, Mr. Alexander A   \n",
      "26          918       1                       Ostby, Miss. Helene Ragnhild   \n",
      "27          919       3                                  Daher, Mr. Shedid   \n",
      "28          920       1                            Brady, Mr. John Bertram   \n",
      "29          921       3                                  Samaan, Mr. Elias   \n",
      "..          ...     ...                                                ...   \n",
      "70          962       3                          Mulvihill, Miss. Bertha E   \n",
      "71          963       3                                 Minkoff, Mr. Lazar   \n",
      "72          964       3                     Nieminen, Miss. Manta Josefina   \n",
      "73          965       1                    Ovies y Rodriguez, Mr. Servando   \n",
      "74          966       1                               Geiger, Miss. Amalie   \n",
      "75          967       1                                 Keeping, Mr. Edwin   \n",
      "76          968       3                                   Miles, Mr. Frank   \n",
      "77          969       1  Cornell, Mrs. Robert Clifford (Malvina Helen L...   \n",
      "78          970       2                     Aldworth, Mr. Charles Augustus   \n",
      "79          971       3                             Doyle, Miss. Elizabeth   \n",
      "80          972       3                               Boulos, Master. Akar   \n",
      "81          973       1                                 Straus, Mr. Isidor   \n",
      "82          974       1                             Case, Mr. Howard Brown   \n",
      "83          975       3                               Demetri, Mr. Marinko   \n",
      "84          976       2                              Lamb, Mr. John Joseph   \n",
      "85          977       3                                 Khalil, Mr. Betros   \n",
      "86          978       3                                 Barry, Miss. Julia   \n",
      "87          979       3                         Badman, Miss. Emily Louisa   \n",
      "88          980       3                            O'Donoghue, Ms. Bridget   \n",
      "89          981       2                        Wells, Master. Ralph Lester   \n",
      "90          982       3  Dyker, Mrs. Adolf Fredrik (Anna Elisabeth Judi...   \n",
      "91          983       3                                 Pedersen, Mr. Olaf   \n",
      "92          984       1               Davidson, Mrs. Thornton (Orian Hays)   \n",
      "93          985       3                                  Guest, Mr. Robert   \n",
      "94          986       1                                Birnbaum, Mr. Jakob   \n",
      "95          987       3                         Tenglin, Mr. Gunnar Isidor   \n",
      "96          988       1  Cavendish, Mrs. Tyrell William (Julia Florence...   \n",
      "97          989       3                          Makinen, Mr. Kalle Edvard   \n",
      "98          990       3                       Braf, Miss. Elin Ester Maria   \n",
      "99          991       3                       Nancarrow, Mr. William Henry   \n",
      "\n",
      "       Sex   Age  SibSp  Parch             Ticket      Fare            Cabin  \\\n",
      "0     male  34.5      0      0             330911    7.8292              NaN   \n",
      "1   female  47.0      1      0             363272    7.0000              NaN   \n",
      "2     male  62.0      0      0             240276    9.6875              NaN   \n",
      "3     male  27.0      0      0             315154    8.6625              NaN   \n",
      "4   female  22.0      1      1            3101298   12.2875              NaN   \n",
      "5     male  14.0      0      0               7538    9.2250              NaN   \n",
      "6   female  30.0      0      0             330972    7.6292              NaN   \n",
      "7     male  26.0      1      1             248738   29.0000              NaN   \n",
      "8   female  18.0      0      0               2657    7.2292              NaN   \n",
      "9     male  21.0      2      0          A/4 48871   24.1500              NaN   \n",
      "10    male   NaN      0      0             349220    7.8958              NaN   \n",
      "11    male  46.0      0      0                694   26.0000              NaN   \n",
      "12  female  23.0      1      0              21228   82.2667              B45   \n",
      "13    male  63.0      1      0              24065   26.0000              NaN   \n",
      "14  female  47.0      1      0        W.E.P. 5734   61.1750              E31   \n",
      "15  female  24.0      1      0      SC/PARIS 2167   27.7208              NaN   \n",
      "16    male  35.0      0      0             233734   12.3500              NaN   \n",
      "17    male  21.0      0      0               2692    7.2250              NaN   \n",
      "18  female  27.0      1      0   STON/O2. 3101270    7.9250              NaN   \n",
      "19  female  45.0      0      0               2696    7.2250              NaN   \n",
      "20    male  55.0      1      0           PC 17603   59.4000              NaN   \n",
      "21    male   9.0      0      1            C 17368    3.1708              NaN   \n",
      "22  female   NaN      0      0           PC 17598   31.6833              NaN   \n",
      "23    male  21.0      0      1           PC 17597   61.3792              NaN   \n",
      "24  female  48.0      1      3           PC 17608  262.3750  B57 B59 B63 B66   \n",
      "25    male  50.0      1      0          A/5. 3337   14.5000              NaN   \n",
      "26  female  22.0      0      1             113509   61.9792              B36   \n",
      "27    male  22.5      0      0               2698    7.2250              NaN   \n",
      "28    male  41.0      0      0             113054   30.5000              A21   \n",
      "29    male   NaN      2      0               2662   21.6792              NaN   \n",
      "..     ...   ...    ...    ...                ...       ...              ...   \n",
      "70  female  24.0      0      0             382653    7.7500              NaN   \n",
      "71    male  21.0      0      0             349211    7.8958              NaN   \n",
      "72  female  29.0      0      0            3101297    7.9250              NaN   \n",
      "73    male  28.5      0      0           PC 17562   27.7208              D43   \n",
      "74  female  35.0      0      0             113503  211.5000             C130   \n",
      "75    male  32.5      0      0             113503  211.5000             C132   \n",
      "76    male   NaN      0      0             359306    8.0500              NaN   \n",
      "77  female  55.0      2      0              11770   25.7000             C101   \n",
      "78    male  30.0      0      0             248744   13.0000              NaN   \n",
      "79  female  24.0      0      0             368702    7.7500              NaN   \n",
      "80    male   6.0      1      1               2678   15.2458              NaN   \n",
      "81    male  67.0      1      0           PC 17483  221.7792          C55 C57   \n",
      "82    male  49.0      0      0              19924   26.0000              NaN   \n",
      "83    male   NaN      0      0             349238    7.8958              NaN   \n",
      "84    male   NaN      0      0             240261   10.7083              NaN   \n",
      "85    male   NaN      1      0               2660   14.4542              NaN   \n",
      "86  female  27.0      0      0             330844    7.8792              NaN   \n",
      "87  female  18.0      0      0          A/4 31416    8.0500              NaN   \n",
      "88  female   NaN      0      0             364856    7.7500              NaN   \n",
      "89    male   2.0      1      1              29103   23.0000              NaN   \n",
      "90  female  22.0      1      0             347072   13.9000              NaN   \n",
      "91    male   NaN      0      0             345498    7.7750              NaN   \n",
      "92  female  27.0      1      2         F.C. 12750   52.0000              B71   \n",
      "93    male   NaN      0      0             376563    8.0500              NaN   \n",
      "94    male  25.0      0      0              13905   26.0000              NaN   \n",
      "95    male  25.0      0      0             350033    7.7958              NaN   \n",
      "96  female  76.0      1      0              19877   78.8500              C46   \n",
      "97    male  29.0      0      0  STON/O 2. 3101268    7.9250              NaN   \n",
      "98  female  20.0      0      0             347471    7.8542              NaN   \n",
      "99    male  33.0      0      0         A./5. 3338    8.0500              NaN   \n",
      "\n",
      "   Embarked Title  \n",
      "0         Q    Mr  \n",
      "1         S   Mrs  \n",
      "2         Q    Mr  \n",
      "3         S    Mr  \n",
      "4         S   Mrs  \n",
      "5         S    Mr  \n",
      "6         Q  Miss  \n",
      "7         S    Mr  \n",
      "8         C   Mrs  \n",
      "9         S    Mr  \n",
      "10        S    Mr  \n",
      "11        S    Mr  \n",
      "12        S   Mrs  \n",
      "13        S    Mr  \n",
      "14        S   Mrs  \n",
      "15        C   Mrs  \n",
      "16        Q    Mr  \n",
      "17        C    Mr  \n",
      "18        S  Miss  \n",
      "19        C   Mrs  \n",
      "20        C    Mr  \n",
      "21        S   Boy  \n",
      "22        S   Mrs  \n",
      "23        C    Mr  \n",
      "24        C   Mrs  \n",
      "25        S    Mr  \n",
      "26        C  Miss  \n",
      "27        C    Mr  \n",
      "28        S    Mr  \n",
      "29        C    Mr  \n",
      "..      ...   ...  \n",
      "70        Q  Miss  \n",
      "71        S    Mr  \n",
      "72        S  Miss  \n",
      "73        C    Mr  \n",
      "74        C  Miss  \n",
      "75        C    Mr  \n",
      "76        S    Mr  \n",
      "77        S   Mrs  \n",
      "78        S    Mr  \n",
      "79        Q  Miss  \n",
      "80        C   Boy  \n",
      "81        S    Mr  \n",
      "82        S    Mr  \n",
      "83        S    Mr  \n",
      "84        Q    Mr  \n",
      "85        C    Mr  \n",
      "86        Q  Miss  \n",
      "87        S  Miss  \n",
      "88        Q  Miss  \n",
      "89        S   Boy  \n",
      "90        S   Mrs  \n",
      "91        S    Mr  \n",
      "92        S   Mrs  \n",
      "93        S    Mr  \n",
      "94        C    Mr  \n",
      "95        S    Mr  \n",
      "96        S   Mrs  \n",
      "97        S    Mr  \n",
      "98        S  Miss  \n",
      "99        S    Mr  \n",
      "\n",
      "[100 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "combine = [train_df, test_df]\n",
    "y = train_df['Survived']\n",
    "train_df.drop(['Survived'], axis=1, inplace=True)\n",
    "test_ids = test_df['PassengerId']\n",
    "def woman_child_or_man(passenger):\n",
    "    age, sex = passenger\n",
    "    if age < 16:\n",
    "        return \"child\"\n",
    "    else:\n",
    "        return dict(male=\"man\", female=\"woman\")[sex]\n",
    "    \n",
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    dataset.loc[dataset.Title=='Ms', 'Title'] = 'Miss' # unify the naming\n",
    "    dataset['Title'] = dataset['Title'].replace(['Mme', 'Mlle'], 'Mrs') # seem to be the title for old lady\n",
    "    dataset.loc[((dataset.Title=='Master') | (dataset.Title.isnull())) & (dataset.Age<15) & (dataset.Sex=='male'), 'Title'] = 'Boy'\n",
    "    dataset.loc[((dataset.Title=='Mrs') | dataset.Title.isnull() | (dataset.Title=='Miss') ) & (dataset.Sex=='female') & (dataset.Age<15), 'Title'] = 'Girl'\n",
    "    dataset['Title'] = dataset['Title'].replace(['Jonkheer', 'Don', 'Sir', 'Countess', 'Lady', 'Dona'], 'Royalty')\n",
    "    dataset['Title'] = dataset['Title'].replace(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 'Officer')\n",
    "#     dataset['Who'] = dataset[['Age', 'Sex']].apply(woman_child_or_man, axis=1) \n",
    "#     dataset['Adult_Male'] = dataset['Who'] == 'man'\n",
    "#     dataset['Adult_Female'] = dataset['Who'] == 'woman'\n",
    "#     print(dataset.loc[(dataset.Title=='Mrs') & dataset.Age<10][['Title', 'Name', 'Age']].sort_values('Age'))\n",
    "print(dataset.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = dict(man=\"#4682B4\", woman=\"#CD5C5C\", child=\"#2E8B57\", male=\"#6495ED\", female=\"#F08080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7faad27c6b38>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot('Sex', data=train_df, kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7faad046cb70>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot('Who', data=train_df, kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7faad05e4b70>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbwAAAFgCAYAAADATMyLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGWdJREFUeJzt3XvUXXV95/H3h0QKghK56SAE4khwlJRLRFhWaGm9TVusdqxKR7E6Tot4oy61ig5eWh3G0JFBQPBWBV0uh1VF6qXSjqOSomID4SpGMYFElEtDEByhSr7zx9mhh0OeJCc8zznneX7v11p7nWf/fnvv8z05a/Hht8/e+5eqQpKkuW6HcRcgSdIoGHiSpCYYeJKkJhh4kqQmGHiSpCYYeJKkJhh4kqQmGHiSpCYYeJKkJhh4kqQmzNnASzI/yQFJ5o+7FknS+M3lMNgXWL169epx1yFJ2yvjLmAumbMjPEmS+hl4kqQmGHiSpCYYeJKkJhh4kqQmGHiSpCYYeJKkJhh4kqQmjCzwklyU5KokVya5NMmhXfuaJDckWdktz+nb56hun1VJLkmy96jqlSTNLaN80srLq+ougCR/AHwcOLzre2FVXdu/cZIdgE8Bf1JVy5O8AzgNeOUIa5YkzREjC7xNYdfZDdi4lV2WAvdW1fJu/VxgDZsJvCQLgAUDzftuX6WSpLlopM/STPJR4Nn0ng/33L6uTycJsBw4pao2AAuBmzZtUFV3JNkhye5VtX7g0CcD75zZ6iVJs9lIL1qpqldV1ULgFGBZ13x0VR0CHEEvCM/ajkOfASwaWI5++BVLkuaKscyWUFUXJPlwkj2qam3Xdl+Sc4CLu81uBvbftE+SPYGNmxnd0Y0IN/S39QaMkqbT0jefP+4StmrFshPGXYIm1EhGeEl2TbJf3/pxwHrg3iS7dW0BXgKs7DZbAeyc5Bnd+onAhaOoV5I094xqhLcLcGGSXYD76YXdccBjgb9NMg+YB1wPnARQVRuTvAw4L8lO9C5YeemI6pUkzTEjCbyquhU4aoruw7aw32XAkhkpSpLUFJ+0IklqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqwsgCL8lFSa5KcmWSS5Mc2rUvTvKtJKu61wP79pmyT5KkYYxyhPfyqjqkqg4DTgc+3rWfC5xdVYuBs4Hz+vbZUp8kSdtsZIFXVXf1re4GbEyyN3A48Jmu/TPA4Un22lLfqGqWJM0d80f5Zkk+CjwbCPBcYD/gx1V1P0BV3Z/klq49W+i7feC4C4AFA2+370x+FknS7DLSi1aq6lVVtRA4BVg2jYc+GVg9sFw6jceXJM1yY7lKs6ouAI4F1gGPTzIPoHvdB1jbLVP1DToDWDSwHD3DH0OSNIuMJPCS7Jpkv77144D1wG3ASuD4rut44Mqqur2qpuwbPH5VbaiqNf0LvTCVJAkY3W94uwAXJtkFuJ9e2B1XVZXkROCTSU4F7gRO6NtvS32SJG2zkQReVd0KHDVF3w3AkcP2SZI0DJ+0IklqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWqCgSdJaoKBJ0lqgoEnSWrCSAIvyR5Jvpzk+0muSfK5JHt1fZXk6iQru2VJ337HJbkhyQ+TfDbJI0dRryRp7hnVCK+A91fVQVW1BLgROK2v/+lVdWi3XAOQZFfgI8BxVfVE4G7gTSOqV5I0x4wk8KpqfVV9va/p28D+W9ntPwL/XFU/6NbPBV68uQ2TLEhyQP8C7PvwqpYkzSXzR/2GSXYAXg1c3Nf89STzga8A76qq+4CFwE1929wM7DfFYU8G3jkD5UqS5ohxXLTyQeAe4KxufWFVPRU4Bngy8N+245hnAIsGlqMffqmSpLlipCO8JKcDB9L7XW4jQFWt7V5/luSjwBu7zW8Gju3bfSGwdnPHraoNwIaB95re4iVJs9rIRnhJ3gcsBZ7fnbIkyWOS7Nz9PR94IbCy2+XvgSOSHNitnwj871HVK0maW0Z1W8JTgLcB+wCXdbcffB54EvCdJFcBVwO/pDulWVV3A38KfDHJD4HdgNNHUa8kae4ZySnNqroOmOoc469vYb8vAF+YkaIkSU3xSSuSpCYYeJKkJhh4kqQmGHiSpCYYeJKkJhh4kqQmGHiSpCYYeJKkJhh4kqQmGHiSpCYYeJKkJhh4kqQmGHiSpCYYeJKkJhh4kqQmGHiSpCYYeJKkJhh4kqQmGHiSpCYYeJKkJhh4kqQmGHiSpCYYeJKkJhh4kqQmGHiSpCYYeJKkJhh4kqQmGHiSpCYYeJKkJhh4kqQmGHiSpCYYeJKkJowk8JLskeTLSb6f5Jokn0uyV9d3VJKrkqxKckmSvfv2m7JPkqRhjGqEV8D7q+qgqloC3AiclmQH4FPAa6pqMfBN4DSALfVJkjSskQReVa2vqq/3NX0b2B9YCtxbVcu79nOBF3V/b6nvQZIsSHJA/wLsO72fQpI0m21z4CV50xTtbxzmDbuR26uBi4GFwE2b+qrqDmCHJLtvpW/QycDqgeXSYeqSJM1tw4zwTp2i/R1DvucHgXuAs4bcb0vOABYNLEdP4/ElSbPc/K1tkOS3uz/nJTkWSF/3E4C7t/XNkpwOHAgcV1Ubk9xM79Tmpv49gY1VtX5LfYPHraoNwIaB99rWsiRJDdhq4AEf6153Aj7e117AT4HXbcsbJXkfvd/lfq+q7uuaVwA7J3lG91vdicCF29AnSdJQthp4VbUIIMn5VXXC9rxJkqcAbwNWAZd1o6/VVfWCJC8DzkuyE7AGeGn3vhun6pMkaVjbMsIDoD/sugtP+vs2bmXf63jwqdD+vsuAJcP2SZI0jGGu0jw8ybeS/Bz4Zbf8qnuVJGmibfMID/gk8HfAK4H/NzPlSJI0M4YJvP2Bt1dVzVQxkiTNlGHuw/s88OyZKkSSpJk0zAhvJ+DzSZbTux3hAdt79aYkaXIleTPwcmAjvQsP31FVXxhvVdtvmMC7vlskSXNckiOBFwNPrap7kzwK2HPMZT0sw9yW8O6ZLESSNFH2Ae4A7gOoqruBu5PsDHwAOJzemb8PVdWHkvw+8FbgN4FdgMuBF1XV1eMofnO2OfD6HjH2EFX1tekpR5I0IS4B3g78MMnXgM9X1ZfpPUTku1V1YvdQkMuS/ENVfTHJ84C/AA4CPjZJYQfDndL82MD6XsCOwDp6z9SUJM0RVfXzJE8Dng78FnBWkr8BfhfYKcmmx0ruBiwGfgj8OXAVcAvwipEXvRXDnNJc1L+eZB69mRK2+eHRkqTZo3uK1nJgeZKv0nue8r8CL6mqazezy78DHkEvBHcE7h1VrdtiuyeArar7gfcCb5m+ciRJkyDJQUme1Nd0GL05Sr8CvGHTIyaTLE6yazcIOh84CfgicNqoa96aYU5pbs6z6F2uKkmaW3YFzuwm3b4PuJXerDU/AU4HrkpvJoDbgf8EvAG4pqq+lOQS4NtJfqeq/s94yn+oYS5aWUtvSqBNHknvCp2TprsoSdJ4VdUK4Dem6H7tZtre27fvL+lNBzdRhhnhDU7N83NgVVX9bBrrkSRpRgxz0co34IGpgR4L3Lq1aYEkSZoUw0wP9Kgk5wO/AH4M/CLJJ5PsNmPVSZI0TYa5SvOD9O6eXwLs3L0+EjhzBuqSJGlaDfMb3nOBJ1TVprnwViV5BXDj9JclSdL0GmaEdy+9p6v025PuOWuSJE2yYUZ4HwX+Icn/pHfz4f70HiPzkZkoTJL0UEvffP6MTMK9YtkJmYnjbk6SdwG7VtWbRvWeMFzgvZfexSr/md5TtG8B3l9Vg8/YlCRp4gxzSvN/Ad+vqmdW1ZOr6pnA95KcMUO1SZImTJJK8vYk303yoyS/k+S/J7kyybVJ/kO33eOS/N8kK5Jcl+T9WzjmXyS5PMkVSf4uyeNmovZhAu944J8H2lYAfzx95UiSZoENVXUEvamAvgD8U1UdRu9Zmm/ftA1wXFUtBQ4FnprkuYMHSvJS4N8DR1XV4cCXgb+eiaKHOaVZwLyBtnk8jAdQS5Jmpc92r1cAVVVf7NZXAH/Y/T0PWJbk6UCAx9ELvr8fONbzgKcCV/Qezcl84K6ZKHqYwLsU+Mskb6mqjd0TV97VtUuS2rFp2p/7efCV+vfzb7nyRuAxwJFVdW+SD9N7/vKgAH9VVR+fqWI3GWZ09gbgmcBPklxO76KVZwGv2+JekqQWLQB+0oXd44E/mGK7i4GTkjwGIMmvJTlkJgoa5lma65IcDjwN2A9YC1zu8zQlSZtxJnBhkmuBdcBmpwmqqguS7Al8ozuluQNwDr2Z06dVqmbklo6xS3IAsHr16tUccMAB4y1GmiOWvvn8cZewVSuWnTDuEqbTyO6Na4EXnEiSmmDgSZKaYOBJkpowssBLcnqS1d1d+gf3ta9JckOSld3ynL6+o5JclWRVkkuS7D2qeiVJc8soR3gXAcfQe/D0oBdW1aHd8lV4YGb1TwGvqarFwDeB00ZWrSRpThnmxvOHpaqWA3SXnW6LpcC9m/YDzgXWAK8c3DDJAnr3fPTbd7sKlSTNSSMLvK34dHpJuBw4pao2AAvpGw1W1R1Jdkiye1WtH9j/ZOCdoytXksbj5vcsmZF7yRaees2cvwViEi5aObqqDgGOoHfPyVnbcYwzgEUDy9HTVqEkCYAkz0/yvW52hINm+L0+keS103W8sY/wqmpt93pfknPoPWYG4GZ6k8wC0N2Jv3Ezozu6EeGG/rYhTp1KkrbdnwGnVtWF4y5kWGMd4SXZJclu3d8BXgKs7LpXADsneUa3fiIw6/6BJWmuSPIBemfP/kc3192RfXPerUjye912ByS5o2+evBuSLE3ykSRXJ/nOpjnvkixJcmk3F971SU6e4r13TLKsmzfvqiQXJNl1mPpHeVvCmUnW0buY5B+TXAc8Fvh6kquBa4HFwEkA3TM6XwZ8KMkPgN8E3jqqeiVJD1ZVf05vXtTXAy+gdzHhH3dz3v0+cF53ESHAHsDybp68j9F7lubZVfXr9AY0m05VrgGe2c2F9zTgTzdNIjvgLcBdVfW07mewW4C3DVP/KK/SfD29f6RBh21hn8uAJTNWlKQ55+b3TP5/Mhaees24S5gOT6d3vcRX+n5CKuCJwB3APVX1pa79CmBdVfWfwXtW9/cj6Q1sDgE2AvsAhwDfG3i/5wGPTvLCbv3XGPIB02P/DU+SNCsFuLqqjnlIR+/h/YPz5N07sL4pf94H/BT4k6r6VZJLmHrevJOq6mvbW/AkXKUpSZp9LgMOTHLspoYkR2T4KwYXAGu7sDuYqa+wvxh4Y5Kdu/d61BSnPqfkCE+SZpFJuV+uqu5M8jxgWZIzgB2BHwHHDXmovwIuSPJfgFX0nqq1OacB7wK+m2QjvdOn7+ahpz6n5Hx4krbZbJgP7/OPWjbuErZqiN/wJiLc5gpPaUqSmmDgSZKaYOBJkppg4EmSmmDgSZKaYOBJkppg4EmSmmDgSZKaYOBJkppg4EmSmmDgSZKaYOBJkppg4EmSmuD0QMyOJ8CvWHbCuEuQpFnNEZ4kqQkGniSpCQaeJKkJBp4kqQkGniSpCQaeJKkJBp4kqQkGniSpCQaeJKkJBp4kqQkGniSpCT5LUxNr0p9x6vNNpdnFEZ4kqQkGniSpCSMJvCSnJ1mdpJIc3Ne+OMm3kqzqXg/clj5JkoY1qhHeRcAxwE0D7ecCZ1fVYuBs4Lxt7JMkaSgjuWilqpYDJHmgLcnewOHAs7qmzwBnJdkLyFR9VXX74PGTLAAWDDTvO52fQZI0u43zKs39gB9X1f0AVXV/klu69myh7yGBB5wMvHM0ZUuSZqO5clvCGcAnBtr2BS4dfSmSpEk0zsBbCzw+ybxuBDcP2Kdrzxb6HqKqNgAb+tv6T59KkjS22xKq6jZgJXB813Q8cGVV3b6lvtFXKkmaC0Z1W8KZSdbRO834j0mu67pOBF6XZBXwum6dbeiTJGkoo7pK8/XA6zfTfgNw5BT7TNnXopvfs2TcJWzRwlOvGXcJkrRFPmlFktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1IRxznguzWqTPmUTOG2T1M8RniSpCQaeJKkJBp4kqQkGniSpCQaeJKkJBp4kqQkGniSpCQaeJKkJBp4kqQkGniSpCQaeJKkJBp4kqQkGniSpCQaeJKkJBp4kqQkGniSpCRMReEnWJLkhycpueU7XflSSq5KsSnJJkr3HXaskaXaaiMDrvLCqDu2WrybZAfgU8JqqWgx8EzhtvCVKkmarSQq8QUuBe6tqebd+LvCiMdYjSZrF5o+7gD6fThJgOXAKsBC4aVNnVd2RZIcku1fV+v4dkywAFgwcb9+ZLliSNHtMygjv6Ko6BDgCCHDWkPufDKweWC6d1golSbPaRAReVa3tXu8DzgF+A7gZ2H/TNkn2BDYOju46ZwCLBpajZ7hsSdIsMvZTmkl2AeZX1V3dKc2XACuBFcDOSZ7R/Y53InDh5o5RVRuADQPHndnCJUmzytgDD3gs8LdJ5gHzgOuBk6pqY5KXAecl2QlYA7x0fGVKkmazsQdeVf0IOGyKvsuAJaOtSJI0F03Eb3iSJM00A0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1AQDT5LUBANPktQEA0+S1ISJD7wki5N8K8mq7vXAcdckSZp9Jj7wgHOBs6tqMXA2cN6Y65EkzULzx13AliTZGzgceFbX9BngrCR7VdXtfdstABYM7L4/wLp167b6PvfddftWtxm3tb/613GXsEUb16yZ9mNO+vcy6d8JTP/3MunfCcyt72XRokUHAOuq6lczV007UlXjrmFKSZYC51fVU/rargdeWlVX9LW9C3jn6CuUpBm3qKrWjLuIuWCiR3hDOAP4xEDbjsATgB8A94+6oGm2L3ApcDSw9SGrRsHvZDLNxe9lrnyOsZv0wFsLPD7JvKq6P8k8YJ+u/QFVtQHYsJn9V42gxhmXZNOf6/w/vcngdzKZ/F60JRN90UpV3QasBI7vmo4Hruz//U6SpG0x6SM8gBOBTyY5FbgTOGHM9UiSZqGJD7yqugE4ctx1SJJmt4k+pakHbADezeZ/p9R4+J1MJr8XTWmib0uQJGm6OMKTJDXBwJMkNcHAkyQ1wcCbYElOT7I6SSU5eNz1qCfJHkm+nOT7Sa5J8rkke427LkGSi5JcleTKJJcmOXTcNWlyGHiT7SLgGOCmcReiByng/VV1UFUtAW4EThtzTep5eVUdUlWHAacDHx93QZocBt4Eq6rlVbV261tqlKpqfVV9va/p23Szc2i8ququvtXdgI3jqkWTZ+JvPJcmWZIdgFcDF4+7FvUk+SjwbCDAc8dcjiaIIzzp4fkgcA9w1rgLUU9VvaqqFgKnAMvGXY8mh4EnbackpwMHAi+uKk+dTZiqugA4Nske465Fk8HAk7ZDkvcBS4HnV9V9465HkGTXJPv1rR8HrO8WyUeLTbIkZwJ/CDwOuAP4l/7Z3zUeSZ4CXEtvvsVfdM2rq+oF46tKSR4LfAHYhd6kz+uBN1XVFWMtTBPDwJMkNcFTmpKkJhh4kqQmGHiSpCYYeJKkJhh4kqQmGHjSNkjyW0nWjbsOSdvPwFOTkqxJ8osk9yS5Ncknkuw67rokzRwDTy07rqp2BQ4Hngq8Y8z1SJpBBp6aV1U/Br4CHJxk9yR/k+SWJHcmuWhz+yR5a5Ibk9yd5PokL+jre2KSbyS5K8kdST7btSfJB5LcluRn3eSxTuwrjYjTA6l53fMXfxf4HHABvdkPntK9Pn2K3W4EjgZ+CvwR8KkkT6yqnwB/CVwCHAvsSG/0CL0pa44BFgN3AU8CNszAR5K0GQaeWnZRkl/RC58vAecAPwb2qKo7u22+sbkdq+rCvtXPJnkb8DR6z3L8Jb0JYfepqnXA8m67XwKPohd0l1fV96b580jaAk9pqmXPr6oFVbV/VZ0E7Aes7wu7KSU5IcnKJBuSbAAOBvbsut9Cb/LRy5Ncl+SVAFX1NXrz5p0N3Jbkw0kePRMfTNJDGXjSv1kL7J5kwZY2SrI/8BHgtfRGgwvozZ4QgKr6aVX916raB/gz4JwkT+z6zqyqpcCT6Z3afPOMfRpJD2LgSZ3u97ev0AuoxyR5RJJjNrPpLkABtwMkeQW9ER7d+h8l2bdbvbPbdmOSI5IcmeQRwM+BewEnjpVGxMCTHuxl9H5ruwG4DTh5cIOquh74a+BbwK3AEuCf+jY5AvhOknuAi4E3VNWPgEfTGxneCdwE/AuwbMY+iaQHcT48SVITHOFJkppg4EmSmmDgSZKaYOBJkppg4EmSmmDgSZKaYOBJkppg4EmSmvD/AfmuvPpWku1yAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 437.975x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot('Pclass', data=train_df, hue='Sex', kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7faad043f128>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 439.85x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot('Pclass', data=train_df, hue='Who', kind='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T10:21:46.515738Z",
     "start_time": "2018-09-27T10:21:45.429391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Pclass                                               Name  \\\n",
      "0            1       3                            Braund, Mr. Owen Harris   \n",
      "1            2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
      "2            3       3                             Heikkinen, Miss. Laina   \n",
      "3            4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
      "4            5       3                           Allen, Mr. William Henry   \n",
      "\n",
      "      Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked Title  \n",
      "0    male  22.0      1      0         A/5 21171   7.2500   NaN        S    Mr  \n",
      "1  female  38.0      1      0          PC 17599  71.2833   C85        C   Mrs  \n",
      "2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  Miss  \n",
      "3  female  35.0      1      0            113803  53.1000  C123        S   Mrs  \n",
      "4    male  35.0      0      0            373450   8.0500   NaN        S    Mr  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['IsAlone', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male',\n",
      "       'Ticket_A4', 'Ticket_A5', 'Ticket_AS', 'Ticket_C', 'Ticket_CA',\n",
      "       'Ticket_CASOTON', 'Ticket_FC', 'Ticket_FCC', 'Ticket_Fa', 'Ticket_LINE',\n",
      "       'Ticket_PC', 'Ticket_PP', 'Ticket_PPP', 'Ticket_SC', 'Ticket_SCA4',\n",
      "       'Ticket_SCAH', 'Ticket_SCOW', 'Ticket_SCPARIS', 'Ticket_SCParis',\n",
      "       'Ticket_SOC', 'Ticket_SOP', 'Ticket_SOPP', 'Ticket_SOTONO2',\n",
      "       'Ticket_SOTONOQ', 'Ticket_SP', 'Ticket_STONO', 'Ticket_STONO2',\n",
      "       'Ticket_SWPP', 'Ticket_WC', 'Ticket_WEP', 'Ticket_XXX', 'Cabin_A',\n",
      "       'Cabin_B', 'Cabin_C', 'Cabin_D', 'Cabin_E', 'Cabin_F', 'Cabin_G',\n",
      "       'Cabin_T', 'Cabin_U', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n",
      "       'Title_Boy', 'Title_Girl', 'Title_Master', 'Title_Miss', 'Title_Mr',\n",
      "       'Title_Mrs', 'Title_Officer', 'Title_Royalty', 'AgeBand_0.0',\n",
      "       'AgeBand_1.0', 'AgeBand_2.0', 'AgeBand_nan', 'FareBand_0.0',\n",
      "       'FareBand_1.0', 'FareBand_2.0', 'FareBand_3.0'],\n",
      "      dtype='object')\n",
      "   IsAlone  Pclass_1  Pclass_2  Pclass_3  Sex_female  Sex_male  Ticket_A4  \\\n",
      "0        0         0         0         1           0         1          0   \n",
      "1        0         1         0         0           1         0          0   \n",
      "2        1         0         0         1           1         0          0   \n",
      "3        0         1         0         0           1         0          0   \n",
      "4        1         0         0         1           0         1          0   \n",
      "\n",
      "   Ticket_A5  Ticket_AS  Ticket_C      ...       Title_Officer  Title_Royalty  \\\n",
      "0          1          0         0      ...                   0              0   \n",
      "1          0          0         0      ...                   0              0   \n",
      "2          0          0         0      ...                   0              0   \n",
      "3          0          0         0      ...                   0              0   \n",
      "4          0          0         0      ...                   0              0   \n",
      "\n",
      "   AgeBand_0.0  AgeBand_1.0  AgeBand_2.0  AgeBand_nan  FareBand_0.0  \\\n",
      "0            0            1            0            0             1   \n",
      "1            0            1            0            0             0   \n",
      "2            0            1            0            0             1   \n",
      "3            0            1            0            0             0   \n",
      "4            0            1            0            0             0   \n",
      "\n",
      "   FareBand_1.0  FareBand_2.0  FareBand_3.0  \n",
      "0             0             0             0  \n",
      "1             0             0             1  \n",
      "2             0             0             0  \n",
      "3             0             0             1  \n",
      "4             1             0             0  \n",
      "\n",
      "[5 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_df.head())\n",
    "combined = pd.concat([train_df, test_df])\n",
    "grouped_median_age = combined[train_df.Age.notnull()].groupby(['Sex', 'Pclass', 'Title']).median()\n",
    "grouped_median_age = grouped_median_age.reset_index()[['Sex', 'Pclass', 'Title', 'Age']]\n",
    "grouped_median_age\n",
    "\n",
    "def assign_age(row):\n",
    "    condition = (\n",
    "        (grouped_median_age['Sex'] == row['Sex']) & \n",
    "        (grouped_median_age['Title'] == row['Title']) & \n",
    "        (grouped_median_age['Pclass'] == row['Pclass'])\n",
    "    )\n",
    "    return grouped_median_age[condition]['Age'].values[0]\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset['Age'] = dataset.apply(lambda row: assign_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "    dataset.loc[dataset.Embarked.isnull(), 'Embarked'] = 'S'\n",
    "    dataset.loc[dataset.Cabin.notnull(), 'Cabin'] = dataset.loc[dataset.Cabin.notnull(), 'Cabin'].map(lambda x: x[0])\n",
    "    dataset.loc[dataset.Cabin.isnull(), 'Cabin'] = 'U' #unknown\n",
    "    \n",
    "def cleanTicket(ticket):\n",
    "    ticket = ticket.replace('.', '')\n",
    "    ticket = ticket.replace('/', '')\n",
    "    ticket = ticket.split()\n",
    "    ticket = map(lambda t : t.strip(), ticket)\n",
    "    ticket = list(filter(lambda t : not t.isdigit(), ticket))\n",
    "    if len(ticket) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "for dataset in combine:    \n",
    "    dataset.loc[ dataset['Age'] <= 15, 'AgeBand'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 15) & (dataset['Age'] <= 40), 'AgeBand'] = 1\n",
    "    dataset.loc[ dataset['Age'] > 40, 'AgeBand'] = 2\n",
    "    dataset['AgeBand'] = dataset['AgeBand'].astype(str)\n",
    "    dataset.loc[dataset['Fare'].isnull(), 'Fare'] = 8\n",
    "    dataset.loc[dataset['Fare'] <= 8, 'FareBand'] = 0\n",
    "    dataset.loc[(dataset['Fare'] > 8) & (dataset['Fare'] <= 14.454), 'FareBand'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'FareBand']   = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'FareBand'] = 3\n",
    "    dataset['FareBand'] = dataset['FareBand'].astype(str)\n",
    "    dataset['Pclass'] = dataset['Pclass'].astype(str)\n",
    "    dataset['Ticket'] = dataset['Ticket'].map(cleanTicket)\n",
    "#     for column in ['Ticket']:\n",
    "#         dataset[column] = dataset[column].astype('category')\n",
    "#         dataset[column] = dataset[column].cat.codes\n",
    "for dataset in combine:\n",
    "#     dataset.drop(['PassengerId', 'Name', 'Age', 'SibSp', 'Parch', 'FamilySize'], axis=1, inplace=True)\n",
    "    dataset.drop(['PassengerId', 'Name', 'Age', 'SibSp', 'Parch', 'Fare', 'FamilySize'], axis=1, inplace=True)\n",
    "train_df = pd.get_dummies(train_df)\n",
    "print(train_df.columns)\n",
    "test_df = pd.get_dummies(test_df)\n",
    "# Get missing columns in the training test\n",
    "missing_cols = set( train_df.columns ) - set( test_df.columns )\n",
    "# Add a missing column in test set with default value equal to 0\n",
    "for c in missing_cols:\n",
    "    test_df[c] = 0\n",
    "# Ensure the order of column in the test set is in the same order than in train set\n",
    "test_df = test_df[train_df.columns]\n",
    "train_df.to_csv('train_temp.csv')\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T10:30:05.693021Z",
     "start_time": "2018-09-27T10:30:02.355617Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828313734335253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "gbm = xgboost.XGBClassifier(max_depth=7, n_estimators=300, learning_rate=0.05)\n",
    "# gbm.fit(train_df, y)\n",
    "print(np.mean(cross_val_score(gbm, train_df, y, cv=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T10:32:27.457582Z",
     "start_time": "2018-09-27T10:32:14.842137Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** before reduced = 0.8305672733436958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/sklearn/utils/__init__.py:93: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced shape (891, 26) (418, 26)\n",
      "*** retrain after reducing***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8372899442404691\n",
      "Cross-validation of : <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "CV score = 0.8092377190192025\n",
      "**********\n",
      "Cross-validation of : <class 'sklearn.linear_model.logistic.LogisticRegressionCV'>\n",
      "CV score = 0.8103675915946844\n",
      "**********\n",
      "Cross-validation of : <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "CV score = 0.8204926471396599\n",
      "**********\n",
      "Cross-validation of : <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>\n",
      "CV score = 0.8238507376621002\n",
      "**********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1169b0dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## array([0.81564246, 0.80446927, 0.80898876, 0.79213483, 0.8079096 ])\n",
    "# clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, min_samples_leaf=100, max_depth=2, random_state=random_state)\n",
    "\n",
    "## array([0.79888268, 0.81564246, 0.81460674, 0.80337079, 0.85875706])\n",
    "# clf = RandomForestClassifier(max_depth=4, max_features=3, n_estimators=400)\n",
    "clf = xgboost.XGBClassifier(max_depth=6, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "## array([0.75977654, 0.75977654, 0.7752809 , 0.76966292, 0.82485876])\n",
    "# clf = VotingClassifier(\n",
    "#     estimators=[\n",
    "#         ('lr', LogisticRegression()),\n",
    "#         ('svc', SVC()),\n",
    "#         ('tree', DecisionTreeClassifier(max_depth=2, max_features=3)),\n",
    "#         ('sgd', SGDClassifier()),\n",
    "#         ('linear_svc', LinearSVC()),\n",
    "#         ('gaussian', GaussianNB()),\n",
    "#         ('knn', KNeighborsClassifier(n_neighbors = 2))\n",
    "#     ],\n",
    "#     voting='hard',\n",
    "# )\n",
    "\n",
    "## array([0.82122905, 0.80446927, 0.79775281, 0.75842697, 0.8079096 ])\n",
    "# clf = AdaBoostClassifier(\n",
    "#     DecisionTreeClassifier(max_depth=1), n_estimators=300,\n",
    "#     algorithm='SAMME', learning_rate=0.2\n",
    "# )\n",
    "\n",
    "# clf = DecisionTreeClassifier()\n",
    "# cv = cross_val_score(clf, train_df, y, cv=5)\n",
    "# print(cv)\n",
    "clf.fit(train_df, y)\n",
    "# print(\"*\"*40, \"running grid search\")\n",
    "# param_test = {'n_estimators': np.linspace(100,500,5, dtype=np.int),\n",
    "#               'max_depth': [1,2,3,4,6,8],\n",
    "#               'max_features': ['sqrt', 'auto', 'log2'],\n",
    "#               'min_samples_split': [2, 3, 10],\n",
    "#               'min_samples_leaf': [1, 3, 10],\n",
    "#               'bootstrap': [True, False],\n",
    "#              }\n",
    "# gs = GridSearchCV(estimator=clf, param_grid=param_test, cv=5, n_jobs=5, verbose=1)\n",
    "# gs.fit(train_df, y)\n",
    "# print(\"*\"*40, \"best params\", gs.best_params_, gs.best_score_)\n",
    "# clf.fit(train_df, y)\n",
    "cv = np.mean(cross_val_score(clf, train_df, y, cv=5))\n",
    "print(f\"** before reduced = {cv}\")\n",
    "features = pd.DataFrame()\n",
    "features['feature'] = train_df.columns\n",
    "features['importance'] = clf.feature_importances_\n",
    "features.sort_values(by=['importance'], ascending=True, inplace=True)\n",
    "features.set_index('feature', inplace=True)\n",
    "features.plot(kind='barh', figsize=(25, 25))\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "train_reduced = model.transform(train_df)\n",
    "test_reduced = model.transform(test_df)\n",
    "print(\"reduced shape\", train_reduced.shape, test_reduced.shape)\n",
    "print(\"*** retrain after reducing***\")\n",
    "print(np.mean(cross_val_score(clf, train_reduced, y, cv=5)))\n",
    "clf.fit(train_reduced, y)\n",
    "logreg = LogisticRegression()\n",
    "logreg_cv = LogisticRegressionCV()\n",
    "rf = RandomForestClassifier()\n",
    "gboost = GradientBoostingClassifier()\n",
    "\n",
    "models = [logreg, logreg_cv, rf, gboost]\n",
    "for m in models:\n",
    "    print('Cross-validation of : {0}'.format(m.__class__))\n",
    "    score = np.mean(cross_val_score(m, X=train_reduced, y=y, scoring='accuracy', cv=5))\n",
    "    print('CV score = {0}'.format(score))\n",
    "    print('*'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*\"*40, \"running grid search for xgboost\")\n",
    "param_test = {'n_estimators': np.linspace(100,500,5, dtype=np.int),\n",
    "              'max_depth': [1,2,3,4,6,8],\n",
    "              'max_features': ['sqrt', 'auto', 'log2', 2, 3, 4],\n",
    "              'min_samples_split': [2, 3, 10],\n",
    "              'min_samples_leaf': [1, 3, 10],\n",
    "              'bootstrap': [True, False],\n",
    "             }\n",
    "gs = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_test, cv=5, n_jobs=5, verbose=1)\n",
    "gs.fit(train_reduced, y)\n",
    "print(\"*\"*40, \"best params\", gs.best_params_, gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:29:14.890139Z",
     "start_time": "2018-09-24T08:02:31.531399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************** running grid search\n",
      "Fitting 5 folds for each of 3240 candidates, totalling 16200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=5)]: Done 440 tasks      | elapsed:   42.4s\n",
      "[Parallel(n_jobs=5)]: Done 790 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=5)]: Done 1240 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=5)]: Done 1790 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=5)]: Done 2440 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=5)]: Done 3190 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=5)]: Done 4040 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=5)]: Done 4990 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=5)]: Done 6040 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=5)]: Done 7190 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=5)]: Done 8440 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=5)]: Done 9790 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=5)]: Done 11240 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=5)]: Done 12790 tasks      | elapsed: 21.4min\n",
      "[Parallel(n_jobs=5)]: Done 14440 tasks      | elapsed: 23.9min\n",
      "[Parallel(n_jobs=5)]: Done 16190 tasks      | elapsed: 26.7min\n",
      "[Parallel(n_jobs=5)]: Done 16200 out of 16200 | elapsed: 26.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************** best params {'bootstrap': True, 'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200} 0.8148148148148148\n"
     ]
    }
   ],
   "source": [
    "print(\"*\"*40, \"running grid search\")\n",
    "param_test = {'n_estimators': np.linspace(100,500,5, dtype=np.int),\n",
    "              'max_depth': [1,2,3,4,6,8],\n",
    "              'max_features': ['sqrt', 'auto', 'log2', 2, 3, 4],\n",
    "              'min_samples_split': [2, 3, 10],\n",
    "              'min_samples_leaf': [1, 3, 10],\n",
    "              'bootstrap': [True, False],\n",
    "             }\n",
    "gs = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_test, cv=5, n_jobs=5, verbose=1)\n",
    "gs.fit(train_reduced, y)\n",
    "print(\"*\"*40, \"best params\", gs.best_params_, gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T07:14:45.501102Z",
     "start_time": "2018-09-24T06:39:03.922274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3600 candidates, totalling 18000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done 175 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=5)]: Done 625 tasks      | elapsed:   25.0s\n",
      "[Parallel(n_jobs=5)]: Done 1017 tasks      | elapsed:   46.6s\n",
      "[Parallel(n_jobs=5)]: Done 1367 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=5)]: Done 1817 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=5)]: Done 2367 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=5)]: Done 3017 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=5)]: Done 3767 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=5)]: Done 4617 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=5)]: Done 5567 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=5)]: Done 6617 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=5)]: Done 7767 tasks      | elapsed: 15.1min\n",
      "[Parallel(n_jobs=5)]: Done 9017 tasks      | elapsed: 16.7min\n",
      "[Parallel(n_jobs=5)]: Done 10367 tasks      | elapsed: 19.9min\n",
      "[Parallel(n_jobs=5)]: Done 11817 tasks      | elapsed: 22.3min\n",
      "[Parallel(n_jobs=5)]: Done 13367 tasks      | elapsed: 24.8min\n",
      "[Parallel(n_jobs=5)]: Done 15017 tasks      | elapsed: 28.6min\n",
      "[Parallel(n_jobs=5)]: Done 16767 tasks      | elapsed: 31.4min\n",
      "[Parallel(n_jobs=5)]: Done 18000 out of 18000 | elapsed: 35.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=5,\n",
       "       param_grid={'learning_rate': array([0.1, 0.2, 0.3, 0.4, 0.5]), 'n_estimators': array([ 100,  212,  325,  437,  550,  662,  775,  887, 1000]), 'max_depth': array([1, 2, 3, 4, 5]), 'min_samples_split': [2, 5, 10, 20], 'max_features': [2, 4, 6, 8]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test = {\n",
    "    'learning_rate': np.linspace(0.1, 0.5, 5),\n",
    "    'n_estimators': np.linspace(100, 1000, 9, dtype=np.int),\n",
    "    'max_depth': np.linspace(1, 5, 5, dtype=np.int),\n",
    "    'min_samples_split': [2,5,10,20],\n",
    "    'max_features': [2,4,6,8]\n",
    "}\n",
    "gs = GridSearchCV(estimator=GradientBoostingClassifier(), param_grid=param_test, cv=5, n_jobs=5, verbose=1)\n",
    "gs.fit(train_reduced, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T03:58:02.373900Z",
     "start_time": "2018-09-24T03:57:27.405Z"
    }
   },
   "outputs": [],
   "source": [
    "gs.best_params_, gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T09:13:49.527300Z",
     "start_time": "2018-09-24T09:13:48.167802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** best score for random forest = 0.8025338793320204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=6, max_features=3, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=3, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_forest = RandomForestClassifier(**{'bootstrap': False, 'max_depth': 6, 'max_features': 3, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 100})\n",
    "score = np.mean(cross_val_score(best_forest, train_reduced, y, cv=5))\n",
    "print(f\"** best score for random forest = {score}\")\n",
    "best_forest.fit(train_reduced, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T09:35:36.158814Z",
     "start_time": "2018-09-24T09:35:33.144719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** score = 0.7844803171303365\n"
     ]
    }
   ],
   "source": [
    "ada_boosting = AdaBoostClassifier(n_estimators=500, learning_rate=0.1, algorithm='SAMME', random_state=random_state)\n",
    "score = np.mean(cross_val_score(ada_boosting, train_reduced, y, cv=5))\n",
    "print(f\"** score = {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T06:38:41.024658Z",
     "start_time": "2018-09-24T06:38:39.833955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** score = 0.81939423084084\n"
     ]
    }
   ],
   "source": [
    "gradient_boosting = GradientBoostingClassifier(\n",
    "    learning_rate=0.5,\n",
    "    n_estimators=437, \n",
    "    max_depth= 2,\n",
    "    min_samples_leaf= 2,\n",
    "    min_samples_split=2)\n",
    "score = np.mean(cross_val_score(gradient_boosting, train_reduced, y, cv=5))\n",
    "gradient_boosting.fit(train_reduced, y)\n",
    "print(f\"** score = {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T10:33:17.617475Z",
     "start_time": "2018-09-27T10:33:17.593172Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdang/.virtualenvs/tensorflow/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(test_reduced)\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": test_ids,\n",
    "    \"Survived\": y_pred\n",
    "})\n",
    "submission.to_csv('titanic_result_feature_engineer_reduced_xgboost.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-27T10:13:37.911803Z",
     "start_time": "2018-09-27T10:12:26.145614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by otherusers on this system! To fix this, you can run'chmod 600 /Users/kdang/.kaggle/kaggle.json'\n",
      "^C\n",
      "User cancelled operation\n"
     ]
    }
   ],
   "source": [
    "! kaggle competitions submit titanic -f titanic_result_feature_engineer_reduced_xgboost.csv -m \"feature engineer + grid search for random forest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T04:29:42.388511Z",
     "start_time": "2018-09-24T04:29:42.344004Z"
    }
   },
   "outputs": [],
   "source": [
    "# try using stacking\n",
    "from mlxtend.classifier import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T07:48:03.184053Z",
     "start_time": "2018-09-24T07:47:57.010174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8171470398296041"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = KNeighborsClassifier(n_neighbors=2)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[best_forest, ada_boosting, gradient_boosting, clf1, clf3], \n",
    "                          meta_classifier=lr)\n",
    "np.mean(cross_val_score(sclf, train_reduced, y, cv=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T07:57:28.036289Z",
     "start_time": "2018-09-24T07:57:28.032290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reduced.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:02:18.986796Z",
     "start_time": "2018-09-24T08:00:53.707652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 712 samples, validate on 179 samples\n",
      "Epoch 1/1000\n",
      "712/712 [==============================] - 1s 971us/step - loss: 0.6998 - acc: 0.5084 - val_loss: 0.6752 - val_acc: 0.7095\n",
      "Epoch 2/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6893 - acc: 0.5478 - val_loss: 0.6751 - val_acc: 0.7095\n",
      "Epoch 3/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6964 - acc: 0.5028 - val_loss: 0.6751 - val_acc: 0.7095\n",
      "Epoch 4/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6903 - acc: 0.5295 - val_loss: 0.6751 - val_acc: 0.7095\n",
      "Epoch 5/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6889 - acc: 0.5520 - val_loss: 0.6751 - val_acc: 0.7095\n",
      "Epoch 6/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6967 - acc: 0.5098 - val_loss: 0.6751 - val_acc: 0.7095\n",
      "Epoch 7/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6919 - acc: 0.5281 - val_loss: 0.6751 - val_acc: 0.7095\n",
      "Epoch 8/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6953 - acc: 0.5154 - val_loss: 0.6750 - val_acc: 0.7095\n",
      "Epoch 9/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6930 - acc: 0.5239 - val_loss: 0.6750 - val_acc: 0.7095\n",
      "Epoch 10/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6935 - acc: 0.5295 - val_loss: 0.6750 - val_acc: 0.7095\n",
      "Epoch 11/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6908 - acc: 0.5028 - val_loss: 0.6750 - val_acc: 0.7095\n",
      "Epoch 12/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6877 - acc: 0.5393 - val_loss: 0.6750 - val_acc: 0.7095\n",
      "Epoch 13/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6969 - acc: 0.5098 - val_loss: 0.6749 - val_acc: 0.7095\n",
      "Epoch 14/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6940 - acc: 0.5309 - val_loss: 0.6749 - val_acc: 0.7095\n",
      "Epoch 15/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6845 - acc: 0.5646 - val_loss: 0.6749 - val_acc: 0.7095\n",
      "Epoch 16/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6863 - acc: 0.5646 - val_loss: 0.6749 - val_acc: 0.7095\n",
      "Epoch 17/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6926 - acc: 0.5365 - val_loss: 0.6749 - val_acc: 0.7095\n",
      "Epoch 18/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6955 - acc: 0.5379 - val_loss: 0.6749 - val_acc: 0.7095\n",
      "Epoch 19/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6871 - acc: 0.5492 - val_loss: 0.6748 - val_acc: 0.7095\n",
      "Epoch 20/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6883 - acc: 0.5506 - val_loss: 0.6748 - val_acc: 0.6983\n",
      "Epoch 21/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6934 - acc: 0.5070 - val_loss: 0.6748 - val_acc: 0.6983\n",
      "Epoch 22/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6989 - acc: 0.5056 - val_loss: 0.6748 - val_acc: 0.6983\n",
      "Epoch 23/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6856 - acc: 0.5379 - val_loss: 0.6748 - val_acc: 0.6983\n",
      "Epoch 24/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6954 - acc: 0.5028 - val_loss: 0.6747 - val_acc: 0.6983\n",
      "Epoch 25/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6930 - acc: 0.5084 - val_loss: 0.6747 - val_acc: 0.6983\n",
      "Epoch 26/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6911 - acc: 0.5323 - val_loss: 0.6747 - val_acc: 0.6983\n",
      "Epoch 27/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6905 - acc: 0.5309 - val_loss: 0.6747 - val_acc: 0.6983\n",
      "Epoch 28/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6875 - acc: 0.5407 - val_loss: 0.6747 - val_acc: 0.6983\n",
      "Epoch 29/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6977 - acc: 0.5084 - val_loss: 0.6747 - val_acc: 0.6983\n",
      "Epoch 30/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6911 - acc: 0.5337 - val_loss: 0.6746 - val_acc: 0.6983\n",
      "Epoch 31/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6954 - acc: 0.5267 - val_loss: 0.6746 - val_acc: 0.6983\n",
      "Epoch 32/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6900 - acc: 0.5267 - val_loss: 0.6746 - val_acc: 0.6983\n",
      "Epoch 33/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6888 - acc: 0.5548 - val_loss: 0.6746 - val_acc: 0.6983\n",
      "Epoch 34/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6942 - acc: 0.5154 - val_loss: 0.6746 - val_acc: 0.6983\n",
      "Epoch 35/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6962 - acc: 0.5183 - val_loss: 0.6746 - val_acc: 0.6983\n",
      "Epoch 36/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6873 - acc: 0.5730 - val_loss: 0.6745 - val_acc: 0.6927\n",
      "Epoch 37/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6954 - acc: 0.5098 - val_loss: 0.6745 - val_acc: 0.6927\n",
      "Epoch 38/1000\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.6927 - acc: 0.5309 - val_loss: 0.6745 - val_acc: 0.6927\n",
      "Epoch 39/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6903 - acc: 0.5140 - val_loss: 0.6745 - val_acc: 0.6927\n",
      "Epoch 40/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6928 - acc: 0.5140 - val_loss: 0.6745 - val_acc: 0.6927\n",
      "Epoch 41/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6949 - acc: 0.5239 - val_loss: 0.6744 - val_acc: 0.6927\n",
      "Epoch 42/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6844 - acc: 0.5463 - val_loss: 0.6744 - val_acc: 0.6927\n",
      "Epoch 43/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6926 - acc: 0.5169 - val_loss: 0.6744 - val_acc: 0.6927\n",
      "Epoch 44/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6939 - acc: 0.5211 - val_loss: 0.6744 - val_acc: 0.6927\n",
      "Epoch 45/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6953 - acc: 0.5112 - val_loss: 0.6744 - val_acc: 0.6927\n",
      "Epoch 46/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6987 - acc: 0.5056 - val_loss: 0.6744 - val_acc: 0.6927\n",
      "Epoch 47/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6913 - acc: 0.5197 - val_loss: 0.6743 - val_acc: 0.6927\n",
      "Epoch 48/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6834 - acc: 0.5590 - val_loss: 0.6743 - val_acc: 0.6927\n",
      "Epoch 49/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6857 - acc: 0.5548 - val_loss: 0.6743 - val_acc: 0.6927\n",
      "Epoch 50/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6908 - acc: 0.5253 - val_loss: 0.6743 - val_acc: 0.6927\n",
      "Epoch 51/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6932 - acc: 0.5337 - val_loss: 0.6743 - val_acc: 0.6927\n",
      "Epoch 52/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6862 - acc: 0.5421 - val_loss: 0.6743 - val_acc: 0.6927\n",
      "Epoch 53/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6937 - acc: 0.5295 - val_loss: 0.6742 - val_acc: 0.6927\n",
      "Epoch 54/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6873 - acc: 0.5393 - val_loss: 0.6742 - val_acc: 0.6927\n",
      "Epoch 55/1000\n",
      "712/712 [==============================] - 0s 108us/step - loss: 0.6879 - acc: 0.5295 - val_loss: 0.6742 - val_acc: 0.6927\n",
      "Epoch 56/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6854 - acc: 0.5365 - val_loss: 0.6742 - val_acc: 0.6927\n",
      "Epoch 57/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6922 - acc: 0.5253 - val_loss: 0.6742 - val_acc: 0.6927\n",
      "Epoch 58/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6921 - acc: 0.5225 - val_loss: 0.6741 - val_acc: 0.6927\n",
      "Epoch 59/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6891 - acc: 0.5295 - val_loss: 0.6741 - val_acc: 0.6927\n",
      "Epoch 60/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6932 - acc: 0.5239 - val_loss: 0.6741 - val_acc: 0.6927\n",
      "Epoch 61/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6879 - acc: 0.5295 - val_loss: 0.6741 - val_acc: 0.6927\n",
      "Epoch 62/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6863 - acc: 0.5421 - val_loss: 0.6741 - val_acc: 0.6927\n",
      "Epoch 63/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6902 - acc: 0.5379 - val_loss: 0.6741 - val_acc: 0.6927\n",
      "Epoch 64/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6948 - acc: 0.4874 - val_loss: 0.6740 - val_acc: 0.6927\n",
      "Epoch 65/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6926 - acc: 0.5112 - val_loss: 0.6740 - val_acc: 0.6927\n",
      "Epoch 66/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6902 - acc: 0.5492 - val_loss: 0.6740 - val_acc: 0.6927\n",
      "Epoch 67/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6972 - acc: 0.5154 - val_loss: 0.6740 - val_acc: 0.6927\n",
      "Epoch 68/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6827 - acc: 0.5632 - val_loss: 0.6740 - val_acc: 0.6927\n",
      "Epoch 69/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6929 - acc: 0.5253 - val_loss: 0.6739 - val_acc: 0.6927\n",
      "Epoch 70/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6933 - acc: 0.5211 - val_loss: 0.6739 - val_acc: 0.6927\n",
      "Epoch 71/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6845 - acc: 0.5365 - val_loss: 0.6739 - val_acc: 0.6927\n",
      "Epoch 72/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6914 - acc: 0.5337 - val_loss: 0.6739 - val_acc: 0.6927\n",
      "Epoch 73/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6940 - acc: 0.5211 - val_loss: 0.6739 - val_acc: 0.6927\n",
      "Epoch 74/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6916 - acc: 0.5197 - val_loss: 0.6738 - val_acc: 0.6927\n",
      "Epoch 75/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6922 - acc: 0.5323 - val_loss: 0.6738 - val_acc: 0.6927\n",
      "Epoch 76/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6945 - acc: 0.5393 - val_loss: 0.6738 - val_acc: 0.6927\n",
      "Epoch 77/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6912 - acc: 0.5267 - val_loss: 0.6738 - val_acc: 0.6927\n",
      "Epoch 78/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6854 - acc: 0.5295 - val_loss: 0.6738 - val_acc: 0.6927\n",
      "Epoch 79/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6896 - acc: 0.5421 - val_loss: 0.6738 - val_acc: 0.6927\n",
      "Epoch 80/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6945 - acc: 0.5295 - val_loss: 0.6737 - val_acc: 0.6927\n",
      "Epoch 81/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6926 - acc: 0.5365 - val_loss: 0.6737 - val_acc: 0.6927\n",
      "Epoch 82/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6873 - acc: 0.5365 - val_loss: 0.6737 - val_acc: 0.6927\n",
      "Epoch 83/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6932 - acc: 0.4972 - val_loss: 0.6737 - val_acc: 0.6927\n",
      "Epoch 84/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6925 - acc: 0.5393 - val_loss: 0.6737 - val_acc: 0.6927\n",
      "Epoch 85/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6975 - acc: 0.5014 - val_loss: 0.6737 - val_acc: 0.6927\n",
      "Epoch 86/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6912 - acc: 0.5393 - val_loss: 0.6736 - val_acc: 0.6927\n",
      "Epoch 87/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6904 - acc: 0.5183 - val_loss: 0.6736 - val_acc: 0.6927\n",
      "Epoch 88/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6924 - acc: 0.5393 - val_loss: 0.6736 - val_acc: 0.6927\n",
      "Epoch 89/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6917 - acc: 0.5449 - val_loss: 0.6736 - val_acc: 0.6927\n",
      "Epoch 90/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6866 - acc: 0.5421 - val_loss: 0.6736 - val_acc: 0.6927\n",
      "Epoch 91/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6920 - acc: 0.5197 - val_loss: 0.6736 - val_acc: 0.6927\n",
      "Epoch 92/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6855 - acc: 0.5646 - val_loss: 0.6735 - val_acc: 0.6927\n",
      "Epoch 93/1000\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.6876 - acc: 0.5323 - val_loss: 0.6735 - val_acc: 0.6927\n",
      "Epoch 94/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6930 - acc: 0.5295 - val_loss: 0.6735 - val_acc: 0.6927\n",
      "Epoch 95/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6850 - acc: 0.5716 - val_loss: 0.6735 - val_acc: 0.6927\n",
      "Epoch 96/1000\n",
      "712/712 [==============================] - 0s 177us/step - loss: 0.6876 - acc: 0.5337 - val_loss: 0.6735 - val_acc: 0.6927\n",
      "Epoch 97/1000\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.6836 - acc: 0.5534 - val_loss: 0.6734 - val_acc: 0.6927\n",
      "Epoch 98/1000\n",
      "712/712 [==============================] - 0s 162us/step - loss: 0.6916 - acc: 0.5281 - val_loss: 0.6734 - val_acc: 0.6927\n",
      "Epoch 99/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6875 - acc: 0.5688 - val_loss: 0.6734 - val_acc: 0.6927\n",
      "Epoch 100/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6870 - acc: 0.5337 - val_loss: 0.6734 - val_acc: 0.6927\n",
      "Epoch 101/1000\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.6850 - acc: 0.5520 - val_loss: 0.6734 - val_acc: 0.6927\n",
      "Epoch 102/1000\n",
      "712/712 [==============================] - 0s 162us/step - loss: 0.6905 - acc: 0.5239 - val_loss: 0.6734 - val_acc: 0.6927\n",
      "Epoch 103/1000\n",
      "712/712 [==============================] - 0s 183us/step - loss: 0.6873 - acc: 0.5492 - val_loss: 0.6733 - val_acc: 0.6927\n",
      "Epoch 104/1000\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.6895 - acc: 0.5337 - val_loss: 0.6733 - val_acc: 0.6927\n",
      "Epoch 105/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6931 - acc: 0.5478 - val_loss: 0.6733 - val_acc: 0.6927\n",
      "Epoch 106/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6933 - acc: 0.5197 - val_loss: 0.6733 - val_acc: 0.6927\n",
      "Epoch 107/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6932 - acc: 0.5225 - val_loss: 0.6733 - val_acc: 0.6927\n",
      "Epoch 108/1000\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.6934 - acc: 0.5407 - val_loss: 0.6732 - val_acc: 0.6927\n",
      "Epoch 109/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6927 - acc: 0.5365 - val_loss: 0.6732 - val_acc: 0.6927\n",
      "Epoch 110/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6929 - acc: 0.5042 - val_loss: 0.6732 - val_acc: 0.6927\n",
      "Epoch 111/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6884 - acc: 0.5337 - val_loss: 0.6732 - val_acc: 0.6927\n",
      "Epoch 112/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6908 - acc: 0.5520 - val_loss: 0.6732 - val_acc: 0.6927\n",
      "Epoch 113/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6932 - acc: 0.5309 - val_loss: 0.6732 - val_acc: 0.6927\n",
      "Epoch 114/1000\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.6857 - acc: 0.5449 - val_loss: 0.6731 - val_acc: 0.6927\n",
      "Epoch 115/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6894 - acc: 0.5323 - val_loss: 0.6731 - val_acc: 0.6927\n",
      "Epoch 116/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6892 - acc: 0.5281 - val_loss: 0.6731 - val_acc: 0.6927\n",
      "Epoch 117/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6890 - acc: 0.5351 - val_loss: 0.6731 - val_acc: 0.6927\n",
      "Epoch 118/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6887 - acc: 0.5169 - val_loss: 0.6731 - val_acc: 0.6927\n",
      "Epoch 119/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6982 - acc: 0.4860 - val_loss: 0.6731 - val_acc: 0.6927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6967 - acc: 0.5154 - val_loss: 0.6730 - val_acc: 0.6927\n",
      "Epoch 121/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6897 - acc: 0.5351 - val_loss: 0.6730 - val_acc: 0.6927\n",
      "Epoch 122/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6908 - acc: 0.5407 - val_loss: 0.6730 - val_acc: 0.6927\n",
      "Epoch 123/1000\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.6868 - acc: 0.5548 - val_loss: 0.6730 - val_acc: 0.6927\n",
      "Epoch 124/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6871 - acc: 0.5323 - val_loss: 0.6730 - val_acc: 0.6927\n",
      "Epoch 125/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6915 - acc: 0.5421 - val_loss: 0.6729 - val_acc: 0.6927\n",
      "Epoch 126/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6932 - acc: 0.5379 - val_loss: 0.6729 - val_acc: 0.6927\n",
      "Epoch 127/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6850 - acc: 0.5449 - val_loss: 0.6729 - val_acc: 0.6927\n",
      "Epoch 128/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6891 - acc: 0.5463 - val_loss: 0.6729 - val_acc: 0.6927\n",
      "Epoch 129/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6837 - acc: 0.5590 - val_loss: 0.6729 - val_acc: 0.6927\n",
      "Epoch 130/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6929 - acc: 0.5267 - val_loss: 0.6729 - val_acc: 0.6927\n",
      "Epoch 131/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6946 - acc: 0.5393 - val_loss: 0.6728 - val_acc: 0.6927\n",
      "Epoch 132/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6911 - acc: 0.5084 - val_loss: 0.6728 - val_acc: 0.6927\n",
      "Epoch 133/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6917 - acc: 0.5281 - val_loss: 0.6728 - val_acc: 0.6927\n",
      "Epoch 134/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6849 - acc: 0.5506 - val_loss: 0.6728 - val_acc: 0.6927\n",
      "Epoch 135/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6950 - acc: 0.5225 - val_loss: 0.6728 - val_acc: 0.6927\n",
      "Epoch 136/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6905 - acc: 0.5548 - val_loss: 0.6728 - val_acc: 0.6927\n",
      "Epoch 137/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6947 - acc: 0.5183 - val_loss: 0.6727 - val_acc: 0.6927\n",
      "Epoch 138/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6867 - acc: 0.5421 - val_loss: 0.6727 - val_acc: 0.6927\n",
      "Epoch 139/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6916 - acc: 0.4972 - val_loss: 0.6727 - val_acc: 0.6927\n",
      "Epoch 140/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6914 - acc: 0.5407 - val_loss: 0.6727 - val_acc: 0.6927\n",
      "Epoch 141/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6929 - acc: 0.5365 - val_loss: 0.6727 - val_acc: 0.6927\n",
      "Epoch 142/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6852 - acc: 0.5463 - val_loss: 0.6727 - val_acc: 0.6927\n",
      "Epoch 143/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6957 - acc: 0.5295 - val_loss: 0.6726 - val_acc: 0.6927\n",
      "Epoch 144/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6954 - acc: 0.5140 - val_loss: 0.6726 - val_acc: 0.6927\n",
      "Epoch 145/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6861 - acc: 0.5449 - val_loss: 0.6726 - val_acc: 0.6927\n",
      "Epoch 146/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6838 - acc: 0.5379 - val_loss: 0.6726 - val_acc: 0.6927\n",
      "Epoch 147/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6938 - acc: 0.5183 - val_loss: 0.6726 - val_acc: 0.6927\n",
      "Epoch 148/1000\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.6848 - acc: 0.5449 - val_loss: 0.6725 - val_acc: 0.6927\n",
      "Epoch 149/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6970 - acc: 0.5028 - val_loss: 0.6725 - val_acc: 0.6927\n",
      "Epoch 150/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6938 - acc: 0.5267 - val_loss: 0.6725 - val_acc: 0.6927\n",
      "Epoch 151/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6861 - acc: 0.5492 - val_loss: 0.6725 - val_acc: 0.6927\n",
      "Epoch 152/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6879 - acc: 0.5492 - val_loss: 0.6725 - val_acc: 0.6927\n",
      "Epoch 153/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6850 - acc: 0.5604 - val_loss: 0.6725 - val_acc: 0.6927\n",
      "Epoch 154/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6856 - acc: 0.5520 - val_loss: 0.6724 - val_acc: 0.6927\n",
      "Epoch 155/1000\n",
      "712/712 [==============================] - 0s 170us/step - loss: 0.6926 - acc: 0.5183 - val_loss: 0.6724 - val_acc: 0.6927\n",
      "Epoch 156/1000\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.6796 - acc: 0.5871 - val_loss: 0.6724 - val_acc: 0.6927\n",
      "Epoch 157/1000\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.6904 - acc: 0.5225 - val_loss: 0.6724 - val_acc: 0.6927\n",
      "Epoch 158/1000\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.6824 - acc: 0.5688 - val_loss: 0.6724 - val_acc: 0.6927\n",
      "Epoch 159/1000\n",
      "712/712 [==============================] - 0s 164us/step - loss: 0.6854 - acc: 0.5506 - val_loss: 0.6724 - val_acc: 0.6927\n",
      "Epoch 160/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6903 - acc: 0.5379 - val_loss: 0.6723 - val_acc: 0.6927\n",
      "Epoch 161/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6887 - acc: 0.5534 - val_loss: 0.6723 - val_acc: 0.6927\n",
      "Epoch 162/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6875 - acc: 0.5337 - val_loss: 0.6723 - val_acc: 0.6927\n",
      "Epoch 163/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6921 - acc: 0.5169 - val_loss: 0.6723 - val_acc: 0.6927\n",
      "Epoch 164/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6891 - acc: 0.5449 - val_loss: 0.6723 - val_acc: 0.6927\n",
      "Epoch 165/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6974 - acc: 0.5309 - val_loss: 0.6723 - val_acc: 0.6927\n",
      "Epoch 166/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6965 - acc: 0.4972 - val_loss: 0.6722 - val_acc: 0.6927\n",
      "Epoch 167/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6873 - acc: 0.5435 - val_loss: 0.6722 - val_acc: 0.6927\n",
      "Epoch 168/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6842 - acc: 0.5632 - val_loss: 0.6722 - val_acc: 0.6927\n",
      "Epoch 169/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6885 - acc: 0.5520 - val_loss: 0.6722 - val_acc: 0.6927\n",
      "Epoch 170/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6864 - acc: 0.5183 - val_loss: 0.6722 - val_acc: 0.6927\n",
      "Epoch 171/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6878 - acc: 0.5295 - val_loss: 0.6721 - val_acc: 0.6927\n",
      "Epoch 172/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6891 - acc: 0.5562 - val_loss: 0.6721 - val_acc: 0.6927\n",
      "Epoch 173/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6967 - acc: 0.5084 - val_loss: 0.6721 - val_acc: 0.6927\n",
      "Epoch 174/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6867 - acc: 0.5548 - val_loss: 0.6721 - val_acc: 0.6927\n",
      "Epoch 175/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6897 - acc: 0.5183 - val_loss: 0.6721 - val_acc: 0.6927\n",
      "Epoch 176/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6919 - acc: 0.5379 - val_loss: 0.6721 - val_acc: 0.6927\n",
      "Epoch 177/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6895 - acc: 0.5492 - val_loss: 0.6720 - val_acc: 0.6927\n",
      "Epoch 178/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6897 - acc: 0.5281 - val_loss: 0.6720 - val_acc: 0.6927\n",
      "Epoch 179/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6928 - acc: 0.5449 - val_loss: 0.6720 - val_acc: 0.6927\n",
      "Epoch 180/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6908 - acc: 0.5154 - val_loss: 0.6720 - val_acc: 0.6927\n",
      "Epoch 181/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6865 - acc: 0.5365 - val_loss: 0.6720 - val_acc: 0.6927\n",
      "Epoch 182/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6858 - acc: 0.5646 - val_loss: 0.6720 - val_acc: 0.6927\n",
      "Epoch 183/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6829 - acc: 0.5393 - val_loss: 0.6719 - val_acc: 0.6927\n",
      "Epoch 184/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6905 - acc: 0.5267 - val_loss: 0.6719 - val_acc: 0.6927\n",
      "Epoch 185/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6888 - acc: 0.5393 - val_loss: 0.6719 - val_acc: 0.6927\n",
      "Epoch 186/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6879 - acc: 0.5267 - val_loss: 0.6719 - val_acc: 0.6927\n",
      "Epoch 187/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6828 - acc: 0.5365 - val_loss: 0.6719 - val_acc: 0.6927\n",
      "Epoch 188/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6887 - acc: 0.5183 - val_loss: 0.6718 - val_acc: 0.6927\n",
      "Epoch 189/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6912 - acc: 0.5463 - val_loss: 0.6718 - val_acc: 0.6927\n",
      "Epoch 190/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6898 - acc: 0.5281 - val_loss: 0.6718 - val_acc: 0.6927\n",
      "Epoch 191/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6886 - acc: 0.5478 - val_loss: 0.6718 - val_acc: 0.6927\n",
      "Epoch 192/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6943 - acc: 0.5154 - val_loss: 0.6718 - val_acc: 0.6927\n",
      "Epoch 193/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6871 - acc: 0.5253 - val_loss: 0.6718 - val_acc: 0.6927\n",
      "Epoch 194/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6849 - acc: 0.5407 - val_loss: 0.6717 - val_acc: 0.6927\n",
      "Epoch 195/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6873 - acc: 0.5351 - val_loss: 0.6717 - val_acc: 0.6927\n",
      "Epoch 196/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6909 - acc: 0.5169 - val_loss: 0.6717 - val_acc: 0.6927\n",
      "Epoch 197/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6897 - acc: 0.5140 - val_loss: 0.6717 - val_acc: 0.6927\n",
      "Epoch 198/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6886 - acc: 0.5281 - val_loss: 0.6717 - val_acc: 0.6927\n",
      "Epoch 199/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6918 - acc: 0.5309 - val_loss: 0.6717 - val_acc: 0.6927\n",
      "Epoch 200/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6836 - acc: 0.5562 - val_loss: 0.6716 - val_acc: 0.6927\n",
      "Epoch 201/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6956 - acc: 0.5183 - val_loss: 0.6716 - val_acc: 0.6927\n",
      "Epoch 202/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6898 - acc: 0.5211 - val_loss: 0.6716 - val_acc: 0.6927\n",
      "Epoch 203/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6828 - acc: 0.5744 - val_loss: 0.6716 - val_acc: 0.6927\n",
      "Epoch 204/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6870 - acc: 0.5042 - val_loss: 0.6716 - val_acc: 0.6927\n",
      "Epoch 205/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6884 - acc: 0.5562 - val_loss: 0.6716 - val_acc: 0.6927\n",
      "Epoch 206/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6847 - acc: 0.5632 - val_loss: 0.6715 - val_acc: 0.6927\n",
      "Epoch 207/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6891 - acc: 0.5407 - val_loss: 0.6715 - val_acc: 0.6927\n",
      "Epoch 208/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6890 - acc: 0.5267 - val_loss: 0.6715 - val_acc: 0.6927\n",
      "Epoch 209/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6798 - acc: 0.5674 - val_loss: 0.6715 - val_acc: 0.6927\n",
      "Epoch 210/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6896 - acc: 0.5421 - val_loss: 0.6715 - val_acc: 0.6927\n",
      "Epoch 211/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6862 - acc: 0.5463 - val_loss: 0.6715 - val_acc: 0.6927\n",
      "Epoch 212/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6793 - acc: 0.5632 - val_loss: 0.6714 - val_acc: 0.6927\n",
      "Epoch 213/1000\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.6861 - acc: 0.5548 - val_loss: 0.6714 - val_acc: 0.6983\n",
      "Epoch 214/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6902 - acc: 0.5365 - val_loss: 0.6714 - val_acc: 0.6983\n",
      "Epoch 215/1000\n",
      "712/712 [==============================] - 0s 163us/step - loss: 0.6816 - acc: 0.5520 - val_loss: 0.6714 - val_acc: 0.6983\n",
      "Epoch 216/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6965 - acc: 0.5098 - val_loss: 0.6714 - val_acc: 0.6983\n",
      "Epoch 217/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6911 - acc: 0.5211 - val_loss: 0.6714 - val_acc: 0.6983\n",
      "Epoch 218/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6845 - acc: 0.5435 - val_loss: 0.6713 - val_acc: 0.6983\n",
      "Epoch 219/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6946 - acc: 0.5098 - val_loss: 0.6713 - val_acc: 0.6983\n",
      "Epoch 220/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6848 - acc: 0.5253 - val_loss: 0.6713 - val_acc: 0.6983\n",
      "Epoch 221/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6899 - acc: 0.5197 - val_loss: 0.6713 - val_acc: 0.6983\n",
      "Epoch 222/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6855 - acc: 0.5365 - val_loss: 0.6713 - val_acc: 0.6983\n",
      "Epoch 223/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6884 - acc: 0.5562 - val_loss: 0.6713 - val_acc: 0.6983\n",
      "Epoch 224/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6907 - acc: 0.5548 - val_loss: 0.6712 - val_acc: 0.6983\n",
      "Epoch 225/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6893 - acc: 0.5239 - val_loss: 0.6712 - val_acc: 0.6983\n",
      "Epoch 226/1000\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.6934 - acc: 0.5154 - val_loss: 0.6712 - val_acc: 0.6983\n",
      "Epoch 227/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6867 - acc: 0.5337 - val_loss: 0.6712 - val_acc: 0.6983\n",
      "Epoch 228/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6904 - acc: 0.5239 - val_loss: 0.6712 - val_acc: 0.6983\n",
      "Epoch 229/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6951 - acc: 0.5028 - val_loss: 0.6712 - val_acc: 0.6983\n",
      "Epoch 230/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6859 - acc: 0.5351 - val_loss: 0.6711 - val_acc: 0.6983\n",
      "Epoch 231/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6905 - acc: 0.5126 - val_loss: 0.6711 - val_acc: 0.6983\n",
      "Epoch 232/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6887 - acc: 0.5112 - val_loss: 0.6711 - val_acc: 0.6983\n",
      "Epoch 233/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6795 - acc: 0.5843 - val_loss: 0.6711 - val_acc: 0.6983\n",
      "Epoch 234/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6899 - acc: 0.5323 - val_loss: 0.6711 - val_acc: 0.6983\n",
      "Epoch 235/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6852 - acc: 0.5421 - val_loss: 0.6711 - val_acc: 0.6983\n",
      "Epoch 236/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6821 - acc: 0.5618 - val_loss: 0.6710 - val_acc: 0.6983\n",
      "Epoch 237/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6856 - acc: 0.5407 - val_loss: 0.6710 - val_acc: 0.6983\n",
      "Epoch 238/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 138us/step - loss: 0.6839 - acc: 0.5435 - val_loss: 0.6710 - val_acc: 0.6983\n",
      "Epoch 239/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6899 - acc: 0.5421 - val_loss: 0.6710 - val_acc: 0.6983\n",
      "Epoch 240/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6904 - acc: 0.5604 - val_loss: 0.6710 - val_acc: 0.6983\n",
      "Epoch 241/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6839 - acc: 0.5744 - val_loss: 0.6710 - val_acc: 0.6983\n",
      "Epoch 242/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6950 - acc: 0.5183 - val_loss: 0.6709 - val_acc: 0.6983\n",
      "Epoch 243/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6897 - acc: 0.5309 - val_loss: 0.6709 - val_acc: 0.6983\n",
      "Epoch 244/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6763 - acc: 0.5702 - val_loss: 0.6709 - val_acc: 0.6983\n",
      "Epoch 245/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6813 - acc: 0.5674 - val_loss: 0.6709 - val_acc: 0.6983\n",
      "Epoch 246/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6751 - acc: 0.5913 - val_loss: 0.6709 - val_acc: 0.6983\n",
      "Epoch 247/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6895 - acc: 0.5225 - val_loss: 0.6708 - val_acc: 0.6983\n",
      "Epoch 248/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6849 - acc: 0.5449 - val_loss: 0.6708 - val_acc: 0.6983\n",
      "Epoch 249/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6923 - acc: 0.5421 - val_loss: 0.6708 - val_acc: 0.6983\n",
      "Epoch 250/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6949 - acc: 0.5014 - val_loss: 0.6708 - val_acc: 0.6983\n",
      "Epoch 251/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6861 - acc: 0.5323 - val_loss: 0.6708 - val_acc: 0.6983\n",
      "Epoch 252/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6866 - acc: 0.5407 - val_loss: 0.6708 - val_acc: 0.6983\n",
      "Epoch 253/1000\n",
      "712/712 [==============================] - 0s 90us/step - loss: 0.6808 - acc: 0.5646 - val_loss: 0.6707 - val_acc: 0.6983\n",
      "Epoch 254/1000\n",
      "712/712 [==============================] - 0s 76us/step - loss: 0.6847 - acc: 0.5506 - val_loss: 0.6707 - val_acc: 0.6983\n",
      "Epoch 255/1000\n",
      "712/712 [==============================] - 0s 91us/step - loss: 0.6947 - acc: 0.5183 - val_loss: 0.6707 - val_acc: 0.6983\n",
      "Epoch 256/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6868 - acc: 0.5449 - val_loss: 0.6707 - val_acc: 0.6983\n",
      "Epoch 257/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6839 - acc: 0.5632 - val_loss: 0.6707 - val_acc: 0.6983\n",
      "Epoch 258/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6804 - acc: 0.5478 - val_loss: 0.6707 - val_acc: 0.6983\n",
      "Epoch 259/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6867 - acc: 0.5604 - val_loss: 0.6706 - val_acc: 0.6983\n",
      "Epoch 260/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6914 - acc: 0.5337 - val_loss: 0.6706 - val_acc: 0.6983\n",
      "Epoch 261/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6883 - acc: 0.5253 - val_loss: 0.6706 - val_acc: 0.6983\n",
      "Epoch 262/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6802 - acc: 0.5787 - val_loss: 0.6706 - val_acc: 0.6983\n",
      "Epoch 263/1000\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.6936 - acc: 0.5183 - val_loss: 0.6706 - val_acc: 0.6983\n",
      "Epoch 264/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6779 - acc: 0.5646 - val_loss: 0.6706 - val_acc: 0.6704\n",
      "Epoch 265/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6845 - acc: 0.5393 - val_loss: 0.6705 - val_acc: 0.6704\n",
      "Epoch 266/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6898 - acc: 0.5154 - val_loss: 0.6705 - val_acc: 0.6704\n",
      "Epoch 267/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6899 - acc: 0.5323 - val_loss: 0.6705 - val_acc: 0.6704\n",
      "Epoch 268/1000\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.6868 - acc: 0.5407 - val_loss: 0.6705 - val_acc: 0.6704\n",
      "Epoch 269/1000\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.6911 - acc: 0.5197 - val_loss: 0.6705 - val_acc: 0.6704\n",
      "Epoch 270/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6960 - acc: 0.4986 - val_loss: 0.6705 - val_acc: 0.6704\n",
      "Epoch 271/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6875 - acc: 0.5379 - val_loss: 0.6704 - val_acc: 0.6704\n",
      "Epoch 272/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6906 - acc: 0.5253 - val_loss: 0.6704 - val_acc: 0.6704\n",
      "Epoch 273/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6819 - acc: 0.5730 - val_loss: 0.6704 - val_acc: 0.6704\n",
      "Epoch 274/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6916 - acc: 0.5323 - val_loss: 0.6704 - val_acc: 0.6704\n",
      "Epoch 275/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6843 - acc: 0.5520 - val_loss: 0.6704 - val_acc: 0.6704\n",
      "Epoch 276/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6878 - acc: 0.5365 - val_loss: 0.6704 - val_acc: 0.6704\n",
      "Epoch 277/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6904 - acc: 0.5253 - val_loss: 0.6703 - val_acc: 0.6704\n",
      "Epoch 278/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6937 - acc: 0.5169 - val_loss: 0.6703 - val_acc: 0.6704\n",
      "Epoch 279/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6812 - acc: 0.5843 - val_loss: 0.6703 - val_acc: 0.6704\n",
      "Epoch 280/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6955 - acc: 0.5239 - val_loss: 0.6703 - val_acc: 0.6704\n",
      "Epoch 281/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6906 - acc: 0.5154 - val_loss: 0.6703 - val_acc: 0.6704\n",
      "Epoch 282/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6821 - acc: 0.5590 - val_loss: 0.6703 - val_acc: 0.6704\n",
      "Epoch 283/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6879 - acc: 0.5323 - val_loss: 0.6702 - val_acc: 0.6704\n",
      "Epoch 284/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6943 - acc: 0.5281 - val_loss: 0.6702 - val_acc: 0.6704\n",
      "Epoch 285/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6836 - acc: 0.5534 - val_loss: 0.6702 - val_acc: 0.6704\n",
      "Epoch 286/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6862 - acc: 0.5660 - val_loss: 0.6702 - val_acc: 0.6704\n",
      "Epoch 287/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6856 - acc: 0.5379 - val_loss: 0.6702 - val_acc: 0.6704\n",
      "Epoch 288/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6863 - acc: 0.5576 - val_loss: 0.6702 - val_acc: 0.6704\n",
      "Epoch 289/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6879 - acc: 0.5393 - val_loss: 0.6701 - val_acc: 0.6704\n",
      "Epoch 290/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6869 - acc: 0.5674 - val_loss: 0.6701 - val_acc: 0.6704\n",
      "Epoch 291/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6997 - acc: 0.4944 - val_loss: 0.6701 - val_acc: 0.6704\n",
      "Epoch 292/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6906 - acc: 0.5197 - val_loss: 0.6701 - val_acc: 0.6704\n",
      "Epoch 293/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6851 - acc: 0.5393 - val_loss: 0.6701 - val_acc: 0.6704\n",
      "Epoch 294/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6835 - acc: 0.5449 - val_loss: 0.6700 - val_acc: 0.6704\n",
      "Epoch 295/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6865 - acc: 0.5478 - val_loss: 0.6700 - val_acc: 0.6704\n",
      "Epoch 296/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6831 - acc: 0.5379 - val_loss: 0.6700 - val_acc: 0.6704\n",
      "Epoch 297/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6913 - acc: 0.5140 - val_loss: 0.6700 - val_acc: 0.6704\n",
      "Epoch 298/1000\n",
      "712/712 [==============================] - 0s 120us/step - loss: 0.6839 - acc: 0.5772 - val_loss: 0.6700 - val_acc: 0.6704\n",
      "Epoch 299/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6818 - acc: 0.5590 - val_loss: 0.6700 - val_acc: 0.6704\n",
      "Epoch 300/1000\n",
      "712/712 [==============================] - 0s 109us/step - loss: 0.6878 - acc: 0.5295 - val_loss: 0.6699 - val_acc: 0.6704\n",
      "Epoch 301/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6926 - acc: 0.5140 - val_loss: 0.6699 - val_acc: 0.6704\n",
      "Epoch 302/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6891 - acc: 0.5183 - val_loss: 0.6699 - val_acc: 0.6704\n",
      "Epoch 303/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6868 - acc: 0.5351 - val_loss: 0.6699 - val_acc: 0.6704\n",
      "Epoch 304/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6887 - acc: 0.5323 - val_loss: 0.6699 - val_acc: 0.6704\n",
      "Epoch 305/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6894 - acc: 0.5197 - val_loss: 0.6699 - val_acc: 0.6704\n",
      "Epoch 306/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6929 - acc: 0.5154 - val_loss: 0.6698 - val_acc: 0.6704\n",
      "Epoch 307/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6879 - acc: 0.5478 - val_loss: 0.6698 - val_acc: 0.6704\n",
      "Epoch 308/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6948 - acc: 0.5028 - val_loss: 0.6698 - val_acc: 0.6704\n",
      "Epoch 309/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6936 - acc: 0.5225 - val_loss: 0.6698 - val_acc: 0.6704\n",
      "Epoch 310/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6872 - acc: 0.5478 - val_loss: 0.6698 - val_acc: 0.6704\n",
      "Epoch 311/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6879 - acc: 0.5183 - val_loss: 0.6698 - val_acc: 0.6704\n",
      "Epoch 312/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6909 - acc: 0.5197 - val_loss: 0.6697 - val_acc: 0.6704\n",
      "Epoch 313/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6822 - acc: 0.5716 - val_loss: 0.6697 - val_acc: 0.6704\n",
      "Epoch 314/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6881 - acc: 0.5435 - val_loss: 0.6697 - val_acc: 0.6704\n",
      "Epoch 315/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6890 - acc: 0.5281 - val_loss: 0.6697 - val_acc: 0.6704\n",
      "Epoch 316/1000\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.6919 - acc: 0.5309 - val_loss: 0.6697 - val_acc: 0.6704\n",
      "Epoch 317/1000\n",
      "712/712 [==============================] - 0s 162us/step - loss: 0.6833 - acc: 0.5590 - val_loss: 0.6697 - val_acc: 0.6704\n",
      "Epoch 318/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6925 - acc: 0.5337 - val_loss: 0.6696 - val_acc: 0.6704\n",
      "Epoch 319/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6783 - acc: 0.5604 - val_loss: 0.6696 - val_acc: 0.6704\n",
      "Epoch 320/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6758 - acc: 0.6025 - val_loss: 0.6696 - val_acc: 0.6704\n",
      "Epoch 321/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6925 - acc: 0.5379 - val_loss: 0.6696 - val_acc: 0.6704\n",
      "Epoch 322/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6956 - acc: 0.5112 - val_loss: 0.6696 - val_acc: 0.6704\n",
      "Epoch 323/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.7000 - acc: 0.5070 - val_loss: 0.6696 - val_acc: 0.6704\n",
      "Epoch 324/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6747 - acc: 0.5969 - val_loss: 0.6695 - val_acc: 0.6704\n",
      "Epoch 325/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6890 - acc: 0.5351 - val_loss: 0.6695 - val_acc: 0.6704\n",
      "Epoch 326/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6957 - acc: 0.5281 - val_loss: 0.6695 - val_acc: 0.6704\n",
      "Epoch 327/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6854 - acc: 0.5463 - val_loss: 0.6695 - val_acc: 0.6704\n",
      "Epoch 328/1000\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.6936 - acc: 0.5098 - val_loss: 0.6695 - val_acc: 0.6704\n",
      "Epoch 329/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6904 - acc: 0.5281 - val_loss: 0.6695 - val_acc: 0.6704\n",
      "Epoch 330/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6906 - acc: 0.5309 - val_loss: 0.6694 - val_acc: 0.6704\n",
      "Epoch 331/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6886 - acc: 0.5183 - val_loss: 0.6694 - val_acc: 0.6704\n",
      "Epoch 332/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6893 - acc: 0.5281 - val_loss: 0.6694 - val_acc: 0.6704\n",
      "Epoch 333/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6868 - acc: 0.5506 - val_loss: 0.6694 - val_acc: 0.6704\n",
      "Epoch 334/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6812 - acc: 0.5702 - val_loss: 0.6694 - val_acc: 0.6704\n",
      "Epoch 335/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6867 - acc: 0.5520 - val_loss: 0.6694 - val_acc: 0.6704\n",
      "Epoch 336/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6872 - acc: 0.5463 - val_loss: 0.6693 - val_acc: 0.6704\n",
      "Epoch 337/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6930 - acc: 0.5239 - val_loss: 0.6693 - val_acc: 0.6704\n",
      "Epoch 338/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6832 - acc: 0.5632 - val_loss: 0.6693 - val_acc: 0.6704\n",
      "Epoch 339/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6850 - acc: 0.5449 - val_loss: 0.6693 - val_acc: 0.6704\n",
      "Epoch 340/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6897 - acc: 0.5323 - val_loss: 0.6693 - val_acc: 0.6704\n",
      "Epoch 341/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6882 - acc: 0.5449 - val_loss: 0.6693 - val_acc: 0.6704\n",
      "Epoch 342/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6891 - acc: 0.5337 - val_loss: 0.6692 - val_acc: 0.6704\n",
      "Epoch 343/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6860 - acc: 0.5267 - val_loss: 0.6692 - val_acc: 0.6704\n",
      "Epoch 344/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6870 - acc: 0.5449 - val_loss: 0.6692 - val_acc: 0.6704\n",
      "Epoch 345/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6910 - acc: 0.5295 - val_loss: 0.6692 - val_acc: 0.6704\n",
      "Epoch 346/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6947 - acc: 0.5309 - val_loss: 0.6692 - val_acc: 0.6704\n",
      "Epoch 347/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6770 - acc: 0.5857 - val_loss: 0.6692 - val_acc: 0.6704\n",
      "Epoch 348/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6854 - acc: 0.5590 - val_loss: 0.6691 - val_acc: 0.6704\n",
      "Epoch 349/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6887 - acc: 0.5183 - val_loss: 0.6691 - val_acc: 0.6704\n",
      "Epoch 350/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6874 - acc: 0.5126 - val_loss: 0.6691 - val_acc: 0.6704\n",
      "Epoch 351/1000\n",
      "712/712 [==============================] - 0s 94us/step - loss: 0.6790 - acc: 0.5885 - val_loss: 0.6691 - val_acc: 0.6704\n",
      "Epoch 352/1000\n",
      "712/712 [==============================] - 0s 106us/step - loss: 0.6816 - acc: 0.5758 - val_loss: 0.6691 - val_acc: 0.6704\n",
      "Epoch 353/1000\n",
      "712/712 [==============================] - 0s 97us/step - loss: 0.6896 - acc: 0.5351 - val_loss: 0.6691 - val_acc: 0.6704\n",
      "Epoch 354/1000\n",
      "712/712 [==============================] - 0s 100us/step - loss: 0.6800 - acc: 0.5562 - val_loss: 0.6690 - val_acc: 0.6704\n",
      "Epoch 355/1000\n",
      "712/712 [==============================] - 0s 101us/step - loss: 0.6837 - acc: 0.5281 - val_loss: 0.6690 - val_acc: 0.6704\n",
      "Epoch 356/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 109us/step - loss: 0.6889 - acc: 0.5478 - val_loss: 0.6690 - val_acc: 0.6704\n",
      "Epoch 357/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6941 - acc: 0.5112 - val_loss: 0.6690 - val_acc: 0.6704\n",
      "Epoch 358/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6838 - acc: 0.5646 - val_loss: 0.6690 - val_acc: 0.6704\n",
      "Epoch 359/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6912 - acc: 0.5112 - val_loss: 0.6690 - val_acc: 0.6704\n",
      "Epoch 360/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6926 - acc: 0.5351 - val_loss: 0.6689 - val_acc: 0.6704\n",
      "Epoch 361/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6870 - acc: 0.5463 - val_loss: 0.6689 - val_acc: 0.6704\n",
      "Epoch 362/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6884 - acc: 0.5295 - val_loss: 0.6689 - val_acc: 0.6704\n",
      "Epoch 363/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6920 - acc: 0.5140 - val_loss: 0.6689 - val_acc: 0.6704\n",
      "Epoch 364/1000\n",
      "712/712 [==============================] - 0s 108us/step - loss: 0.6872 - acc: 0.5407 - val_loss: 0.6689 - val_acc: 0.6704\n",
      "Epoch 365/1000\n",
      "712/712 [==============================] - 0s 101us/step - loss: 0.6856 - acc: 0.5520 - val_loss: 0.6688 - val_acc: 0.6704\n",
      "Epoch 366/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6881 - acc: 0.5239 - val_loss: 0.6688 - val_acc: 0.6704\n",
      "Epoch 367/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6895 - acc: 0.5393 - val_loss: 0.6688 - val_acc: 0.6704\n",
      "Epoch 368/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6813 - acc: 0.5604 - val_loss: 0.6688 - val_acc: 0.6704\n",
      "Epoch 369/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6897 - acc: 0.5225 - val_loss: 0.6688 - val_acc: 0.6704\n",
      "Epoch 370/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6838 - acc: 0.5688 - val_loss: 0.6688 - val_acc: 0.6704\n",
      "Epoch 371/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6918 - acc: 0.5421 - val_loss: 0.6688 - val_acc: 0.6704\n",
      "Epoch 372/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6878 - acc: 0.5365 - val_loss: 0.6687 - val_acc: 0.6704\n",
      "Epoch 373/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6896 - acc: 0.5295 - val_loss: 0.6687 - val_acc: 0.6704\n",
      "Epoch 374/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6872 - acc: 0.5267 - val_loss: 0.6687 - val_acc: 0.6704\n",
      "Epoch 375/1000\n",
      "712/712 [==============================] - 0s 106us/step - loss: 0.6905 - acc: 0.5225 - val_loss: 0.6687 - val_acc: 0.6704\n",
      "Epoch 376/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6903 - acc: 0.5351 - val_loss: 0.6687 - val_acc: 0.6592\n",
      "Epoch 377/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6876 - acc: 0.5281 - val_loss: 0.6687 - val_acc: 0.6592\n",
      "Epoch 378/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6866 - acc: 0.5379 - val_loss: 0.6686 - val_acc: 0.6592\n",
      "Epoch 379/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6832 - acc: 0.5407 - val_loss: 0.6686 - val_acc: 0.6592\n",
      "Epoch 380/1000\n",
      "712/712 [==============================] - 0s 102us/step - loss: 0.6886 - acc: 0.5478 - val_loss: 0.6686 - val_acc: 0.6592\n",
      "Epoch 381/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6864 - acc: 0.5365 - val_loss: 0.6686 - val_acc: 0.6592\n",
      "Epoch 382/1000\n",
      "712/712 [==============================] - 0s 103us/step - loss: 0.6797 - acc: 0.5492 - val_loss: 0.6686 - val_acc: 0.6592\n",
      "Epoch 383/1000\n",
      "712/712 [==============================] - 0s 108us/step - loss: 0.6902 - acc: 0.5309 - val_loss: 0.6686 - val_acc: 0.6592\n",
      "Epoch 384/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6860 - acc: 0.5506 - val_loss: 0.6685 - val_acc: 0.6592\n",
      "Epoch 385/1000\n",
      "712/712 [==============================] - 0s 103us/step - loss: 0.6900 - acc: 0.5351 - val_loss: 0.6685 - val_acc: 0.6592\n",
      "Epoch 386/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6879 - acc: 0.5351 - val_loss: 0.6685 - val_acc: 0.6592\n",
      "Epoch 387/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6854 - acc: 0.5744 - val_loss: 0.6685 - val_acc: 0.6592\n",
      "Epoch 388/1000\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.6848 - acc: 0.5674 - val_loss: 0.6685 - val_acc: 0.6592\n",
      "Epoch 389/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6878 - acc: 0.5506 - val_loss: 0.6685 - val_acc: 0.6592\n",
      "Epoch 390/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6850 - acc: 0.5435 - val_loss: 0.6684 - val_acc: 0.6592\n",
      "Epoch 391/1000\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.6793 - acc: 0.5899 - val_loss: 0.6684 - val_acc: 0.6592\n",
      "Epoch 392/1000\n",
      "712/712 [==============================] - 0s 163us/step - loss: 0.6870 - acc: 0.5309 - val_loss: 0.6684 - val_acc: 0.6592\n",
      "Epoch 393/1000\n",
      "712/712 [==============================] - 0s 171us/step - loss: 0.6882 - acc: 0.5337 - val_loss: 0.6684 - val_acc: 0.6592\n",
      "Epoch 394/1000\n",
      "712/712 [==============================] - 0s 169us/step - loss: 0.6784 - acc: 0.5857 - val_loss: 0.6684 - val_acc: 0.6592\n",
      "Epoch 395/1000\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.6825 - acc: 0.5688 - val_loss: 0.6684 - val_acc: 0.6592\n",
      "Epoch 396/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6868 - acc: 0.5323 - val_loss: 0.6683 - val_acc: 0.6592\n",
      "Epoch 397/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6909 - acc: 0.5534 - val_loss: 0.6683 - val_acc: 0.6592\n",
      "Epoch 398/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6944 - acc: 0.5211 - val_loss: 0.6683 - val_acc: 0.6592\n",
      "Epoch 399/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6864 - acc: 0.5492 - val_loss: 0.6683 - val_acc: 0.6592\n",
      "Epoch 400/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6909 - acc: 0.5225 - val_loss: 0.6683 - val_acc: 0.6592\n",
      "Epoch 401/1000\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.6903 - acc: 0.5126 - val_loss: 0.6683 - val_acc: 0.6592\n",
      "Epoch 402/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6873 - acc: 0.5351 - val_loss: 0.6682 - val_acc: 0.6592\n",
      "Epoch 403/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6849 - acc: 0.5435 - val_loss: 0.6682 - val_acc: 0.6592\n",
      "Epoch 404/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6829 - acc: 0.5562 - val_loss: 0.6682 - val_acc: 0.6592\n",
      "Epoch 405/1000\n",
      "712/712 [==============================] - 0s 108us/step - loss: 0.6858 - acc: 0.5323 - val_loss: 0.6682 - val_acc: 0.6592\n",
      "Epoch 406/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6839 - acc: 0.5365 - val_loss: 0.6682 - val_acc: 0.6592\n",
      "Epoch 407/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6916 - acc: 0.5323 - val_loss: 0.6682 - val_acc: 0.6592\n",
      "Epoch 408/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6886 - acc: 0.5295 - val_loss: 0.6681 - val_acc: 0.6592\n",
      "Epoch 409/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6934 - acc: 0.5140 - val_loss: 0.6681 - val_acc: 0.6592\n",
      "Epoch 410/1000\n",
      "712/712 [==============================] - 0s 109us/step - loss: 0.6865 - acc: 0.5435 - val_loss: 0.6681 - val_acc: 0.6592\n",
      "Epoch 411/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6830 - acc: 0.5758 - val_loss: 0.6681 - val_acc: 0.6592\n",
      "Epoch 412/1000\n",
      "712/712 [==============================] - 0s 176us/step - loss: 0.6882 - acc: 0.5506 - val_loss: 0.6681 - val_acc: 0.6592\n",
      "Epoch 413/1000\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.6810 - acc: 0.5618 - val_loss: 0.6681 - val_acc: 0.6592\n",
      "Epoch 414/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6860 - acc: 0.5548 - val_loss: 0.6680 - val_acc: 0.6592\n",
      "Epoch 415/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6856 - acc: 0.5604 - val_loss: 0.6680 - val_acc: 0.6592\n",
      "Epoch 416/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6810 - acc: 0.5576 - val_loss: 0.6680 - val_acc: 0.6592\n",
      "Epoch 417/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6900 - acc: 0.5463 - val_loss: 0.6680 - val_acc: 0.6592\n",
      "Epoch 418/1000\n",
      "712/712 [==============================] - 0s 98us/step - loss: 0.6885 - acc: 0.5267 - val_loss: 0.6680 - val_acc: 0.6592\n",
      "Epoch 419/1000\n",
      "712/712 [==============================] - 0s 120us/step - loss: 0.6847 - acc: 0.5744 - val_loss: 0.6680 - val_acc: 0.6592\n",
      "Epoch 420/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6876 - acc: 0.5562 - val_loss: 0.6679 - val_acc: 0.6592\n",
      "Epoch 421/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6821 - acc: 0.5660 - val_loss: 0.6679 - val_acc: 0.6592\n",
      "Epoch 422/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6890 - acc: 0.5183 - val_loss: 0.6679 - val_acc: 0.6592\n",
      "Epoch 423/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6856 - acc: 0.5548 - val_loss: 0.6679 - val_acc: 0.6592\n",
      "Epoch 424/1000\n",
      "712/712 [==============================] - 0s 106us/step - loss: 0.6934 - acc: 0.5211 - val_loss: 0.6679 - val_acc: 0.6592\n",
      "Epoch 425/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6841 - acc: 0.5604 - val_loss: 0.6679 - val_acc: 0.6592\n",
      "Epoch 426/1000\n",
      "712/712 [==============================] - ETA: 0s - loss: 0.6996 - acc: 0.502 - 0s 135us/step - loss: 0.6954 - acc: 0.5239 - val_loss: 0.6678 - val_acc: 0.6592\n",
      "Epoch 427/1000\n",
      "712/712 [==============================] - 0s 96us/step - loss: 0.6847 - acc: 0.5337 - val_loss: 0.6678 - val_acc: 0.6592\n",
      "Epoch 428/1000\n",
      "712/712 [==============================] - 0s 98us/step - loss: 0.6895 - acc: 0.5520 - val_loss: 0.6678 - val_acc: 0.6592\n",
      "Epoch 429/1000\n",
      "712/712 [==============================] - 0s 98us/step - loss: 0.6920 - acc: 0.5478 - val_loss: 0.6678 - val_acc: 0.6592\n",
      "Epoch 430/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6807 - acc: 0.5520 - val_loss: 0.6678 - val_acc: 0.6592\n",
      "Epoch 431/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6900 - acc: 0.5239 - val_loss: 0.6678 - val_acc: 0.6592\n",
      "Epoch 432/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6883 - acc: 0.5309 - val_loss: 0.6677 - val_acc: 0.6592\n",
      "Epoch 433/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6857 - acc: 0.5506 - val_loss: 0.6677 - val_acc: 0.6592\n",
      "Epoch 434/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6930 - acc: 0.5098 - val_loss: 0.6677 - val_acc: 0.6592\n",
      "Epoch 435/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6870 - acc: 0.5351 - val_loss: 0.6677 - val_acc: 0.6592\n",
      "Epoch 436/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6833 - acc: 0.5688 - val_loss: 0.6677 - val_acc: 0.6592\n",
      "Epoch 437/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6932 - acc: 0.5225 - val_loss: 0.6677 - val_acc: 0.6592\n",
      "Epoch 438/1000\n",
      "712/712 [==============================] - 0s 100us/step - loss: 0.6891 - acc: 0.5365 - val_loss: 0.6676 - val_acc: 0.6592\n",
      "Epoch 439/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6864 - acc: 0.5351 - val_loss: 0.6676 - val_acc: 0.6592\n",
      "Epoch 440/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6890 - acc: 0.5140 - val_loss: 0.6676 - val_acc: 0.6592\n",
      "Epoch 441/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6868 - acc: 0.5393 - val_loss: 0.6676 - val_acc: 0.6592\n",
      "Epoch 442/1000\n",
      "712/712 [==============================] - 0s 97us/step - loss: 0.6820 - acc: 0.5520 - val_loss: 0.6676 - val_acc: 0.6592\n",
      "Epoch 443/1000\n",
      "712/712 [==============================] - 0s 101us/step - loss: 0.6810 - acc: 0.5590 - val_loss: 0.6676 - val_acc: 0.6592\n",
      "Epoch 444/1000\n",
      "712/712 [==============================] - 0s 99us/step - loss: 0.6856 - acc: 0.5393 - val_loss: 0.6675 - val_acc: 0.6592\n",
      "Epoch 445/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6837 - acc: 0.5618 - val_loss: 0.6675 - val_acc: 0.6592\n",
      "Epoch 446/1000\n",
      "712/712 [==============================] - 0s 102us/step - loss: 0.6901 - acc: 0.5449 - val_loss: 0.6675 - val_acc: 0.6592\n",
      "Epoch 447/1000\n",
      "712/712 [==============================] - 0s 101us/step - loss: 0.6927 - acc: 0.5211 - val_loss: 0.6675 - val_acc: 0.6592\n",
      "Epoch 448/1000\n",
      "712/712 [==============================] - 0s 100us/step - loss: 0.6824 - acc: 0.5435 - val_loss: 0.6675 - val_acc: 0.6592\n",
      "Epoch 449/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6750 - acc: 0.5829 - val_loss: 0.6675 - val_acc: 0.6592\n",
      "Epoch 450/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6865 - acc: 0.5534 - val_loss: 0.6675 - val_acc: 0.6592\n",
      "Epoch 451/1000\n",
      "712/712 [==============================] - 0s 158us/step - loss: 0.6866 - acc: 0.5421 - val_loss: 0.6674 - val_acc: 0.6592\n",
      "Epoch 452/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6794 - acc: 0.5815 - val_loss: 0.6674 - val_acc: 0.6592\n",
      "Epoch 453/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6832 - acc: 0.5463 - val_loss: 0.6674 - val_acc: 0.6592\n",
      "Epoch 454/1000\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.6781 - acc: 0.5787 - val_loss: 0.6674 - val_acc: 0.6592\n",
      "Epoch 455/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6806 - acc: 0.5365 - val_loss: 0.6674 - val_acc: 0.6592\n",
      "Epoch 456/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6813 - acc: 0.5801 - val_loss: 0.6674 - val_acc: 0.6592\n",
      "Epoch 457/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6766 - acc: 0.5801 - val_loss: 0.6673 - val_acc: 0.6592\n",
      "Epoch 458/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6839 - acc: 0.5646 - val_loss: 0.6673 - val_acc: 0.6592\n",
      "Epoch 459/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6890 - acc: 0.5323 - val_loss: 0.6673 - val_acc: 0.6592\n",
      "Epoch 460/1000\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.6910 - acc: 0.5169 - val_loss: 0.6673 - val_acc: 0.6592\n",
      "Epoch 461/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6906 - acc: 0.5323 - val_loss: 0.6673 - val_acc: 0.6592\n",
      "Epoch 462/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6862 - acc: 0.5492 - val_loss: 0.6673 - val_acc: 0.6592\n",
      "Epoch 463/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6825 - acc: 0.5548 - val_loss: 0.6672 - val_acc: 0.6592\n",
      "Epoch 464/1000\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.6908 - acc: 0.5449 - val_loss: 0.6672 - val_acc: 0.6592\n",
      "Epoch 465/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6910 - acc: 0.5295 - val_loss: 0.6672 - val_acc: 0.6592\n",
      "Epoch 466/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6915 - acc: 0.5239 - val_loss: 0.6672 - val_acc: 0.6592\n",
      "Epoch 467/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6808 - acc: 0.5548 - val_loss: 0.6672 - val_acc: 0.6592\n",
      "Epoch 468/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6871 - acc: 0.5492 - val_loss: 0.6672 - val_acc: 0.6592\n",
      "Epoch 469/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6828 - acc: 0.5576 - val_loss: 0.6671 - val_acc: 0.6592\n",
      "Epoch 470/1000\n",
      "712/712 [==============================] - 0s 164us/step - loss: 0.6817 - acc: 0.5393 - val_loss: 0.6671 - val_acc: 0.6592\n",
      "Epoch 471/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6845 - acc: 0.5646 - val_loss: 0.6671 - val_acc: 0.6592\n",
      "Epoch 472/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6827 - acc: 0.5829 - val_loss: 0.6671 - val_acc: 0.6592\n",
      "Epoch 473/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6809 - acc: 0.5492 - val_loss: 0.6671 - val_acc: 0.6592\n",
      "Epoch 474/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 128us/step - loss: 0.6874 - acc: 0.5463 - val_loss: 0.6671 - val_acc: 0.6592\n",
      "Epoch 475/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6838 - acc: 0.5534 - val_loss: 0.6670 - val_acc: 0.6592\n",
      "Epoch 476/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6828 - acc: 0.5478 - val_loss: 0.6670 - val_acc: 0.6592\n",
      "Epoch 477/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6805 - acc: 0.5646 - val_loss: 0.6670 - val_acc: 0.6592\n",
      "Epoch 478/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6914 - acc: 0.5225 - val_loss: 0.6670 - val_acc: 0.6592\n",
      "Epoch 479/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6775 - acc: 0.5955 - val_loss: 0.6670 - val_acc: 0.6592\n",
      "Epoch 480/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6898 - acc: 0.5435 - val_loss: 0.6670 - val_acc: 0.6592\n",
      "Epoch 481/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6866 - acc: 0.5506 - val_loss: 0.6670 - val_acc: 0.6592\n",
      "Epoch 482/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6807 - acc: 0.5604 - val_loss: 0.6669 - val_acc: 0.6592\n",
      "Epoch 483/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6815 - acc: 0.5590 - val_loss: 0.6669 - val_acc: 0.6592\n",
      "Epoch 484/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6869 - acc: 0.5351 - val_loss: 0.6669 - val_acc: 0.6592\n",
      "Epoch 485/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6842 - acc: 0.5674 - val_loss: 0.6669 - val_acc: 0.6592\n",
      "Epoch 486/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6799 - acc: 0.5632 - val_loss: 0.6669 - val_acc: 0.6592\n",
      "Epoch 487/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6809 - acc: 0.5435 - val_loss: 0.6669 - val_acc: 0.6592\n",
      "Epoch 488/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6923 - acc: 0.5337 - val_loss: 0.6668 - val_acc: 0.6592\n",
      "Epoch 489/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6888 - acc: 0.5337 - val_loss: 0.6668 - val_acc: 0.6592\n",
      "Epoch 490/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6810 - acc: 0.5688 - val_loss: 0.6668 - val_acc: 0.6592\n",
      "Epoch 491/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6785 - acc: 0.5815 - val_loss: 0.6668 - val_acc: 0.6592\n",
      "Epoch 492/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6879 - acc: 0.5351 - val_loss: 0.6668 - val_acc: 0.6592\n",
      "Epoch 493/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6817 - acc: 0.5660 - val_loss: 0.6668 - val_acc: 0.6592\n",
      "Epoch 494/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6828 - acc: 0.5449 - val_loss: 0.6667 - val_acc: 0.6592\n",
      "Epoch 495/1000\n",
      "712/712 [==============================] - 0s 105us/step - loss: 0.6853 - acc: 0.5646 - val_loss: 0.6667 - val_acc: 0.6592\n",
      "Epoch 496/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6837 - acc: 0.5562 - val_loss: 0.6667 - val_acc: 0.6592\n",
      "Epoch 497/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6871 - acc: 0.5323 - val_loss: 0.6667 - val_acc: 0.6592\n",
      "Epoch 498/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6821 - acc: 0.5787 - val_loss: 0.6667 - val_acc: 0.6592\n",
      "Epoch 499/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6910 - acc: 0.4944 - val_loss: 0.6667 - val_acc: 0.6592\n",
      "Epoch 500/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6920 - acc: 0.5140 - val_loss: 0.6666 - val_acc: 0.6592\n",
      "Epoch 501/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6879 - acc: 0.5407 - val_loss: 0.6666 - val_acc: 0.6592\n",
      "Epoch 502/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6890 - acc: 0.5253 - val_loss: 0.6666 - val_acc: 0.6592\n",
      "Epoch 503/1000\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.6860 - acc: 0.5506 - val_loss: 0.6666 - val_acc: 0.6592\n",
      "Epoch 504/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6808 - acc: 0.5365 - val_loss: 0.6666 - val_acc: 0.6592\n",
      "Epoch 505/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6865 - acc: 0.5463 - val_loss: 0.6666 - val_acc: 0.6592\n",
      "Epoch 506/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6882 - acc: 0.5478 - val_loss: 0.6666 - val_acc: 0.6592\n",
      "Epoch 507/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6871 - acc: 0.5351 - val_loss: 0.6665 - val_acc: 0.6592\n",
      "Epoch 508/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6863 - acc: 0.5548 - val_loss: 0.6665 - val_acc: 0.6592\n",
      "Epoch 509/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6823 - acc: 0.5927 - val_loss: 0.6665 - val_acc: 0.6592\n",
      "Epoch 510/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6781 - acc: 0.5772 - val_loss: 0.6665 - val_acc: 0.6592\n",
      "Epoch 511/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6806 - acc: 0.5618 - val_loss: 0.6665 - val_acc: 0.6592\n",
      "Epoch 512/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6876 - acc: 0.5197 - val_loss: 0.6665 - val_acc: 0.6592\n",
      "Epoch 513/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6915 - acc: 0.5112 - val_loss: 0.6664 - val_acc: 0.6592\n",
      "Epoch 514/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6884 - acc: 0.5169 - val_loss: 0.6664 - val_acc: 0.6592\n",
      "Epoch 515/1000\n",
      "712/712 [==============================] - 0s 163us/step - loss: 0.6869 - acc: 0.5281 - val_loss: 0.6664 - val_acc: 0.6592\n",
      "Epoch 516/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6863 - acc: 0.5534 - val_loss: 0.6664 - val_acc: 0.6592\n",
      "Epoch 517/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6815 - acc: 0.5787 - val_loss: 0.6664 - val_acc: 0.6592\n",
      "Epoch 518/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6854 - acc: 0.5463 - val_loss: 0.6664 - val_acc: 0.6592\n",
      "Epoch 519/1000\n",
      "712/712 [==============================] - 0s 120us/step - loss: 0.6845 - acc: 0.5632 - val_loss: 0.6663 - val_acc: 0.6592\n",
      "Epoch 520/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6874 - acc: 0.5393 - val_loss: 0.6663 - val_acc: 0.6592\n",
      "Epoch 521/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6850 - acc: 0.5744 - val_loss: 0.6663 - val_acc: 0.6592\n",
      "Epoch 522/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6863 - acc: 0.5463 - val_loss: 0.6663 - val_acc: 0.6592\n",
      "Epoch 523/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6872 - acc: 0.5534 - val_loss: 0.6663 - val_acc: 0.6592\n",
      "Epoch 524/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6790 - acc: 0.5660 - val_loss: 0.6663 - val_acc: 0.6592\n",
      "Epoch 525/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6847 - acc: 0.5365 - val_loss: 0.6662 - val_acc: 0.6592\n",
      "Epoch 526/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6826 - acc: 0.5449 - val_loss: 0.6662 - val_acc: 0.6592\n",
      "Epoch 527/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6861 - acc: 0.5506 - val_loss: 0.6662 - val_acc: 0.6592\n",
      "Epoch 528/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6933 - acc: 0.5520 - val_loss: 0.6662 - val_acc: 0.6592\n",
      "Epoch 529/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6877 - acc: 0.5351 - val_loss: 0.6662 - val_acc: 0.6592\n",
      "Epoch 530/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6845 - acc: 0.5562 - val_loss: 0.6662 - val_acc: 0.6592\n",
      "Epoch 531/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6867 - acc: 0.5337 - val_loss: 0.6661 - val_acc: 0.6592\n",
      "Epoch 532/1000\n",
      "712/712 [==============================] - 0s 157us/step - loss: 0.6923 - acc: 0.5239 - val_loss: 0.6661 - val_acc: 0.6592\n",
      "Epoch 533/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6846 - acc: 0.5548 - val_loss: 0.6661 - val_acc: 0.6592\n",
      "Epoch 534/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6839 - acc: 0.5506 - val_loss: 0.6661 - val_acc: 0.6592\n",
      "Epoch 535/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6906 - acc: 0.5042 - val_loss: 0.6661 - val_acc: 0.6592\n",
      "Epoch 536/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6940 - acc: 0.5154 - val_loss: 0.6661 - val_acc: 0.6592\n",
      "Epoch 537/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6807 - acc: 0.5871 - val_loss: 0.6661 - val_acc: 0.6592\n",
      "Epoch 538/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6838 - acc: 0.5492 - val_loss: 0.6660 - val_acc: 0.6592\n",
      "Epoch 539/1000\n",
      "712/712 [==============================] - 0s 120us/step - loss: 0.6815 - acc: 0.5618 - val_loss: 0.6660 - val_acc: 0.6592\n",
      "Epoch 540/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6784 - acc: 0.5646 - val_loss: 0.6660 - val_acc: 0.6592\n",
      "Epoch 541/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6915 - acc: 0.5421 - val_loss: 0.6660 - val_acc: 0.6592\n",
      "Epoch 542/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6863 - acc: 0.5478 - val_loss: 0.6660 - val_acc: 0.6592\n",
      "Epoch 543/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6878 - acc: 0.5267 - val_loss: 0.6660 - val_acc: 0.6592\n",
      "Epoch 544/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6772 - acc: 0.5772 - val_loss: 0.6659 - val_acc: 0.6592\n",
      "Epoch 545/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6848 - acc: 0.5548 - val_loss: 0.6659 - val_acc: 0.6592\n",
      "Epoch 546/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6857 - acc: 0.5435 - val_loss: 0.6659 - val_acc: 0.6592\n",
      "Epoch 547/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6851 - acc: 0.5435 - val_loss: 0.6659 - val_acc: 0.6592\n",
      "Epoch 548/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6835 - acc: 0.5478 - val_loss: 0.6659 - val_acc: 0.6592\n",
      "Epoch 549/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6816 - acc: 0.5604 - val_loss: 0.6659 - val_acc: 0.6592\n",
      "Epoch 550/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6832 - acc: 0.5815 - val_loss: 0.6658 - val_acc: 0.6592\n",
      "Epoch 551/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6875 - acc: 0.5463 - val_loss: 0.6658 - val_acc: 0.6592\n",
      "Epoch 552/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6783 - acc: 0.5660 - val_loss: 0.6658 - val_acc: 0.6592\n",
      "Epoch 553/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6928 - acc: 0.5211 - val_loss: 0.6658 - val_acc: 0.6592\n",
      "Epoch 554/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6879 - acc: 0.5393 - val_loss: 0.6658 - val_acc: 0.6592\n",
      "Epoch 555/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6845 - acc: 0.5590 - val_loss: 0.6658 - val_acc: 0.6592\n",
      "Epoch 556/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6820 - acc: 0.5365 - val_loss: 0.6657 - val_acc: 0.6592\n",
      "Epoch 557/1000\n",
      "712/712 [==============================] - 0s 100us/step - loss: 0.6801 - acc: 0.5463 - val_loss: 0.6657 - val_acc: 0.6592\n",
      "Epoch 558/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6872 - acc: 0.5365 - val_loss: 0.6657 - val_acc: 0.6592\n",
      "Epoch 559/1000\n",
      "712/712 [==============================] - 0s 104us/step - loss: 0.6831 - acc: 0.5590 - val_loss: 0.6657 - val_acc: 0.6592\n",
      "Epoch 560/1000\n",
      "712/712 [==============================] - 0s 99us/step - loss: 0.6852 - acc: 0.5393 - val_loss: 0.6657 - val_acc: 0.6592\n",
      "Epoch 561/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6844 - acc: 0.5365 - val_loss: 0.6657 - val_acc: 0.6592\n",
      "Epoch 562/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6861 - acc: 0.5632 - val_loss: 0.6657 - val_acc: 0.6592\n",
      "Epoch 563/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6772 - acc: 0.5955 - val_loss: 0.6656 - val_acc: 0.6592\n",
      "Epoch 564/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6843 - acc: 0.5590 - val_loss: 0.6656 - val_acc: 0.6592\n",
      "Epoch 565/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6825 - acc: 0.5534 - val_loss: 0.6656 - val_acc: 0.6592\n",
      "Epoch 566/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6789 - acc: 0.5632 - val_loss: 0.6656 - val_acc: 0.6592\n",
      "Epoch 567/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6870 - acc: 0.5534 - val_loss: 0.6656 - val_acc: 0.6592\n",
      "Epoch 568/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6901 - acc: 0.5548 - val_loss: 0.6656 - val_acc: 0.6592\n",
      "Epoch 569/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6837 - acc: 0.5660 - val_loss: 0.6655 - val_acc: 0.6592\n",
      "Epoch 570/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6840 - acc: 0.5548 - val_loss: 0.6655 - val_acc: 0.6592\n",
      "Epoch 571/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6777 - acc: 0.5801 - val_loss: 0.6655 - val_acc: 0.6592\n",
      "Epoch 572/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6959 - acc: 0.5042 - val_loss: 0.6655 - val_acc: 0.6592\n",
      "Epoch 573/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6791 - acc: 0.5618 - val_loss: 0.6655 - val_acc: 0.6592\n",
      "Epoch 574/1000\n",
      "712/712 [==============================] - 0s 161us/step - loss: 0.6830 - acc: 0.5492 - val_loss: 0.6655 - val_acc: 0.6592\n",
      "Epoch 575/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6867 - acc: 0.5309 - val_loss: 0.6654 - val_acc: 0.6592\n",
      "Epoch 576/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6836 - acc: 0.5548 - val_loss: 0.6654 - val_acc: 0.6592\n",
      "Epoch 577/1000\n",
      "712/712 [==============================] - 0s 166us/step - loss: 0.6899 - acc: 0.5365 - val_loss: 0.6654 - val_acc: 0.6592\n",
      "Epoch 578/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6840 - acc: 0.5604 - val_loss: 0.6654 - val_acc: 0.6592\n",
      "Epoch 579/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6859 - acc: 0.5365 - val_loss: 0.6654 - val_acc: 0.6592\n",
      "Epoch 580/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6847 - acc: 0.5548 - val_loss: 0.6654 - val_acc: 0.6592\n",
      "Epoch 581/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6922 - acc: 0.5169 - val_loss: 0.6654 - val_acc: 0.6592\n",
      "Epoch 582/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6868 - acc: 0.5393 - val_loss: 0.6653 - val_acc: 0.6592\n",
      "Epoch 583/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6873 - acc: 0.5253 - val_loss: 0.6653 - val_acc: 0.6592\n",
      "Epoch 584/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6904 - acc: 0.5323 - val_loss: 0.6653 - val_acc: 0.6592\n",
      "Epoch 585/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6893 - acc: 0.5604 - val_loss: 0.6653 - val_acc: 0.6592\n",
      "Epoch 586/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6817 - acc: 0.5843 - val_loss: 0.6653 - val_acc: 0.6592\n",
      "Epoch 587/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6809 - acc: 0.5674 - val_loss: 0.6653 - val_acc: 0.6592\n",
      "Epoch 588/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6784 - acc: 0.5632 - val_loss: 0.6652 - val_acc: 0.6592\n",
      "Epoch 589/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6868 - acc: 0.5126 - val_loss: 0.6652 - val_acc: 0.6592\n",
      "Epoch 590/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6823 - acc: 0.5576 - val_loss: 0.6652 - val_acc: 0.6592\n",
      "Epoch 591/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6830 - acc: 0.5548 - val_loss: 0.6652 - val_acc: 0.6592\n",
      "Epoch 592/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 146us/step - loss: 0.6797 - acc: 0.5758 - val_loss: 0.6652 - val_acc: 0.6592\n",
      "Epoch 593/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6801 - acc: 0.5618 - val_loss: 0.6652 - val_acc: 0.6592\n",
      "Epoch 594/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6843 - acc: 0.5562 - val_loss: 0.6651 - val_acc: 0.6592\n",
      "Epoch 595/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6913 - acc: 0.5253 - val_loss: 0.6651 - val_acc: 0.6592\n",
      "Epoch 596/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6854 - acc: 0.5407 - val_loss: 0.6651 - val_acc: 0.6592\n",
      "Epoch 597/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6757 - acc: 0.5548 - val_loss: 0.6651 - val_acc: 0.6592\n",
      "Epoch 598/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6788 - acc: 0.5520 - val_loss: 0.6651 - val_acc: 0.6592\n",
      "Epoch 599/1000\n",
      "712/712 [==============================] - 0s 120us/step - loss: 0.6858 - acc: 0.5449 - val_loss: 0.6651 - val_acc: 0.6592\n",
      "Epoch 600/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6813 - acc: 0.5520 - val_loss: 0.6651 - val_acc: 0.6592\n",
      "Epoch 601/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6809 - acc: 0.5576 - val_loss: 0.6650 - val_acc: 0.6592\n",
      "Epoch 602/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6903 - acc: 0.5351 - val_loss: 0.6650 - val_acc: 0.6592\n",
      "Epoch 603/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6841 - acc: 0.5688 - val_loss: 0.6650 - val_acc: 0.6592\n",
      "Epoch 604/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6801 - acc: 0.5379 - val_loss: 0.6650 - val_acc: 0.6592\n",
      "Epoch 605/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6882 - acc: 0.5407 - val_loss: 0.6650 - val_acc: 0.6592\n",
      "Epoch 606/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6868 - acc: 0.5281 - val_loss: 0.6650 - val_acc: 0.6592\n",
      "Epoch 607/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6827 - acc: 0.5435 - val_loss: 0.6650 - val_acc: 0.6592\n",
      "Epoch 608/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6853 - acc: 0.5463 - val_loss: 0.6649 - val_acc: 0.6592\n",
      "Epoch 609/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6796 - acc: 0.5815 - val_loss: 0.6649 - val_acc: 0.6592\n",
      "Epoch 610/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6876 - acc: 0.5421 - val_loss: 0.6649 - val_acc: 0.6592\n",
      "Epoch 611/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6832 - acc: 0.5337 - val_loss: 0.6649 - val_acc: 0.6592\n",
      "Epoch 612/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6871 - acc: 0.5632 - val_loss: 0.6649 - val_acc: 0.6592\n",
      "Epoch 613/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6846 - acc: 0.5646 - val_loss: 0.6649 - val_acc: 0.6592\n",
      "Epoch 614/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6867 - acc: 0.5421 - val_loss: 0.6648 - val_acc: 0.6592\n",
      "Epoch 615/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6889 - acc: 0.5618 - val_loss: 0.6648 - val_acc: 0.6592\n",
      "Epoch 616/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6849 - acc: 0.5534 - val_loss: 0.6648 - val_acc: 0.6592\n",
      "Epoch 617/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6906 - acc: 0.5365 - val_loss: 0.6648 - val_acc: 0.6592\n",
      "Epoch 618/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6864 - acc: 0.5365 - val_loss: 0.6648 - val_acc: 0.6592\n",
      "Epoch 619/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6772 - acc: 0.5730 - val_loss: 0.6648 - val_acc: 0.6592\n",
      "Epoch 620/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6813 - acc: 0.5590 - val_loss: 0.6648 - val_acc: 0.6592\n",
      "Epoch 621/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6840 - acc: 0.5660 - val_loss: 0.6647 - val_acc: 0.6536\n",
      "Epoch 622/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6908 - acc: 0.5478 - val_loss: 0.6647 - val_acc: 0.6536\n",
      "Epoch 623/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6819 - acc: 0.5688 - val_loss: 0.6647 - val_acc: 0.6536\n",
      "Epoch 624/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6813 - acc: 0.5618 - val_loss: 0.6647 - val_acc: 0.6536\n",
      "Epoch 625/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6903 - acc: 0.5211 - val_loss: 0.6647 - val_acc: 0.6536\n",
      "Epoch 626/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6855 - acc: 0.5393 - val_loss: 0.6647 - val_acc: 0.6536\n",
      "Epoch 627/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6804 - acc: 0.5590 - val_loss: 0.6646 - val_acc: 0.6536\n",
      "Epoch 628/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6890 - acc: 0.5323 - val_loss: 0.6646 - val_acc: 0.6536\n",
      "Epoch 629/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6818 - acc: 0.5674 - val_loss: 0.6646 - val_acc: 0.6536\n",
      "Epoch 630/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6815 - acc: 0.5646 - val_loss: 0.6646 - val_acc: 0.6536\n",
      "Epoch 631/1000\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.6773 - acc: 0.5688 - val_loss: 0.6646 - val_acc: 0.6536\n",
      "Epoch 632/1000\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.6771 - acc: 0.5787 - val_loss: 0.6646 - val_acc: 0.6536\n",
      "Epoch 633/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6839 - acc: 0.5309 - val_loss: 0.6645 - val_acc: 0.6536\n",
      "Epoch 634/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6898 - acc: 0.5534 - val_loss: 0.6645 - val_acc: 0.6536\n",
      "Epoch 635/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6937 - acc: 0.5351 - val_loss: 0.6645 - val_acc: 0.6536\n",
      "Epoch 636/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6815 - acc: 0.5576 - val_loss: 0.6645 - val_acc: 0.6536\n",
      "Epoch 637/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6854 - acc: 0.5520 - val_loss: 0.6645 - val_acc: 0.6536\n",
      "Epoch 638/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6853 - acc: 0.5421 - val_loss: 0.6645 - val_acc: 0.6536\n",
      "Epoch 639/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6806 - acc: 0.5632 - val_loss: 0.6645 - val_acc: 0.6536\n",
      "Epoch 640/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6864 - acc: 0.5492 - val_loss: 0.6644 - val_acc: 0.6536\n",
      "Epoch 641/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6924 - acc: 0.5154 - val_loss: 0.6644 - val_acc: 0.6536\n",
      "Epoch 642/1000\n",
      "712/712 [==============================] - 0s 109us/step - loss: 0.6887 - acc: 0.5506 - val_loss: 0.6644 - val_acc: 0.6536\n",
      "Epoch 643/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6917 - acc: 0.5421 - val_loss: 0.6644 - val_acc: 0.6536\n",
      "Epoch 644/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6817 - acc: 0.5632 - val_loss: 0.6644 - val_acc: 0.6536\n",
      "Epoch 645/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6790 - acc: 0.5899 - val_loss: 0.6644 - val_acc: 0.6536\n",
      "Epoch 646/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6845 - acc: 0.5449 - val_loss: 0.6643 - val_acc: 0.6536\n",
      "Epoch 647/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6868 - acc: 0.5646 - val_loss: 0.6643 - val_acc: 0.6536\n",
      "Epoch 648/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6961 - acc: 0.5126 - val_loss: 0.6643 - val_acc: 0.6536\n",
      "Epoch 649/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6825 - acc: 0.5548 - val_loss: 0.6643 - val_acc: 0.6536\n",
      "Epoch 650/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6871 - acc: 0.5478 - val_loss: 0.6643 - val_acc: 0.6536\n",
      "Epoch 651/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6824 - acc: 0.5506 - val_loss: 0.6643 - val_acc: 0.6536\n",
      "Epoch 652/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6941 - acc: 0.5351 - val_loss: 0.6642 - val_acc: 0.6536\n",
      "Epoch 653/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6835 - acc: 0.5744 - val_loss: 0.6642 - val_acc: 0.6536\n",
      "Epoch 654/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6875 - acc: 0.5253 - val_loss: 0.6642 - val_acc: 0.6536\n",
      "Epoch 655/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6752 - acc: 0.5969 - val_loss: 0.6642 - val_acc: 0.6536\n",
      "Epoch 656/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6898 - acc: 0.5520 - val_loss: 0.6642 - val_acc: 0.6536\n",
      "Epoch 657/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6904 - acc: 0.5337 - val_loss: 0.6642 - val_acc: 0.6536\n",
      "Epoch 658/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6794 - acc: 0.5520 - val_loss: 0.6642 - val_acc: 0.6536\n",
      "Epoch 659/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6839 - acc: 0.5534 - val_loss: 0.6641 - val_acc: 0.6536\n",
      "Epoch 660/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6823 - acc: 0.5548 - val_loss: 0.6641 - val_acc: 0.6536\n",
      "Epoch 661/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6786 - acc: 0.5688 - val_loss: 0.6641 - val_acc: 0.6536\n",
      "Epoch 662/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6889 - acc: 0.5309 - val_loss: 0.6641 - val_acc: 0.6536\n",
      "Epoch 663/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6817 - acc: 0.5618 - val_loss: 0.6641 - val_acc: 0.6536\n",
      "Epoch 664/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6942 - acc: 0.5225 - val_loss: 0.6641 - val_acc: 0.6536\n",
      "Epoch 665/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6871 - acc: 0.5815 - val_loss: 0.6640 - val_acc: 0.6536\n",
      "Epoch 666/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6782 - acc: 0.5927 - val_loss: 0.6640 - val_acc: 0.6536\n",
      "Epoch 667/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6871 - acc: 0.5520 - val_loss: 0.6640 - val_acc: 0.6536\n",
      "Epoch 668/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6835 - acc: 0.5576 - val_loss: 0.6640 - val_acc: 0.6536\n",
      "Epoch 669/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6827 - acc: 0.5435 - val_loss: 0.6640 - val_acc: 0.6536\n",
      "Epoch 670/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6906 - acc: 0.5267 - val_loss: 0.6640 - val_acc: 0.6536\n",
      "Epoch 671/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6834 - acc: 0.5506 - val_loss: 0.6640 - val_acc: 0.6536\n",
      "Epoch 672/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6870 - acc: 0.5393 - val_loss: 0.6639 - val_acc: 0.6536\n",
      "Epoch 673/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6840 - acc: 0.5506 - val_loss: 0.6639 - val_acc: 0.6536\n",
      "Epoch 674/1000\n",
      "712/712 [==============================] - 0s 98us/step - loss: 0.6805 - acc: 0.5716 - val_loss: 0.6639 - val_acc: 0.6536\n",
      "Epoch 675/1000\n",
      "712/712 [==============================] - 0s 108us/step - loss: 0.6909 - acc: 0.5211 - val_loss: 0.6639 - val_acc: 0.6536\n",
      "Epoch 676/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6830 - acc: 0.5646 - val_loss: 0.6639 - val_acc: 0.6536\n",
      "Epoch 677/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6853 - acc: 0.5548 - val_loss: 0.6639 - val_acc: 0.6536\n",
      "Epoch 678/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6799 - acc: 0.5744 - val_loss: 0.6639 - val_acc: 0.6536\n",
      "Epoch 679/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6808 - acc: 0.5632 - val_loss: 0.6638 - val_acc: 0.6536\n",
      "Epoch 680/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6796 - acc: 0.5506 - val_loss: 0.6638 - val_acc: 0.6536\n",
      "Epoch 681/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6792 - acc: 0.5857 - val_loss: 0.6638 - val_acc: 0.6536\n",
      "Epoch 682/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6809 - acc: 0.5520 - val_loss: 0.6638 - val_acc: 0.6536\n",
      "Epoch 683/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6896 - acc: 0.5309 - val_loss: 0.6638 - val_acc: 0.6536\n",
      "Epoch 684/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6775 - acc: 0.5646 - val_loss: 0.6638 - val_acc: 0.6536\n",
      "Epoch 685/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6772 - acc: 0.5815 - val_loss: 0.6637 - val_acc: 0.6536\n",
      "Epoch 686/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6824 - acc: 0.5548 - val_loss: 0.6637 - val_acc: 0.6536\n",
      "Epoch 687/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6846 - acc: 0.5576 - val_loss: 0.6637 - val_acc: 0.6536\n",
      "Epoch 688/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6778 - acc: 0.5787 - val_loss: 0.6637 - val_acc: 0.6536\n",
      "Epoch 689/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6762 - acc: 0.5688 - val_loss: 0.6637 - val_acc: 0.6536\n",
      "Epoch 690/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6799 - acc: 0.5744 - val_loss: 0.6637 - val_acc: 0.6536\n",
      "Epoch 691/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6885 - acc: 0.5632 - val_loss: 0.6637 - val_acc: 0.6536\n",
      "Epoch 692/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6863 - acc: 0.5365 - val_loss: 0.6636 - val_acc: 0.6536\n",
      "Epoch 693/1000\n",
      "712/712 [==============================] - 0s 146us/step - loss: 0.6804 - acc: 0.5632 - val_loss: 0.6636 - val_acc: 0.6536\n",
      "Epoch 694/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6864 - acc: 0.5520 - val_loss: 0.6636 - val_acc: 0.6536\n",
      "Epoch 695/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6844 - acc: 0.5660 - val_loss: 0.6636 - val_acc: 0.6536\n",
      "Epoch 696/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6790 - acc: 0.5829 - val_loss: 0.6636 - val_acc: 0.6536\n",
      "Epoch 697/1000\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.6874 - acc: 0.5576 - val_loss: 0.6636 - val_acc: 0.6536\n",
      "Epoch 698/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6809 - acc: 0.5660 - val_loss: 0.6635 - val_acc: 0.6536\n",
      "Epoch 699/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6728 - acc: 0.5955 - val_loss: 0.6635 - val_acc: 0.6536\n",
      "Epoch 700/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6870 - acc: 0.5702 - val_loss: 0.6635 - val_acc: 0.6536\n",
      "Epoch 701/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6831 - acc: 0.5492 - val_loss: 0.6635 - val_acc: 0.6536\n",
      "Epoch 702/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6863 - acc: 0.5618 - val_loss: 0.6635 - val_acc: 0.6536\n",
      "Epoch 703/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6784 - acc: 0.5646 - val_loss: 0.6635 - val_acc: 0.6536\n",
      "Epoch 704/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6824 - acc: 0.5534 - val_loss: 0.6635 - val_acc: 0.6536\n",
      "Epoch 705/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6820 - acc: 0.5562 - val_loss: 0.6634 - val_acc: 0.6536\n",
      "Epoch 706/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6878 - acc: 0.5492 - val_loss: 0.6634 - val_acc: 0.6536\n",
      "Epoch 707/1000\n",
      "712/712 [==============================] - 0s 103us/step - loss: 0.6732 - acc: 0.5955 - val_loss: 0.6634 - val_acc: 0.6536\n",
      "Epoch 708/1000\n",
      "712/712 [==============================] - 0s 118us/step - loss: 0.6847 - acc: 0.5772 - val_loss: 0.6634 - val_acc: 0.6536\n",
      "Epoch 709/1000\n",
      "712/712 [==============================] - 0s 124us/step - loss: 0.6880 - acc: 0.5323 - val_loss: 0.6634 - val_acc: 0.6536\n",
      "Epoch 710/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 109us/step - loss: 0.6789 - acc: 0.5618 - val_loss: 0.6634 - val_acc: 0.6536\n",
      "Epoch 711/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6825 - acc: 0.5295 - val_loss: 0.6634 - val_acc: 0.6536\n",
      "Epoch 712/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6792 - acc: 0.5520 - val_loss: 0.6633 - val_acc: 0.6536\n",
      "Epoch 713/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6854 - acc: 0.5744 - val_loss: 0.6633 - val_acc: 0.6536\n",
      "Epoch 714/1000\n",
      "712/712 [==============================] - 0s 104us/step - loss: 0.6877 - acc: 0.5562 - val_loss: 0.6633 - val_acc: 0.6536\n",
      "Epoch 715/1000\n",
      "712/712 [==============================] - 0s 100us/step - loss: 0.6827 - acc: 0.5534 - val_loss: 0.6633 - val_acc: 0.6536\n",
      "Epoch 716/1000\n",
      "712/712 [==============================] - 0s 106us/step - loss: 0.6863 - acc: 0.5323 - val_loss: 0.6633 - val_acc: 0.6536\n",
      "Epoch 717/1000\n",
      "712/712 [==============================] - 0s 104us/step - loss: 0.6850 - acc: 0.5562 - val_loss: 0.6633 - val_acc: 0.6536\n",
      "Epoch 718/1000\n",
      "712/712 [==============================] - 0s 102us/step - loss: 0.6827 - acc: 0.5632 - val_loss: 0.6632 - val_acc: 0.6536\n",
      "Epoch 719/1000\n",
      "712/712 [==============================] - 0s 98us/step - loss: 0.6815 - acc: 0.5618 - val_loss: 0.6632 - val_acc: 0.6536\n",
      "Epoch 720/1000\n",
      "712/712 [==============================] - 0s 97us/step - loss: 0.6846 - acc: 0.5520 - val_loss: 0.6632 - val_acc: 0.6536\n",
      "Epoch 721/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6803 - acc: 0.5646 - val_loss: 0.6632 - val_acc: 0.6536\n",
      "Epoch 722/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6843 - acc: 0.5604 - val_loss: 0.6632 - val_acc: 0.6536\n",
      "Epoch 723/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6853 - acc: 0.5534 - val_loss: 0.6632 - val_acc: 0.6536\n",
      "Epoch 724/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6875 - acc: 0.5323 - val_loss: 0.6632 - val_acc: 0.6536\n",
      "Epoch 725/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6764 - acc: 0.5716 - val_loss: 0.6631 - val_acc: 0.6536\n",
      "Epoch 726/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6831 - acc: 0.5506 - val_loss: 0.6631 - val_acc: 0.6536\n",
      "Epoch 727/1000\n",
      "712/712 [==============================] - 0s 159us/step - loss: 0.6915 - acc: 0.5225 - val_loss: 0.6631 - val_acc: 0.6536\n",
      "Epoch 728/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6773 - acc: 0.5772 - val_loss: 0.6631 - val_acc: 0.6536\n",
      "Epoch 729/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6795 - acc: 0.5520 - val_loss: 0.6631 - val_acc: 0.6536\n",
      "Epoch 730/1000\n",
      "712/712 [==============================] - 0s 101us/step - loss: 0.6796 - acc: 0.5590 - val_loss: 0.6631 - val_acc: 0.6536\n",
      "Epoch 731/1000\n",
      "712/712 [==============================] - 0s 122us/step - loss: 0.6873 - acc: 0.5379 - val_loss: 0.6631 - val_acc: 0.6536\n",
      "Epoch 732/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6817 - acc: 0.5520 - val_loss: 0.6630 - val_acc: 0.6536\n",
      "Epoch 733/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6822 - acc: 0.5463 - val_loss: 0.6630 - val_acc: 0.6536\n",
      "Epoch 734/1000\n",
      "712/712 [==============================] - 0s 168us/step - loss: 0.6776 - acc: 0.5815 - val_loss: 0.6630 - val_acc: 0.6536\n",
      "Epoch 735/1000\n",
      "712/712 [==============================] - 0s 174us/step - loss: 0.6765 - acc: 0.5857 - val_loss: 0.6630 - val_acc: 0.6536\n",
      "Epoch 736/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6799 - acc: 0.5632 - val_loss: 0.6630 - val_acc: 0.6536\n",
      "Epoch 737/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6761 - acc: 0.5857 - val_loss: 0.6630 - val_acc: 0.6536\n",
      "Epoch 738/1000\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.6762 - acc: 0.5660 - val_loss: 0.6629 - val_acc: 0.6536\n",
      "Epoch 739/1000\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.6770 - acc: 0.5492 - val_loss: 0.6629 - val_acc: 0.6536\n",
      "Epoch 740/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6824 - acc: 0.5463 - val_loss: 0.6629 - val_acc: 0.6536\n",
      "Epoch 741/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6779 - acc: 0.5801 - val_loss: 0.6629 - val_acc: 0.6536\n",
      "Epoch 742/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6878 - acc: 0.5520 - val_loss: 0.6629 - val_acc: 0.6536\n",
      "Epoch 743/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6836 - acc: 0.5520 - val_loss: 0.6629 - val_acc: 0.6536\n",
      "Epoch 744/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6879 - acc: 0.5534 - val_loss: 0.6629 - val_acc: 0.6536\n",
      "Epoch 745/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6823 - acc: 0.5590 - val_loss: 0.6628 - val_acc: 0.6536\n",
      "Epoch 746/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6780 - acc: 0.5913 - val_loss: 0.6628 - val_acc: 0.6536\n",
      "Epoch 747/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6774 - acc: 0.5787 - val_loss: 0.6628 - val_acc: 0.6536\n",
      "Epoch 748/1000\n",
      "712/712 [==============================] - 0s 148us/step - loss: 0.6924 - acc: 0.5253 - val_loss: 0.6628 - val_acc: 0.6536\n",
      "Epoch 749/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6782 - acc: 0.5801 - val_loss: 0.6628 - val_acc: 0.6536\n",
      "Epoch 750/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6860 - acc: 0.5421 - val_loss: 0.6628 - val_acc: 0.6536\n",
      "Epoch 751/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6818 - acc: 0.5393 - val_loss: 0.6627 - val_acc: 0.6536\n",
      "Epoch 752/1000\n",
      "712/712 [==============================] - 0s 140us/step - loss: 0.6816 - acc: 0.5492 - val_loss: 0.6627 - val_acc: 0.6536\n",
      "Epoch 753/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6822 - acc: 0.5857 - val_loss: 0.6627 - val_acc: 0.6536\n",
      "Epoch 754/1000\n",
      "712/712 [==============================] - 0s 153us/step - loss: 0.6825 - acc: 0.5590 - val_loss: 0.6627 - val_acc: 0.6536\n",
      "Epoch 755/1000\n",
      "712/712 [==============================] - 0s 162us/step - loss: 0.6801 - acc: 0.5646 - val_loss: 0.6627 - val_acc: 0.6536\n",
      "Epoch 756/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6806 - acc: 0.5660 - val_loss: 0.6627 - val_acc: 0.6536\n",
      "Epoch 757/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6820 - acc: 0.5688 - val_loss: 0.6627 - val_acc: 0.6536\n",
      "Epoch 758/1000\n",
      "712/712 [==============================] - 0s 91us/step - loss: 0.6844 - acc: 0.5449 - val_loss: 0.6626 - val_acc: 0.6536\n",
      "Epoch 759/1000\n",
      "712/712 [==============================] - 0s 76us/step - loss: 0.6905 - acc: 0.5225 - val_loss: 0.6626 - val_acc: 0.6536\n",
      "Epoch 760/1000\n",
      "712/712 [==============================] - 0s 92us/step - loss: 0.6903 - acc: 0.5379 - val_loss: 0.6626 - val_acc: 0.6536\n",
      "Epoch 761/1000\n",
      "712/712 [==============================] - 0s 76us/step - loss: 0.6907 - acc: 0.5309 - val_loss: 0.6626 - val_acc: 0.6536\n",
      "Epoch 762/1000\n",
      "712/712 [==============================] - 0s 75us/step - loss: 0.6831 - acc: 0.5702 - val_loss: 0.6626 - val_acc: 0.6536\n",
      "Epoch 763/1000\n",
      "712/712 [==============================] - 0s 75us/step - loss: 0.6733 - acc: 0.5871 - val_loss: 0.6626 - val_acc: 0.6536\n",
      "Epoch 764/1000\n",
      "712/712 [==============================] - 0s 76us/step - loss: 0.6862 - acc: 0.5646 - val_loss: 0.6625 - val_acc: 0.6536\n",
      "Epoch 765/1000\n",
      "712/712 [==============================] - 0s 77us/step - loss: 0.6776 - acc: 0.5674 - val_loss: 0.6625 - val_acc: 0.6536\n",
      "Epoch 766/1000\n",
      "712/712 [==============================] - 0s 76us/step - loss: 0.6900 - acc: 0.5534 - val_loss: 0.6625 - val_acc: 0.6536\n",
      "Epoch 767/1000\n",
      "712/712 [==============================] - 0s 76us/step - loss: 0.6846 - acc: 0.5435 - val_loss: 0.6625 - val_acc: 0.6536\n",
      "Epoch 768/1000\n",
      "712/712 [==============================] - 0s 96us/step - loss: 0.6811 - acc: 0.5674 - val_loss: 0.6625 - val_acc: 0.6536\n",
      "Epoch 769/1000\n",
      "712/712 [==============================] - 0s 78us/step - loss: 0.6788 - acc: 0.5674 - val_loss: 0.6625 - val_acc: 0.6536\n",
      "Epoch 770/1000\n",
      "712/712 [==============================] - 0s 76us/step - loss: 0.6841 - acc: 0.5534 - val_loss: 0.6625 - val_acc: 0.6536\n",
      "Epoch 771/1000\n",
      "712/712 [==============================] - 0s 75us/step - loss: 0.6767 - acc: 0.5758 - val_loss: 0.6624 - val_acc: 0.6536\n",
      "Epoch 772/1000\n",
      "712/712 [==============================] - 0s 75us/step - loss: 0.6794 - acc: 0.5646 - val_loss: 0.6624 - val_acc: 0.6536\n",
      "Epoch 773/1000\n",
      "712/712 [==============================] - 0s 75us/step - loss: 0.6800 - acc: 0.5829 - val_loss: 0.6624 - val_acc: 0.6536\n",
      "Epoch 774/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6736 - acc: 0.5829 - val_loss: 0.6624 - val_acc: 0.6536\n",
      "Epoch 775/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6820 - acc: 0.5688 - val_loss: 0.6624 - val_acc: 0.6536\n",
      "Epoch 776/1000\n",
      "712/712 [==============================] - 0s 98us/step - loss: 0.6728 - acc: 0.6039 - val_loss: 0.6624 - val_acc: 0.6536\n",
      "Epoch 777/1000\n",
      "712/712 [==============================] - 0s 97us/step - loss: 0.6858 - acc: 0.5407 - val_loss: 0.6623 - val_acc: 0.6536\n",
      "Epoch 778/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6809 - acc: 0.5337 - val_loss: 0.6623 - val_acc: 0.6536\n",
      "Epoch 779/1000\n",
      "712/712 [==============================] - 0s 97us/step - loss: 0.6868 - acc: 0.5478 - val_loss: 0.6623 - val_acc: 0.6536\n",
      "Epoch 780/1000\n",
      "712/712 [==============================] - 0s 103us/step - loss: 0.6904 - acc: 0.5211 - val_loss: 0.6623 - val_acc: 0.6536\n",
      "Epoch 781/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6908 - acc: 0.5435 - val_loss: 0.6623 - val_acc: 0.6536\n",
      "Epoch 782/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6821 - acc: 0.5590 - val_loss: 0.6623 - val_acc: 0.6536\n",
      "Epoch 783/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6810 - acc: 0.5548 - val_loss: 0.6623 - val_acc: 0.6536\n",
      "Epoch 784/1000\n",
      "712/712 [==============================] - 0s 106us/step - loss: 0.6803 - acc: 0.5702 - val_loss: 0.6622 - val_acc: 0.6536\n",
      "Epoch 785/1000\n",
      "712/712 [==============================] - 0s 102us/step - loss: 0.6877 - acc: 0.5506 - val_loss: 0.6622 - val_acc: 0.6536\n",
      "Epoch 786/1000\n",
      "712/712 [==============================] - 0s 103us/step - loss: 0.6777 - acc: 0.5857 - val_loss: 0.6622 - val_acc: 0.6536\n",
      "Epoch 787/1000\n",
      "712/712 [==============================] - 0s 106us/step - loss: 0.6851 - acc: 0.5393 - val_loss: 0.6622 - val_acc: 0.6536\n",
      "Epoch 788/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6815 - acc: 0.5716 - val_loss: 0.6622 - val_acc: 0.6536\n",
      "Epoch 789/1000\n",
      "712/712 [==============================] - 0s 109us/step - loss: 0.6895 - acc: 0.5295 - val_loss: 0.6622 - val_acc: 0.6536\n",
      "Epoch 790/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6840 - acc: 0.5730 - val_loss: 0.6622 - val_acc: 0.6536\n",
      "Epoch 791/1000\n",
      "712/712 [==============================] - 0s 120us/step - loss: 0.6862 - acc: 0.5295 - val_loss: 0.6621 - val_acc: 0.6536\n",
      "Epoch 792/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6817 - acc: 0.5674 - val_loss: 0.6621 - val_acc: 0.6536\n",
      "Epoch 793/1000\n",
      "712/712 [==============================] - 0s 112us/step - loss: 0.6765 - acc: 0.5787 - val_loss: 0.6621 - val_acc: 0.6536\n",
      "Epoch 794/1000\n",
      "712/712 [==============================] - 0s 114us/step - loss: 0.6828 - acc: 0.5351 - val_loss: 0.6621 - val_acc: 0.6536\n",
      "Epoch 795/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6840 - acc: 0.5506 - val_loss: 0.6621 - val_acc: 0.6536\n",
      "Epoch 796/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6741 - acc: 0.5829 - val_loss: 0.6621 - val_acc: 0.6536\n",
      "Epoch 797/1000\n",
      "712/712 [==============================] - 0s 120us/step - loss: 0.6728 - acc: 0.5758 - val_loss: 0.6620 - val_acc: 0.6536\n",
      "Epoch 798/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6815 - acc: 0.5772 - val_loss: 0.6620 - val_acc: 0.6536\n",
      "Epoch 799/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6828 - acc: 0.5674 - val_loss: 0.6620 - val_acc: 0.6536\n",
      "Epoch 800/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6771 - acc: 0.5730 - val_loss: 0.6620 - val_acc: 0.6536\n",
      "Epoch 801/1000\n",
      "712/712 [==============================] - 0s 121us/step - loss: 0.6855 - acc: 0.5576 - val_loss: 0.6620 - val_acc: 0.6536\n",
      "Epoch 802/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6894 - acc: 0.5211 - val_loss: 0.6620 - val_acc: 0.6536\n",
      "Epoch 803/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6853 - acc: 0.5646 - val_loss: 0.6620 - val_acc: 0.6536\n",
      "Epoch 804/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6856 - acc: 0.5478 - val_loss: 0.6619 - val_acc: 0.6536\n",
      "Epoch 805/1000\n",
      "712/712 [==============================] - 0s 136us/step - loss: 0.6751 - acc: 0.5871 - val_loss: 0.6619 - val_acc: 0.6536\n",
      "Epoch 806/1000\n",
      "712/712 [==============================] - 0s 138us/step - loss: 0.6814 - acc: 0.5730 - val_loss: 0.6619 - val_acc: 0.6536\n",
      "Epoch 807/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6784 - acc: 0.5548 - val_loss: 0.6619 - val_acc: 0.6536\n",
      "Epoch 808/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6789 - acc: 0.5913 - val_loss: 0.6619 - val_acc: 0.6536\n",
      "Epoch 809/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6805 - acc: 0.5421 - val_loss: 0.6619 - val_acc: 0.6536\n",
      "Epoch 810/1000\n",
      "712/712 [==============================] - 0s 149us/step - loss: 0.6828 - acc: 0.5618 - val_loss: 0.6619 - val_acc: 0.6536\n",
      "Epoch 811/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6858 - acc: 0.5449 - val_loss: 0.6618 - val_acc: 0.6536\n",
      "Epoch 812/1000\n",
      "712/712 [==============================] - 0s 160us/step - loss: 0.6819 - acc: 0.5646 - val_loss: 0.6618 - val_acc: 0.6536\n",
      "Epoch 813/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6821 - acc: 0.5337 - val_loss: 0.6618 - val_acc: 0.6536\n",
      "Epoch 814/1000\n",
      "712/712 [==============================] - 0s 156us/step - loss: 0.6764 - acc: 0.5829 - val_loss: 0.6618 - val_acc: 0.6536\n",
      "Epoch 815/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6835 - acc: 0.5801 - val_loss: 0.6618 - val_acc: 0.6536\n",
      "Epoch 816/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6795 - acc: 0.5632 - val_loss: 0.6618 - val_acc: 0.6536\n",
      "Epoch 817/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6896 - acc: 0.5154 - val_loss: 0.6617 - val_acc: 0.6536\n",
      "Epoch 818/1000\n",
      "712/712 [==============================] - 0s 147us/step - loss: 0.6833 - acc: 0.5379 - val_loss: 0.6617 - val_acc: 0.6536\n",
      "Epoch 819/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6830 - acc: 0.5253 - val_loss: 0.6617 - val_acc: 0.6536\n",
      "Epoch 820/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6827 - acc: 0.5478 - val_loss: 0.6617 - val_acc: 0.6536\n",
      "Epoch 821/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6803 - acc: 0.5730 - val_loss: 0.6617 - val_acc: 0.6536\n",
      "Epoch 822/1000\n",
      "712/712 [==============================] - 0s 151us/step - loss: 0.6915 - acc: 0.5323 - val_loss: 0.6617 - val_acc: 0.6536\n",
      "Epoch 823/1000\n",
      "712/712 [==============================] - 0s 145us/step - loss: 0.6832 - acc: 0.5534 - val_loss: 0.6617 - val_acc: 0.6536\n",
      "Epoch 824/1000\n",
      "712/712 [==============================] - 0s 154us/step - loss: 0.6760 - acc: 0.5702 - val_loss: 0.6616 - val_acc: 0.6536\n",
      "Epoch 825/1000\n",
      "712/712 [==============================] - 0s 155us/step - loss: 0.6768 - acc: 0.5815 - val_loss: 0.6616 - val_acc: 0.6536\n",
      "Epoch 826/1000\n",
      "712/712 [==============================] - 0s 152us/step - loss: 0.6841 - acc: 0.5506 - val_loss: 0.6616 - val_acc: 0.6536\n",
      "Epoch 827/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6828 - acc: 0.5435 - val_loss: 0.6616 - val_acc: 0.6536\n",
      "Epoch 828/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712/712 [==============================] - 0s 146us/step - loss: 0.6783 - acc: 0.5772 - val_loss: 0.6616 - val_acc: 0.6536\n",
      "Epoch 829/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6827 - acc: 0.5562 - val_loss: 0.6616 - val_acc: 0.6536\n",
      "Epoch 830/1000\n",
      "712/712 [==============================] - 0s 150us/step - loss: 0.6788 - acc: 0.5632 - val_loss: 0.6616 - val_acc: 0.6536\n",
      "Epoch 831/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6819 - acc: 0.5576 - val_loss: 0.6615 - val_acc: 0.6536\n",
      "Epoch 832/1000\n",
      "712/712 [==============================] - 0s 110us/step - loss: 0.6760 - acc: 0.5829 - val_loss: 0.6615 - val_acc: 0.6536\n",
      "Epoch 833/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6785 - acc: 0.5801 - val_loss: 0.6615 - val_acc: 0.6536\n",
      "Epoch 834/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6775 - acc: 0.5688 - val_loss: 0.6615 - val_acc: 0.6536\n",
      "Epoch 835/1000\n",
      "712/712 [==============================] - 0s 142us/step - loss: 0.6903 - acc: 0.5295 - val_loss: 0.6615 - val_acc: 0.6536\n",
      "Epoch 836/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6840 - acc: 0.5590 - val_loss: 0.6615 - val_acc: 0.6536\n",
      "Epoch 837/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6838 - acc: 0.5506 - val_loss: 0.6614 - val_acc: 0.6536\n",
      "Epoch 838/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6886 - acc: 0.5393 - val_loss: 0.6614 - val_acc: 0.6536\n",
      "Epoch 839/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6827 - acc: 0.5801 - val_loss: 0.6614 - val_acc: 0.6536\n",
      "Epoch 840/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6837 - acc: 0.5646 - val_loss: 0.6614 - val_acc: 0.6536\n",
      "Epoch 841/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6773 - acc: 0.5744 - val_loss: 0.6614 - val_acc: 0.6536\n",
      "Epoch 842/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6808 - acc: 0.5576 - val_loss: 0.6614 - val_acc: 0.6536\n",
      "Epoch 843/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6765 - acc: 0.5843 - val_loss: 0.6614 - val_acc: 0.6536\n",
      "Epoch 844/1000\n",
      "712/712 [==============================] - 0s 120us/step - loss: 0.6787 - acc: 0.5744 - val_loss: 0.6613 - val_acc: 0.6536\n",
      "Epoch 845/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6831 - acc: 0.5688 - val_loss: 0.6613 - val_acc: 0.6536\n",
      "Epoch 846/1000\n",
      "712/712 [==============================] - 0s 141us/step - loss: 0.6873 - acc: 0.5253 - val_loss: 0.6613 - val_acc: 0.6536\n",
      "Epoch 847/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6853 - acc: 0.5478 - val_loss: 0.6613 - val_acc: 0.6536\n",
      "Epoch 848/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6710 - acc: 0.6081 - val_loss: 0.6613 - val_acc: 0.6536\n",
      "Epoch 849/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6773 - acc: 0.5702 - val_loss: 0.6613 - val_acc: 0.6536\n",
      "Epoch 850/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6827 - acc: 0.5604 - val_loss: 0.6613 - val_acc: 0.6536\n",
      "Epoch 851/1000\n",
      "712/712 [==============================] - 0s 137us/step - loss: 0.6783 - acc: 0.5562 - val_loss: 0.6612 - val_acc: 0.6536\n",
      "Epoch 852/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6780 - acc: 0.5604 - val_loss: 0.6612 - val_acc: 0.6536\n",
      "Epoch 853/1000\n",
      "712/712 [==============================] - 0s 135us/step - loss: 0.6799 - acc: 0.5674 - val_loss: 0.6612 - val_acc: 0.6536\n",
      "Epoch 854/1000\n",
      "712/712 [==============================] - 0s 144us/step - loss: 0.6803 - acc: 0.5506 - val_loss: 0.6612 - val_acc: 0.6536\n",
      "Epoch 855/1000\n",
      "712/712 [==============================] - 0s 128us/step - loss: 0.6760 - acc: 0.5885 - val_loss: 0.6612 - val_acc: 0.6536\n",
      "Epoch 856/1000\n",
      "712/712 [==============================] - 0s 127us/step - loss: 0.6776 - acc: 0.5758 - val_loss: 0.6612 - val_acc: 0.6536\n",
      "Epoch 857/1000\n",
      "712/712 [==============================] - 0s 120us/step - loss: 0.6913 - acc: 0.5337 - val_loss: 0.6611 - val_acc: 0.6536\n",
      "Epoch 858/1000\n",
      "712/712 [==============================] - 0s 132us/step - loss: 0.6864 - acc: 0.5211 - val_loss: 0.6611 - val_acc: 0.6536\n",
      "Epoch 859/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6830 - acc: 0.5632 - val_loss: 0.6611 - val_acc: 0.6536\n",
      "Epoch 860/1000\n",
      "712/712 [==============================] - 0s 129us/step - loss: 0.6850 - acc: 0.5492 - val_loss: 0.6611 - val_acc: 0.6536\n",
      "Epoch 861/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6795 - acc: 0.5730 - val_loss: 0.6611 - val_acc: 0.6536\n",
      "Epoch 862/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6850 - acc: 0.5379 - val_loss: 0.6611 - val_acc: 0.6536\n",
      "Epoch 863/1000\n",
      "712/712 [==============================] - 0s 98us/step - loss: 0.6847 - acc: 0.5449 - val_loss: 0.6611 - val_acc: 0.6536\n",
      "Epoch 864/1000\n",
      "712/712 [==============================] - 0s 101us/step - loss: 0.6850 - acc: 0.5449 - val_loss: 0.6610 - val_acc: 0.6536\n",
      "Epoch 865/1000\n",
      "712/712 [==============================] - 0s 108us/step - loss: 0.6771 - acc: 0.5927 - val_loss: 0.6610 - val_acc: 0.6536\n",
      "Epoch 866/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6828 - acc: 0.5660 - val_loss: 0.6610 - val_acc: 0.6536\n",
      "Epoch 867/1000\n",
      "712/712 [==============================] - 0s 133us/step - loss: 0.6841 - acc: 0.5618 - val_loss: 0.6610 - val_acc: 0.6536\n",
      "Epoch 868/1000\n",
      "712/712 [==============================] - 0s 134us/step - loss: 0.6758 - acc: 0.5787 - val_loss: 0.6610 - val_acc: 0.6536\n",
      "Epoch 869/1000\n",
      "712/712 [==============================] - 0s 99us/step - loss: 0.6734 - acc: 0.5857 - val_loss: 0.6610 - val_acc: 0.6536\n",
      "Epoch 870/1000\n",
      "712/712 [==============================] - 0s 107us/step - loss: 0.6849 - acc: 0.5463 - val_loss: 0.6609 - val_acc: 0.6536\n",
      "Epoch 871/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6835 - acc: 0.5548 - val_loss: 0.6609 - val_acc: 0.6536\n",
      "Epoch 872/1000\n",
      "712/712 [==============================] - 0s 115us/step - loss: 0.6831 - acc: 0.5337 - val_loss: 0.6609 - val_acc: 0.6536\n",
      "Epoch 873/1000\n",
      "712/712 [==============================] - 0s 104us/step - loss: 0.6769 - acc: 0.5843 - val_loss: 0.6609 - val_acc: 0.6536\n",
      "Epoch 874/1000\n",
      "712/712 [==============================] - 0s 102us/step - loss: 0.6794 - acc: 0.5660 - val_loss: 0.6609 - val_acc: 0.6536\n",
      "Epoch 875/1000\n",
      "712/712 [==============================] - 0s 111us/step - loss: 0.6758 - acc: 0.5618 - val_loss: 0.6609 - val_acc: 0.6536\n",
      "Epoch 876/1000\n",
      "712/712 [==============================] - 0s 103us/step - loss: 0.6862 - acc: 0.5534 - val_loss: 0.6609 - val_acc: 0.6536\n",
      "Epoch 877/1000\n",
      "712/712 [==============================] - 0s 117us/step - loss: 0.6801 - acc: 0.5590 - val_loss: 0.6608 - val_acc: 0.6536\n",
      "Epoch 878/1000\n",
      "712/712 [==============================] - 0s 108us/step - loss: 0.6774 - acc: 0.5562 - val_loss: 0.6608 - val_acc: 0.6536\n",
      "Epoch 879/1000\n",
      "712/712 [==============================] - 0s 105us/step - loss: 0.6752 - acc: 0.5871 - val_loss: 0.6608 - val_acc: 0.6536\n",
      "Epoch 880/1000\n",
      "712/712 [==============================] - 0s 113us/step - loss: 0.6848 - acc: 0.5379 - val_loss: 0.6608 - val_acc: 0.6536\n",
      "Epoch 881/1000\n",
      "712/712 [==============================] - 0s 116us/step - loss: 0.6844 - acc: 0.5492 - val_loss: 0.6608 - val_acc: 0.6536\n",
      "Epoch 882/1000\n",
      "712/712 [==============================] - 0s 119us/step - loss: 0.6752 - acc: 0.5744 - val_loss: 0.6608 - val_acc: 0.6536\n",
      "Epoch 883/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6866 - acc: 0.5506 - val_loss: 0.6607 - val_acc: 0.6536\n",
      "Epoch 884/1000\n",
      "712/712 [==============================] - 0s 123us/step - loss: 0.6806 - acc: 0.5520 - val_loss: 0.6607 - val_acc: 0.6536\n",
      "Epoch 885/1000\n",
      "712/712 [==============================] - 0s 131us/step - loss: 0.6870 - acc: 0.5435 - val_loss: 0.6607 - val_acc: 0.6536\n",
      "Epoch 886/1000\n",
      "712/712 [==============================] - 0s 139us/step - loss: 0.6783 - acc: 0.5646 - val_loss: 0.6607 - val_acc: 0.6536\n",
      "Epoch 887/1000\n",
      "712/712 [==============================] - 0s 126us/step - loss: 0.6792 - acc: 0.5534 - val_loss: 0.6607 - val_acc: 0.6536\n",
      "Epoch 888/1000\n",
      "712/712 [==============================] - 0s 143us/step - loss: 0.6783 - acc: 0.5604 - val_loss: 0.6607 - val_acc: 0.6536\n",
      "Epoch 889/1000\n",
      "712/712 [==============================] - 0s 125us/step - loss: 0.6812 - acc: 0.5562 - val_loss: 0.6607 - val_acc: 0.6536\n",
      "Epoch 890/1000\n",
      "712/712 [==============================] - 0s 130us/step - loss: 0.6823 - acc: 0.5463 - val_loss: 0.6606 - val_acc: 0.6536\n",
      "Epoch 891/1000\n",
      " 32/712 [>.............................] - ETA: 0s - loss: 0.7262 - acc: 0.3438"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-a6024ef984eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_reduced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/tensorflow/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn = Sequential([\n",
    "    Dense(64, input_shape=(train_reduced.shape[1],), activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10, verbose=1, mode='auto')\n",
    "callback_list = [earlystop]\n",
    "sgd = optimizers.SGD(lr=1e-6, decay=1e-8, momentum=0.9, nesterov=True)\n",
    "# For a binary classification problem\n",
    "nn.compile(optimizer=sgd,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "nn.fit(train_reduced, y, epochs=1000, batch_size=32, validation_split=0.2, shuffle=True, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
